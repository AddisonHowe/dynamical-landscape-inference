Args:
Namespace(name='model_phi1_1a_distortion_v2r_3_v_mmd1', outdir='out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.033376977, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2282364683

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.56015085417764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.56015085417764 | validation: 4.4911291840802345]
	TIME [epoch: 358 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.209406327166084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.209406327166084 | validation: 4.299260244178703]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5649804880656006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5649804880656006 | validation: 3.7044813863664885]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0312867314019885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0312867314019885 | validation: 3.3265749279402916]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9207059694795023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9207059694795023 | validation: 3.0982898738522584]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6993717458834388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6993717458834388 | validation: 3.030359665038504]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4749793624974292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4749793624974292 | validation: 2.943779664822546]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5308218068623574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5308218068623574 | validation: 2.8119047225779568]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.299890288447737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.299890288447737 | validation: 2.9973715398331517]
	TIME [epoch: 5.94 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3677107903439287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3677107903439287 | validation: 2.960473578732307]
	TIME [epoch: 6.34 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2398158245946203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2398158245946203 | validation: 2.9201861343370856]
	TIME [epoch: 6.18 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1570276644878423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1570276644878423 | validation: 2.7570341825893436]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.083178192138672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.083178192138672 | validation: 2.8762807041830065]
	TIME [epoch: 5.94 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.211509093194079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.211509093194079 | validation: 2.793714818538448]
	TIME [epoch: 5.92 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0176314417401224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0176314417401224 | validation: 2.591373148853334]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0081652302672994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0081652302672994 | validation: 2.47267082764548]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9752903576478178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9752903576478178 | validation: 2.608412471208551]
	TIME [epoch: 5.92 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8770322060692701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8770322060692701 | validation: 2.6232348150227]
	TIME [epoch: 5.92 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9500477881037255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9500477881037255 | validation: 2.608268471688028]
	TIME [epoch: 5.94 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8825802150059858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8825802150059858 | validation: 2.5626250230126524]
	TIME [epoch: 5.94 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8568502929261663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8568502929261663 | validation: 2.473716465259879]
	TIME [epoch: 5.93 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8637542761273997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8637542761273997 | validation: 2.442025281502729]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7783223585536168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7783223585536168 | validation: 2.435406555521369]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654308790731335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8654308790731335 | validation: 2.491609427027732]
	TIME [epoch: 5.93 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7921505024551436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7921505024551436 | validation: 2.3608423935621863]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8113953631684419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8113953631684419 | validation: 2.433922037575485]
	TIME [epoch: 5.95 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8421368055761744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8421368055761744 | validation: 2.272830553089599]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7815330533275509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7815330533275509 | validation: 2.4002755075502002]
	TIME [epoch: 5.94 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8129839807418668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8129839807418668 | validation: 2.305608624045022]
	TIME [epoch: 5.93 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.753545659325332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.753545659325332 | validation: 2.285602621226542]
	TIME [epoch: 5.93 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7323708751557398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7323708751557398 | validation: 2.239146362737212]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.755914377439822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.755914377439822 | validation: 2.1983451966303527]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6642903266268094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6642903266268094 | validation: 2.1045530927012477]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7679039793238305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7679039793238305 | validation: 2.268416707414955]
	TIME [epoch: 5.94 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6481923252196056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6481923252196056 | validation: 2.133815386374497]
	TIME [epoch: 5.93 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7703730434540053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7703730434540053 | validation: 2.1567607796429065]
	TIME [epoch: 6.04 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6386345443648211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6386345443648211 | validation: 2.1207851717369453]
	TIME [epoch: 5.95 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7544014836748756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7544014836748756 | validation: 2.0198366010060522]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.675687323816935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.675687323816935 | validation: 2.100153411974013]
	TIME [epoch: 5.94 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.692315125081513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.692315125081513 | validation: 2.1891511767825564]
	TIME [epoch: 5.93 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6553699616273803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6553699616273803 | validation: 2.274325415304921]
	TIME [epoch: 5.92 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8789416855512973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8789416855512973 | validation: 1.9355377529708115]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7554584040631327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7554584040631327 | validation: 2.1600035153660757]
	TIME [epoch: 5.93 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7475344359166194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7475344359166194 | validation: 2.0305721322017947]
	TIME [epoch: 5.95 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.745768505049731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.745768505049731 | validation: 2.020904079882204]
	TIME [epoch: 5.95 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.727168664634272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.727168664634272 | validation: 2.055138523049517]
	TIME [epoch: 5.92 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6778609936899755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6778609936899755 | validation: 1.9180010011428044]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7313920720120655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7313920720120655 | validation: 1.8868249189050301]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6594484007034076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6594484007034076 | validation: 1.9666730805557167]
	TIME [epoch: 5.94 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7491512755922025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7491512755922025 | validation: 1.8695916280323996]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6517970270188091		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.6517970270188091 | validation: 1.8959093085065386]
	TIME [epoch: 5.93 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7071396048998888		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.7071396048998888 | validation: 1.8380948761176572]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.629134096028703		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.629134096028703 | validation: 2.0376387024350544]
	TIME [epoch: 5.93 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.695372462871631		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.695372462871631 | validation: 1.8012559017011225]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6679587746794813		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.6679587746794813 | validation: 1.9180380194240652]
	TIME [epoch: 5.93 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6484993523918914		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.6484993523918914 | validation: 1.824744812154253]
	TIME [epoch: 5.93 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5920837874300373		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.5920837874300373 | validation: 1.7607890368288812]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6848323305558413		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.6848323305558413 | validation: 1.7813025372858005]
	TIME [epoch: 5.94 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6025516974526344		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.6025516974526344 | validation: 1.7571731661649355]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5845617477315337		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.5845617477315337 | validation: 1.6482826314790175]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4695674755849668		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.4695674755849668 | validation: 1.8037496834060731]
	TIME [epoch: 5.94 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5851491368029833		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.5851491368029833 | validation: 1.7000955594277376]
	TIME [epoch: 6.32 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5863904300422225		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.5863904300422225 | validation: 1.541078580726675]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5119694590821515		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.5119694590821515 | validation: 1.601606050674674]
	TIME [epoch: 5.93 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.514473559351576		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.514473559351576 | validation: 1.4240796862706513]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5852397176575137		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 1.5852397176575137 | validation: 1.2872399160448569]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4177557887984595		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.4177557887984595 | validation: 1.250870670511905]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3118670986849263		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.3118670986849263 | validation: 1.3711185294309192]
	TIME [epoch: 5.94 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4858500800969598		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.4858500800969598 | validation: 1.3287584848350384]
	TIME [epoch: 5.93 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274841726488574		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.274841726488574 | validation: 3.299110415155565]
	TIME [epoch: 5.93 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.234298806095845		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 2.234298806095845 | validation: 1.990536757970938]
	TIME [epoch: 5.93 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5279086995868698		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 1.5279086995868698 | validation: 1.3115041179189122]
	TIME [epoch: 5.93 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3021207317635024		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.3021207317635024 | validation: 1.203262940796415]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1232187683913097		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 1.1232187683913097 | validation: 1.1742088242283222]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0818075746371356		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 1.0818075746371356 | validation: 1.0640658122442894]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0752583269178158		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 1.0752583269178158 | validation: 0.9474763166408807]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8116017694920261		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.8116017694920261 | validation: 0.9846008484365294]
	TIME [epoch: 5.94 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9373199058853205		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.9373199058853205 | validation: 1.4695079491174865]
	TIME [epoch: 5.93 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0908622646470212		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.0908622646470212 | validation: 0.9710862836739479]
	TIME [epoch: 5.93 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.859826835629877		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.859826835629877 | validation: 1.0017252638571563]
	TIME [epoch: 5.93 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9495536460943454		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.9495536460943454 | validation: 0.9640887665109772]
	TIME [epoch: 5.93 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7666184687440589		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.7666184687440589 | validation: 0.7874255157699954]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8425041423682813		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.8425041423682813 | validation: 0.9593367139746576]
	TIME [epoch: 5.93 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7517702665737874		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.7517702665737874 | validation: 0.8944707461492827]
	TIME [epoch: 5.92 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7966707305731949		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.7966707305731949 | validation: 0.9174787122679666]
	TIME [epoch: 5.93 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8453985572142544		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.8453985572142544 | validation: 0.8627162678040223]
	TIME [epoch: 5.92 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7828804325918629		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.7828804325918629 | validation: 0.7873375026964815]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7226265086763632		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.7226265086763632 | validation: 0.7598954327037533]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7096333624099697		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.7096333624099697 | validation: 0.8813890506487893]
	TIME [epoch: 5.93 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7839097279928788		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.7839097279928788 | validation: 0.6257757329566955]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7758878357303738		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.7758878357303738 | validation: 0.794682174206268]
	TIME [epoch: 5.94 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7413436273922147		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.7413436273922147 | validation: 0.7392711920457863]
	TIME [epoch: 5.92 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7293062205591847		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.7293062205591847 | validation: 0.6771625958895655]
	TIME [epoch: 5.92 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6669152688395668		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.6669152688395668 | validation: 0.7758582528926649]
	TIME [epoch: 5.93 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7084279490833185		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.7084279490833185 | validation: 0.7079586361082066]
	TIME [epoch: 5.93 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6814800863174166		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.6814800863174166 | validation: 0.6699811695547357]
	TIME [epoch: 5.93 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6910101790484048		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.6910101790484048 | validation: 0.8142116891174593]
	TIME [epoch: 5.94 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.794195159977982		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.794195159977982 | validation: 0.6811109084659186]
	TIME [epoch: 5.94 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6771197897993979		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.6771197897993979 | validation: 0.6573385441551542]
	TIME [epoch: 5.93 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6721637983976672		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.6721637983976672 | validation: 0.6181333923089996]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6319073180829998		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.6319073180829998 | validation: 0.7268148940710379]
	TIME [epoch: 5.94 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.70081616201774		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.70081616201774 | validation: 0.6567283759369836]
	TIME [epoch: 5.92 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7740739434124804		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.7740739434124804 | validation: 0.618988300591683]
	TIME [epoch: 5.92 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6073338640072533		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.6073338640072533 | validation: 0.5668898378264837]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6110690140351788		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.6110690140351788 | validation: 0.6145519908443063]
	TIME [epoch: 5.94 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6307098178399125		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.6307098178399125 | validation: 0.6421879631054652]
	TIME [epoch: 5.92 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5238273465236498		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.5238273465236498 | validation: 0.739842514713348]
	TIME [epoch: 5.92 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.124992315326147		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 1.124992315326147 | validation: 1.061949276546256]
	TIME [epoch: 5.93 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8130016348386228		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.8130016348386228 | validation: 0.7335324005352444]
	TIME [epoch: 5.94 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6212720366252771		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.6212720366252771 | validation: 0.7568573947581503]
	TIME [epoch: 5.93 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5570770697932335		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5570770697932335 | validation: 0.6985635866365534]
	TIME [epoch: 5.94 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5682552178717692		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.5682552178717692 | validation: 0.75887269864733]
	TIME [epoch: 5.94 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5806015837155646		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.5806015837155646 | validation: 0.5274784034371923]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6017209891203115		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.6017209891203115 | validation: 0.5358876310775881]
	TIME [epoch: 5.93 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005817840943237		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.6005817840943237 | validation: 0.6730578870440938]
	TIME [epoch: 5.94 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5175558135426954		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.5175558135426954 | validation: 0.6698176551882723]
	TIME [epoch: 5.93 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6393301547320704		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.6393301547320704 | validation: 0.652135329217235]
	TIME [epoch: 5.92 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128717960953247		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.6128717960953247 | validation: 0.5520909871153244]
	TIME [epoch: 5.93 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5642634701386733		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.5642634701386733 | validation: 0.39436252874041444]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5536885489941805		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.5536885489941805 | validation: 0.8196192981818451]
	TIME [epoch: 5.93 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6045176177025543		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.6045176177025543 | validation: 0.6065596705534012]
	TIME [epoch: 5.92 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5731278160747517		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.5731278160747517 | validation: 0.471014797526249]
	TIME [epoch: 5.92 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5390825236696962		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.5390825236696962 | validation: 0.5167806550126224]
	TIME [epoch: 5.94 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5759923649352819		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.5759923649352819 | validation: 0.507280518663666]
	TIME [epoch: 5.94 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5361283766753665		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.5361283766753665 | validation: 0.666910256635095]
	TIME [epoch: 5.95 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5824609403636731		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.5824609403636731 | validation: 0.6450236440245782]
	TIME [epoch: 5.94 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5217655602962445		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.5217655602962445 | validation: 0.45140859905554254]
	TIME [epoch: 5.94 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5446690688514031		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.5446690688514031 | validation: 0.5762181006453182]
	TIME [epoch: 5.94 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5590063361724374		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.5590063361724374 | validation: 0.47643849160965174]
	TIME [epoch: 5.94 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.566158732569021		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.566158732569021 | validation: 0.451939296257302]
	TIME [epoch: 5.94 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48537005914992015		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.48537005914992015 | validation: 0.6891987147407865]
	TIME [epoch: 5.94 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5583271270591694		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.5583271270591694 | validation: 0.5070134054376167]
	TIME [epoch: 5.95 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5223280687290996		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.5223280687290996 | validation: 0.5488625859340046]
	TIME [epoch: 5.94 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5136444029452186		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.5136444029452186 | validation: 0.4764798210618743]
	TIME [epoch: 5.93 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48510723799477734		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.48510723799477734 | validation: 0.5673451093064958]
	TIME [epoch: 5.94 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5834996474161538		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.5834996474161538 | validation: 0.4219778870339912]
	TIME [epoch: 5.94 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5706576115047656		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.5706576115047656 | validation: 0.5717070147551692]
	TIME [epoch: 5.94 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6186361065581292		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.6186361065581292 | validation: 0.6906506487303656]
	TIME [epoch: 5.95 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4471148979283325		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.4471148979283325 | validation: 0.4894473300520865]
	TIME [epoch: 5.94 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5452292738829984		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.5452292738829984 | validation: 0.44865586835406657]
	TIME [epoch: 5.94 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4995168436653916		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.4995168436653916 | validation: 0.4825078596104001]
	TIME [epoch: 5.97 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48755695611045435		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.48755695611045435 | validation: 0.5454349102865257]
	TIME [epoch: 5.93 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4855460989338276		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.4855460989338276 | validation: 0.4954941564401273]
	TIME [epoch: 5.94 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5018593345066021		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.5018593345066021 | validation: 0.44581689680234415]
	TIME [epoch: 5.94 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5063148783440297		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.5063148783440297 | validation: 0.3770581380323892]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5063364962358533		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.5063364962358533 | validation: 0.35718961425670626]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49369281809725746		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.49369281809725746 | validation: 0.6865758934536172]
	TIME [epoch: 5.94 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.514274470647762		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.514274470647762 | validation: 0.4925290292257873]
	TIME [epoch: 5.93 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4768724387444856		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.4768724387444856 | validation: 0.40703513108381667]
	TIME [epoch: 5.94 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4526894504298642		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.4526894504298642 | validation: 0.5762367361466681]
	TIME [epoch: 5.94 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5042169360110164		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.5042169360110164 | validation: 0.42579802924823607]
	TIME [epoch: 5.94 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44094296714736925		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.44094296714736925 | validation: 0.43612912280166194]
	TIME [epoch: 6.06 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4837802664628482		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.4837802664628482 | validation: 0.39157617607540973]
	TIME [epoch: 5.94 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45089329241452647		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.45089329241452647 | validation: 0.386020827566587]
	TIME [epoch: 5.94 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4799977135944685		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.4799977135944685 | validation: 0.5002699757706879]
	TIME [epoch: 6.02 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4462617735104159		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.4462617735104159 | validation: 0.5818894023944198]
	TIME [epoch: 5.94 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46117642929857156		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.46117642929857156 | validation: 0.5083954576405092]
	TIME [epoch: 5.93 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4675336225092095		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.4675336225092095 | validation: 0.46774003104373274]
	TIME [epoch: 6.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4606995702114447		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.4606995702114447 | validation: 0.387520434642134]
	TIME [epoch: 5.94 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4100267163189394		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.4100267163189394 | validation: 0.5250165041239989]
	TIME [epoch: 5.94 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4654551596357065		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.4654551596357065 | validation: 0.3795178073095089]
	TIME [epoch: 5.93 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4045689230730626		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.4045689230730626 | validation: 0.7189960711015954]
	TIME [epoch: 5.95 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47062586505069065		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.47062586505069065 | validation: 0.4590125941001172]
	TIME [epoch: 6.49 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4546436653333924		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.4546436653333924 | validation: 0.3639347162615786]
	TIME [epoch: 5.93 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.426696469929306		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.426696469929306 | validation: 0.4457217438986255]
	TIME [epoch: 5.93 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4715188555879613		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.4715188555879613 | validation: 0.4845695052564738]
	TIME [epoch: 5.94 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41684873567786423		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.41684873567786423 | validation: 0.5750259833534073]
	TIME [epoch: 5.94 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44931483441707065		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.44931483441707065 | validation: 0.4384169850513523]
	TIME [epoch: 5.94 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4363776628091097		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.4363776628091097 | validation: 0.3966578312676192]
	TIME [epoch: 5.94 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4032417509741823		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.4032417509741823 | validation: 0.5915515201633665]
	TIME [epoch: 5.94 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4180963225297476		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.4180963225297476 | validation: 0.46337404569807034]
	TIME [epoch: 5.93 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3849988434898468		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.3849988434898468 | validation: 0.5130872318514954]
	TIME [epoch: 5.94 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45625804109512746		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.45625804109512746 | validation: 0.2913399407028997]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39020894228425124		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.39020894228425124 | validation: 0.4034911063400356]
	TIME [epoch: 5.92 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42774312696174754		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.42774312696174754 | validation: 0.390680969521365]
	TIME [epoch: 5.94 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4521543476840291		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.4521543476840291 | validation: 0.35325200567330617]
	TIME [epoch: 5.94 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4365332067661745		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.4365332067661745 | validation: 0.44203188704773755]
	TIME [epoch: 6.06 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5248616943009144		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.5248616943009144 | validation: 0.37608705871159936]
	TIME [epoch: 5.94 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4387308016669712		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.4387308016669712 | validation: 0.40412510489937026]
	TIME [epoch: 5.94 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4075639567396325		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.4075639567396325 | validation: 0.35060730881994795]
	TIME [epoch: 5.93 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4037754815012233		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.4037754815012233 | validation: 0.4379628692228768]
	TIME [epoch: 5.94 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3832627191012721		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.3832627191012721 | validation: 0.42919559383453154]
	TIME [epoch: 5.94 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.422442831810209		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.422442831810209 | validation: 0.45481720227440203]
	TIME [epoch: 5.94 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37470493731326476		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.37470493731326476 | validation: 0.42017234541129006]
	TIME [epoch: 5.94 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4473367118952597		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.4473367118952597 | validation: 0.4491018232404952]
	TIME [epoch: 5.98 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42738994736360003		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.42738994736360003 | validation: 0.362041800084528]
	TIME [epoch: 5.94 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38500807877576415		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.38500807877576415 | validation: 0.3481879609948239]
	TIME [epoch: 5.94 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3742026088485992		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.3742026088485992 | validation: 0.40892754002982057]
	TIME [epoch: 5.94 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40034588194169984		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.40034588194169984 | validation: 0.4232176645624869]
	TIME [epoch: 5.94 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40985204670245723		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.40985204670245723 | validation: 0.32187952064300857]
	TIME [epoch: 5.99 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3624382813205387		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.3624382813205387 | validation: 0.4132238858814426]
	TIME [epoch: 5.94 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39147580280053096		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.39147580280053096 | validation: 0.36594059975830706]
	TIME [epoch: 5.94 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41326165188988584		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.41326165188988584 | validation: 0.35168408621654024]
	TIME [epoch: 5.93 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3651253150915091		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.3651253150915091 | validation: 0.3474626257498318]
	TIME [epoch: 5.94 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36391750215687796		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.36391750215687796 | validation: 0.37179802166746523]
	TIME [epoch: 5.94 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37385635026496583		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.37385635026496583 | validation: 0.43981582312411005]
	TIME [epoch: 5.94 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3636547599148766		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.3636547599148766 | validation: 0.575952616726777]
	TIME [epoch: 5.94 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3863797026138034		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.3863797026138034 | validation: 0.484492667868776]
	TIME [epoch: 5.94 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3539553992284488		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.3539553992284488 | validation: 0.4410441722985887]
	TIME [epoch: 5.95 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3991382804651036		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.3991382804651036 | validation: 0.34325722497880984]
	TIME [epoch: 5.94 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3920510007737767		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.3920510007737767 | validation: 0.2989399111309238]
	TIME [epoch: 381 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36499640727587135		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.36499640727587135 | validation: 0.26431112516563515]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3498134300398334		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.3498134300398334 | validation: 0.4724550985441369]
	TIME [epoch: 11.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.323256123183957		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.323256123183957 | validation: 0.4850754702819645]
	TIME [epoch: 11.7 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35302166613280245		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.35302166613280245 | validation: 0.37402833769953264]
	TIME [epoch: 11.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34547730070779603		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.34547730070779603 | validation: 0.45692710998676533]
	TIME [epoch: 11.7 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3669004888267615		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.3669004888267615 | validation: 0.3417645742956102]
	TIME [epoch: 11.7 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3707279699699736		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.3707279699699736 | validation: 0.27956769328482967]
	TIME [epoch: 11.7 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33596070385599025		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.33596070385599025 | validation: 0.32989035949368434]
	TIME [epoch: 11.7 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36849008000064154		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.36849008000064154 | validation: 0.3963234635818062]
	TIME [epoch: 11.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3308414311988474		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.3308414311988474 | validation: 0.47623187974575143]
	TIME [epoch: 11.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3470109837898842		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.3470109837898842 | validation: 0.5247061932552686]
	TIME [epoch: 11.7 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31430535020764083		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.31430535020764083 | validation: 0.4476130433791306]
	TIME [epoch: 11.7 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38793038762328613		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.38793038762328613 | validation: 0.33983439746411104]
	TIME [epoch: 11.7 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34168776100926235		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.34168776100926235 | validation: 0.30384613459505405]
	TIME [epoch: 11.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33768917249112124		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.33768917249112124 | validation: 0.3097510738072181]
	TIME [epoch: 11.7 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3261929737379541		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.3261929737379541 | validation: 0.27993610978962546]
	TIME [epoch: 11.7 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28800860250496335		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.28800860250496335 | validation: 0.3307114406139833]
	TIME [epoch: 11.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3266694902986976		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.3266694902986976 | validation: 0.3312105502202882]
	TIME [epoch: 11.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2872694128816533		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.2872694128816533 | validation: 0.38570837332014074]
	TIME [epoch: 11.7 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38362148457061523		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.38362148457061523 | validation: 0.34452587536036694]
	TIME [epoch: 11.7 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260309360580377		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.3260309360580377 | validation: 0.30803914196686144]
	TIME [epoch: 11.7 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2937635283594999		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.2937635283594999 | validation: 0.377057200276782]
	TIME [epoch: 11.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.326062434234186		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.326062434234186 | validation: 0.33283357325861096]
	TIME [epoch: 11.7 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3206988976484645		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.3206988976484645 | validation: 0.36301328794840815]
	TIME [epoch: 11.7 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32802788264712013		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.32802788264712013 | validation: 0.27189040940327636]
	TIME [epoch: 11.7 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3422779287398099		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.3422779287398099 | validation: 0.2555964877982285]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28122204352745633		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.28122204352745633 | validation: 0.43055949625782053]
	TIME [epoch: 11.7 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3253301905430255		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.3253301905430255 | validation: 0.30407782033462516]
	TIME [epoch: 11.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29348390272491454		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.29348390272491454 | validation: 0.30634636299042184]
	TIME [epoch: 11.7 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3066376057654069		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.3066376057654069 | validation: 0.429179676917569]
	TIME [epoch: 11.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33773879359917663		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.33773879359917663 | validation: 0.3735087979525811]
	TIME [epoch: 11.7 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29999570597036274		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.29999570597036274 | validation: 0.41077788128555154]
	TIME [epoch: 11.7 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33167482823314925		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.33167482823314925 | validation: 0.3815609949202525]
	TIME [epoch: 11.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3163366175612182		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.3163366175612182 | validation: 0.31934839453596164]
	TIME [epoch: 11.7 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27855514037086054		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.27855514037086054 | validation: 0.3645365069175629]
	TIME [epoch: 11.7 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3111600239439998		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.3111600239439998 | validation: 0.4071202715018989]
	TIME [epoch: 11.7 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4147869760797538		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.4147869760797538 | validation: 0.284184290289432]
	TIME [epoch: 11.7 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29188679294344233		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.29188679294344233 | validation: 0.26426207914627553]
	TIME [epoch: 11.7 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100587082920779		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.3100587082920779 | validation: 0.3334153842596769]
	TIME [epoch: 11.7 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2787611203247868		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.2787611203247868 | validation: 0.395615465909002]
	TIME [epoch: 11.7 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3155002159601382		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.3155002159601382 | validation: 0.35782914554328293]
	TIME [epoch: 11.7 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2832333138295101		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.2832333138295101 | validation: 0.35682310045131027]
	TIME [epoch: 11.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286986459881299		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.286986459881299 | validation: 0.22856630493541175]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3149725799512687		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.3149725799512687 | validation: 0.2478923192542852]
	TIME [epoch: 11.7 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29463517329911093		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.29463517329911093 | validation: 0.2977549849984027]
	TIME [epoch: 11.7 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29769265520099464		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.29769265520099464 | validation: 0.33314762333751435]
	TIME [epoch: 11.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2970250995676369		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.2970250995676369 | validation: 0.3160378303746804]
	TIME [epoch: 11.7 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593947753879448		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.2593947753879448 | validation: 0.37115292726821314]
	TIME [epoch: 11.7 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.317718652139376		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.317718652139376 | validation: 0.27848768583794875]
	TIME [epoch: 11.7 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27486354827912896		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.27486354827912896 | validation: 0.33242592914500924]
	TIME [epoch: 11.7 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28130592052661246		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.28130592052661246 | validation: 0.33388374294651924]
	TIME [epoch: 11.7 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024049221504616		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.3024049221504616 | validation: 0.24729090816910804]
	TIME [epoch: 11.7 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2951846763980984		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.2951846763980984 | validation: 0.3103455975945573]
	TIME [epoch: 11.7 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2800602160528711		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.2800602160528711 | validation: 0.22028949439294987]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2578030870924848		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.2578030870924848 | validation: 0.24388259210552793]
	TIME [epoch: 11.7 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.254707593747964		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.254707593747964 | validation: 0.340004600589573]
	TIME [epoch: 11.7 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3017992241897737		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.3017992241897737 | validation: 0.3680261671814623]
	TIME [epoch: 11.7 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.282151289320031		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.282151289320031 | validation: 0.3119760614441737]
	TIME [epoch: 11.7 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2607901507634817		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.2607901507634817 | validation: 0.30076242987556684]
	TIME [epoch: 11.7 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28805243028694427		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.28805243028694427 | validation: 0.28008467820338023]
	TIME [epoch: 11.7 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27183358952310444		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.27183358952310444 | validation: 0.3183144011009016]
	TIME [epoch: 11.7 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29512977907169535		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.29512977907169535 | validation: 0.2364591589417964]
	TIME [epoch: 11.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25196699215064144		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.25196699215064144 | validation: 0.2199119222340248]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19579323063675946		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.19579323063675946 | validation: 0.5554253521334599]
	TIME [epoch: 11.7 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2874029312537443		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.2874029312537443 | validation: 0.4356989908458217]
	TIME [epoch: 11.7 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30969933884374945		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.30969933884374945 | validation: 0.3690812832843838]
	TIME [epoch: 11.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28549895601150527		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.28549895601150527 | validation: 0.2166885302043453]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2327600448429148		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.2327600448429148 | validation: 0.2465082800642191]
	TIME [epoch: 11.7 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3174857453842511		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.3174857453842511 | validation: 0.24422752539603265]
	TIME [epoch: 11.7 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2923100080650198		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.2923100080650198 | validation: 0.22312457425741625]
	TIME [epoch: 11.7 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23814114071875125		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.23814114071875125 | validation: 0.309653461179069]
	TIME [epoch: 11.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3027776934243638		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.3027776934243638 | validation: 0.39683135893310173]
	TIME [epoch: 11.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637287226305893		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.2637287226305893 | validation: 0.3050008214639009]
	TIME [epoch: 11.7 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24536616093096458		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.24536616093096458 | validation: 0.31438310845941286]
	TIME [epoch: 11.7 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671276775679477		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.2671276775679477 | validation: 0.3065837587716407]
	TIME [epoch: 11.7 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2743536208015306		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.2743536208015306 | validation: 0.33400127091321175]
	TIME [epoch: 11.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2555214313931043		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.2555214313931043 | validation: 0.3490618294508627]
	TIME [epoch: 11.7 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2569891908082265		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.2569891908082265 | validation: 0.29688769191600717]
	TIME [epoch: 11.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695774545669232		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.2695774545669232 | validation: 0.21936238294403604]
	TIME [epoch: 11.7 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20815102339635005		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.20815102339635005 | validation: 0.35495905300188957]
	TIME [epoch: 11.7 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.274294745732581		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.274294745732581 | validation: 0.22576629229354217]
	TIME [epoch: 11.7 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2941536323261569		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.2941536323261569 | validation: 0.18130897621726577]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2874887299325633		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.2874887299325633 | validation: 0.3378723479635845]
	TIME [epoch: 11.7 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2745558414956579		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.2745558414956579 | validation: 0.27789281217680695]
	TIME [epoch: 11.7 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2534734220806546		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.2534734220806546 | validation: 0.39277150200926825]
	TIME [epoch: 11.7 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26533782554343044		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.26533782554343044 | validation: 0.30291666175224735]
	TIME [epoch: 11.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24400828609565195		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.24400828609565195 | validation: 0.3297452505589814]
	TIME [epoch: 11.7 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2977048402541849		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.2977048402541849 | validation: 0.45874613789512264]
	TIME [epoch: 11.7 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3060297595021853		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.3060297595021853 | validation: 0.28524251224516295]
	TIME [epoch: 11.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2527532649860934		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.2527532649860934 | validation: 0.29320438907836494]
	TIME [epoch: 11.7 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22805576082161053		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.22805576082161053 | validation: 0.2788296027117091]
	TIME [epoch: 11.7 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22104620731061073		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.22104620731061073 | validation: 0.273010187996631]
	TIME [epoch: 11.7 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30800016283472526		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.30800016283472526 | validation: 0.24724098375123665]
	TIME [epoch: 11.7 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3271258692457064		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.3271258692457064 | validation: 0.30233422766682805]
	TIME [epoch: 11.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694191573150895		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.2694191573150895 | validation: 0.1631187378916596]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22948310488467713		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.22948310488467713 | validation: 0.22506405003339286]
	TIME [epoch: 11.7 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2442770569783702		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.2442770569783702 | validation: 0.23556236397148342]
	TIME [epoch: 11.7 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2095669703318478		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.2095669703318478 | validation: 0.2551917668339497]
	TIME [epoch: 12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21493247848077626		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.21493247848077626 | validation: 0.25574211817412873]
	TIME [epoch: 11.7 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695833404800465		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.2695833404800465 | validation: 0.3706624302330924]
	TIME [epoch: 11.7 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26014527843507645		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.26014527843507645 | validation: 0.3129820805380372]
	TIME [epoch: 11.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2261791832847851		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.2261791832847851 | validation: 0.25329507596222817]
	TIME [epoch: 11.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22284972608263953		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.22284972608263953 | validation: 0.1862355918064697]
	TIME [epoch: 11.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22627810834919979		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.22627810834919979 | validation: 0.18139827164489628]
	TIME [epoch: 11.7 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26233074539735063		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.26233074539735063 | validation: 0.226789790118853]
	TIME [epoch: 11.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23320086715153324		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.23320086715153324 | validation: 0.277802771903635]
	TIME [epoch: 11.7 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20475413353328886		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.20475413353328886 | validation: 0.30881436501750936]
	TIME [epoch: 11.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24202191162065048		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.24202191162065048 | validation: 0.3948895212169121]
	TIME [epoch: 12.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26779800270079523		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.26779800270079523 | validation: 0.24130562722965188]
	TIME [epoch: 11.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1939634695008822		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.1939634695008822 | validation: 0.16568297563450682]
	TIME [epoch: 11.7 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19460368310233792		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.19460368310233792 | validation: 0.2081220839614064]
	TIME [epoch: 11.7 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26449398428517673		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.26449398428517673 | validation: 0.28638228356366763]
	TIME [epoch: 11.7 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2334536651217952		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.2334536651217952 | validation: 0.2697947746626635]
	TIME [epoch: 11.7 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22358963601353288		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.22358963601353288 | validation: 0.17630343991259462]
	TIME [epoch: 11.7 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21132295053590722		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.21132295053590722 | validation: 0.19888483297204923]
	TIME [epoch: 11.7 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20365469974801		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.20365469974801 | validation: 0.3022374376322847]
	TIME [epoch: 11.7 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23420303508443874		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.23420303508443874 | validation: 0.2087920940157289]
	TIME [epoch: 11.7 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1745651176291848		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.1745651176291848 | validation: 0.2143287832462328]
	TIME [epoch: 11.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2094134669389352		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.2094134669389352 | validation: 0.18179280038192816]
	TIME [epoch: 11.7 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21894864141068604		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.21894864141068604 | validation: 0.15989154468698644]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19983778348235867		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.19983778348235867 | validation: 0.3355527600528963]
	TIME [epoch: 11.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25261331843020024		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.25261331843020024 | validation: 0.2462609852489926]
	TIME [epoch: 11.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21707192583586205		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.21707192583586205 | validation: 0.2660352451335375]
	TIME [epoch: 11.7 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.192824254520758		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.192824254520758 | validation: 0.26575566650532023]
	TIME [epoch: 11.7 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20271798036063732		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.20271798036063732 | validation: 0.34006638940686473]
	TIME [epoch: 11.7 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20874601736784512		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.20874601736784512 | validation: 0.4383202049676656]
	TIME [epoch: 11.7 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2158033860313048		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.2158033860313048 | validation: 0.2582484603181884]
	TIME [epoch: 11.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2160901762744412		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.2160901762744412 | validation: 0.2384011176953027]
	TIME [epoch: 11.7 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18931898685098236		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.18931898685098236 | validation: 0.3986019963208033]
	TIME [epoch: 11.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23087963270500853		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.23087963270500853 | validation: 0.20500365382896135]
	TIME [epoch: 11.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20863282290466634		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.20863282290466634 | validation: 0.15738517256961543]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21640797160317776		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.21640797160317776 | validation: 0.23097728524733108]
	TIME [epoch: 11.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20601061989886044		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.20601061989886044 | validation: 0.18033261387406269]
	TIME [epoch: 11.7 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20860936090824211		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.20860936090824211 | validation: 0.26038388149516256]
	TIME [epoch: 11.7 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20723011149393322		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.20723011149393322 | validation: 0.20709101125257934]
	TIME [epoch: 11.7 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18762990064388363		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.18762990064388363 | validation: 0.23920362039364637]
	TIME [epoch: 11.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20490516034271342		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.20490516034271342 | validation: 0.22397402964901775]
	TIME [epoch: 11.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18485446400751102		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.18485446400751102 | validation: 0.525592595476124]
	TIME [epoch: 11.7 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28237886048151506		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.28237886048151506 | validation: 0.40478596848474635]
	TIME [epoch: 11.7 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21997455509040958		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.21997455509040958 | validation: 0.2882121140059008]
	TIME [epoch: 11.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20039158722744135		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.20039158722744135 | validation: 0.21181507375397796]
	TIME [epoch: 11.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19908596839637416		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.19908596839637416 | validation: 0.22144032741219044]
	TIME [epoch: 11.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20943304160025714		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.20943304160025714 | validation: 0.2722870802190654]
	TIME [epoch: 11.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18253057184233357		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.18253057184233357 | validation: 0.18230638292051426]
	TIME [epoch: 11.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19864715651401907		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.19864715651401907 | validation: 0.2111238646914813]
	TIME [epoch: 11.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18704471659111305		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.18704471659111305 | validation: 0.17653767467956502]
	TIME [epoch: 11.7 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22482718047773712		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.22482718047773712 | validation: 0.23382264200401187]
	TIME [epoch: 11.7 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1605848870666767		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.1605848870666767 | validation: 0.18245214329160603]
	TIME [epoch: 11.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19008673494577016		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.19008673494577016 | validation: 0.15985206589758916]
	TIME [epoch: 11.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2066921383083205		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.2066921383083205 | validation: 0.19965122666442064]
	TIME [epoch: 11.7 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19644624072377953		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.19644624072377953 | validation: 0.19784897217789627]
	TIME [epoch: 11.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2011339752540773		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.2011339752540773 | validation: 0.24917607402134143]
	TIME [epoch: 11.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18660567344296075		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.18660567344296075 | validation: 0.18441418452498706]
	TIME [epoch: 11.7 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19994446876717356		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.19994446876717356 | validation: 0.23876893521637332]
	TIME [epoch: 11.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17539805776279604		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.17539805776279604 | validation: 0.24713741056815197]
	TIME [epoch: 11.7 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18118229694834148		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.18118229694834148 | validation: 0.1466759451998635]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.196572199895719		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.196572199895719 | validation: 0.26929627338157286]
	TIME [epoch: 11.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19233595623294758		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.19233595623294758 | validation: 0.1618722789173564]
	TIME [epoch: 11.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19806232497071172		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.19806232497071172 | validation: 0.2592662467176416]
	TIME [epoch: 11.7 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18448810469753424		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.18448810469753424 | validation: 0.251644094732917]
	TIME [epoch: 11.7 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18705556777123283		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.18705556777123283 | validation: 0.1818181100986426]
	TIME [epoch: 11.7 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18949095891755338		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.18949095891755338 | validation: 0.2018978039277433]
	TIME [epoch: 11.7 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19766668960202619		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.19766668960202619 | validation: 0.22532590381403744]
	TIME [epoch: 11.7 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22241243856187692		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.22241243856187692 | validation: 0.19875808404840684]
	TIME [epoch: 11.7 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17489556819555374		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.17489556819555374 | validation: 0.2108299686268238]
	TIME [epoch: 11.7 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15841461953030422		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.15841461953030422 | validation: 0.2169634252016283]
	TIME [epoch: 11.7 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20513495269256812		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.20513495269256812 | validation: 0.3195181073680505]
	TIME [epoch: 11.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19568066842822318		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.19568066842822318 | validation: 0.21862872982024548]
	TIME [epoch: 11.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19473384468073557		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.19473384468073557 | validation: 0.2842328095667706]
	TIME [epoch: 11.7 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17654660480469275		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.17654660480469275 | validation: 0.28563638798768065]
	TIME [epoch: 11.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20349644048071508		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.20349644048071508 | validation: 0.16765274413530445]
	TIME [epoch: 11.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19016524315056915		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.19016524315056915 | validation: 0.1857674524875626]
	TIME [epoch: 11.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1799926368726415		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.1799926368726415 | validation: 0.22316437412315748]
	TIME [epoch: 11.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1921980124150361		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.1921980124150361 | validation: 0.23096290647655537]
	TIME [epoch: 11.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1759127154016664		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.1759127154016664 | validation: 0.1661034506420389]
	TIME [epoch: 11.7 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16903011679894714		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.16903011679894714 | validation: 0.15533238870301025]
	TIME [epoch: 11.8 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16988513729946103		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.16988513729946103 | validation: 0.21055722489402295]
	TIME [epoch: 11.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15724795679412404		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.15724795679412404 | validation: 0.25849804835914547]
	TIME [epoch: 11.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24446439182975094		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.24446439182975094 | validation: 0.34155617979737396]
	TIME [epoch: 11.7 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2202166920096292		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.2202166920096292 | validation: 0.1596537485831115]
	TIME [epoch: 11.7 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16534365892870176		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.16534365892870176 | validation: 0.17506749660939463]
	TIME [epoch: 11.7 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1593871022738577		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.1593871022738577 | validation: 0.1866225621696623]
	TIME [epoch: 11.7 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20417758572345285		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.20417758572345285 | validation: 0.19100893893069829]
	TIME [epoch: 11.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14913153021748032		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.14913153021748032 | validation: 0.20783998926513392]
	TIME [epoch: 11.7 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19013458298715888		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.19013458298715888 | validation: 0.195738421251758]
	TIME [epoch: 11.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1987223519165614		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.1987223519165614 | validation: 0.22896104368890557]
	TIME [epoch: 11.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17405739195330752		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.17405739195330752 | validation: 0.25860558965471253]
	TIME [epoch: 11.7 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16673761579800436		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.16673761579800436 | validation: 0.2179548285525036]
	TIME [epoch: 11.7 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18729449645643403		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.18729449645643403 | validation: 0.19500389888577802]
	TIME [epoch: 11.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16303413087549662		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.16303413087549662 | validation: 0.18315885921162633]
	TIME [epoch: 11.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14874214407396685		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.14874214407396685 | validation: 0.18692878708030317]
	TIME [epoch: 11.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17139139706985557		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.17139139706985557 | validation: 0.1780604146439384]
	TIME [epoch: 11.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1522182397578601		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.1522182397578601 | validation: 0.16676343435349308]
	TIME [epoch: 11.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.170784413193274		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.170784413193274 | validation: 0.24534528216758175]
	TIME [epoch: 11.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1675476804161654		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.1675476804161654 | validation: 0.17602189775643678]
	TIME [epoch: 11.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16453256089830892		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.16453256089830892 | validation: 0.18217413076818478]
	TIME [epoch: 11.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17468377711429878		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.17468377711429878 | validation: 0.21844722995513227]
	TIME [epoch: 11.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1552911152337833		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.1552911152337833 | validation: 0.14744971495760384]
	TIME [epoch: 11.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1231364048780191		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.1231364048780191 | validation: 0.21619639218942327]
	TIME [epoch: 11.7 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1504629735586774		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.1504629735586774 | validation: 0.25673716290995013]
	TIME [epoch: 11.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21525302197764373		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.21525302197764373 | validation: 0.12683550639575292]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15202843757440757		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.15202843757440757 | validation: 0.1881556908592643]
	TIME [epoch: 11.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14203261393288152		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.14203261393288152 | validation: 0.21917327345247434]
	TIME [epoch: 11.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16505443612062817		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.16505443612062817 | validation: 0.17442677670024925]
	TIME [epoch: 11.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15933917820065494		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.15933917820065494 | validation: 0.1950801680955473]
	TIME [epoch: 11.7 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1438319166158085		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.1438319166158085 | validation: 0.23761946117009802]
	TIME [epoch: 11.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17612587479946762		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.17612587479946762 | validation: 0.19275018682375578]
	TIME [epoch: 11.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13728839035040985		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.13728839035040985 | validation: 0.23365212293349463]
	TIME [epoch: 11.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1525466548615807		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.1525466548615807 | validation: 0.21182646061935256]
	TIME [epoch: 11.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16436870657442204		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.16436870657442204 | validation: 0.15210282478359533]
	TIME [epoch: 11.7 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17200216234291954		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.17200216234291954 | validation: 0.17148975733809657]
	TIME [epoch: 11.7 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1294320914072006		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.1294320914072006 | validation: 0.1592879198383219]
	TIME [epoch: 11.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15732189136695857		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.15732189136695857 | validation: 0.14661587431598355]
	TIME [epoch: 11.7 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14538380770879406		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.14538380770879406 | validation: 0.17894452839393654]
	TIME [epoch: 11.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14023721722594068		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.14023721722594068 | validation: 0.14093115611663526]
	TIME [epoch: 11.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1647691943747146		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.1647691943747146 | validation: 0.14486292184874777]
	TIME [epoch: 11.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14828923819565895		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.14828923819565895 | validation: 0.15916181124011441]
	TIME [epoch: 11.7 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1586376803175224		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.1586376803175224 | validation: 0.15328299365409892]
	TIME [epoch: 11.7 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14450704770136344		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.14450704770136344 | validation: 0.15410362384006032]
	TIME [epoch: 11.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1503661573667291		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.1503661573667291 | validation: 0.1431075794920197]
	TIME [epoch: 11.7 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16355558750419047		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.16355558750419047 | validation: 0.19480802425262433]
	TIME [epoch: 11.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14879760218120808		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.14879760218120808 | validation: 0.1490369141472501]
	TIME [epoch: 11.7 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16208387600078733		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.16208387600078733 | validation: 0.1541094629230604]
	TIME [epoch: 11.7 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14360583204308539		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.14360583204308539 | validation: 0.24279160871795014]
	TIME [epoch: 11.7 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1608734203116052		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.1608734203116052 | validation: 0.16601144708020432]
	TIME [epoch: 11.7 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14447513814497193		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.14447513814497193 | validation: 0.16929996935924108]
	TIME [epoch: 11.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1324603311511795		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.1324603311511795 | validation: 0.22575345892700238]
	TIME [epoch: 11.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17400892307949836		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.17400892307949836 | validation: 0.1442398563203921]
	TIME [epoch: 11.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14896835084639637		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.14896835084639637 | validation: 0.16873714960759495]
	TIME [epoch: 11.7 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1485095075678479		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.1485095075678479 | validation: 0.14644413480827395]
	TIME [epoch: 11.7 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13253592537795506		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.13253592537795506 | validation: 0.16150418301224892]
	TIME [epoch: 11.7 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16309971898045877		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.16309971898045877 | validation: 0.13740353725320276]
	TIME [epoch: 11.7 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11973980416002841		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.11973980416002841 | validation: 0.2080014427265336]
	TIME [epoch: 11.7 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1851288022056326		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.1851288022056326 | validation: 0.1735460737466173]
	TIME [epoch: 11.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14833143036474836		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.14833143036474836 | validation: 0.1379783855336107]
	TIME [epoch: 11.7 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1417256220254593		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.1417256220254593 | validation: 0.15085925864187782]
	TIME [epoch: 11.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1350399172940704		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.1350399172940704 | validation: 0.15098255366483454]
	TIME [epoch: 11.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15826893156975463		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.15826893156975463 | validation: 0.16169942916115676]
	TIME [epoch: 11.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14315815155297232		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.14315815155297232 | validation: 0.15317135523175457]
	TIME [epoch: 11.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1418560386313784		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.1418560386313784 | validation: 0.17350958771788705]
	TIME [epoch: 11.7 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13605538714395807		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.13605538714395807 | validation: 0.14652081211803694]
	TIME [epoch: 11.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15126250945243375		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.15126250945243375 | validation: 0.14518634681185322]
	TIME [epoch: 11.7 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1558548422978606		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.1558548422978606 | validation: 0.20074211913279388]
	TIME [epoch: 11.7 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13504368560589974		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.13504368560589974 | validation: 0.12971295246836825]
	TIME [epoch: 11.7 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14029398949822952		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.14029398949822952 | validation: 0.2426315962486098]
	TIME [epoch: 11.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14546083788566397		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.14546083788566397 | validation: 0.1402205851316165]
	TIME [epoch: 11.7 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488613803190854		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.1488613803190854 | validation: 0.1131993462948052]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11865683098318398		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.11865683098318398 | validation: 0.1400390587109372]
	TIME [epoch: 11.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14132874035591286		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.14132874035591286 | validation: 0.2193902463806804]
	TIME [epoch: 11.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14863679376890465		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.14863679376890465 | validation: 0.14520973952132157]
	TIME [epoch: 11.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14755181382166987		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.14755181382166987 | validation: 0.18635077672967232]
	TIME [epoch: 11.7 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1551218743782407		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.1551218743782407 | validation: 0.19279555831032175]
	TIME [epoch: 11.7 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13192557669363875		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.13192557669363875 | validation: 0.23654858208474838]
	TIME [epoch: 11.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1366915205609779		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.1366915205609779 | validation: 0.13961213724856403]
	TIME [epoch: 11.7 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15154561568460606		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.15154561568460606 | validation: 0.16958692521655577]
	TIME [epoch: 11.7 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13455443692960525		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.13455443692960525 | validation: 0.2012227463133305]
	TIME [epoch: 11.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13345370129287562		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.13345370129287562 | validation: 0.1509075301566526]
	TIME [epoch: 11.9 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13853778487475354		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.13853778487475354 | validation: 0.1329824022145648]
	TIME [epoch: 11.7 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1286104584454669		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.1286104584454669 | validation: 0.13218916524864155]
	TIME [epoch: 11.7 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13672380236212292		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.13672380236212292 | validation: 0.17848350801127816]
	TIME [epoch: 11.7 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1351928347289441		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.1351928347289441 | validation: 0.13959306352043616]
	TIME [epoch: 11.7 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17613397319445842		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.17613397319445842 | validation: 0.1493970782120322]
	TIME [epoch: 11.7 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12706028157436614		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.12706028157436614 | validation: 0.1310915043031159]
	TIME [epoch: 11.7 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13693863583415494		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.13693863583415494 | validation: 0.18421286448947619]
	TIME [epoch: 11.7 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14399386493031066		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.14399386493031066 | validation: 0.16583786339714812]
	TIME [epoch: 11.7 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12280135187776714		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.12280135187776714 | validation: 0.1375721785038769]
	TIME [epoch: 11.7 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12195023745800218		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.12195023745800218 | validation: 0.22512022373801527]
	TIME [epoch: 11.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16932260418592998		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.16932260418592998 | validation: 0.22286843343117446]
	TIME [epoch: 11.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1629971990397416		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.1629971990397416 | validation: 0.25031087147170195]
	TIME [epoch: 11.7 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15595337724184272		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.15595337724184272 | validation: 0.1621341975199122]
	TIME [epoch: 11.7 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13209750473572499		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.13209750473572499 | validation: 0.12836301949935525]
	TIME [epoch: 11.7 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13370918830070144		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.13370918830070144 | validation: 0.13757529722746387]
	TIME [epoch: 11.7 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13494074344582127		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.13494074344582127 | validation: 0.19489214495099688]
	TIME [epoch: 11.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13967975237792604		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.13967975237792604 | validation: 0.13851005070852707]
	TIME [epoch: 11.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14691414150391507		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.14691414150391507 | validation: 0.1257494294171394]
	TIME [epoch: 11.7 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14151140988245126		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.14151140988245126 | validation: 0.117849534535228]
	TIME [epoch: 11.7 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1261332744100898		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.1261332744100898 | validation: 0.13672051206775215]
	TIME [epoch: 11.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11235685323846116		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.11235685323846116 | validation: 0.123029897522368]
	TIME [epoch: 11.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1275031896481928		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.1275031896481928 | validation: 0.15666943478240003]
	TIME [epoch: 11.7 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12844274980649387		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.12844274980649387 | validation: 0.12038896964375018]
	TIME [epoch: 11.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13718400307219164		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.13718400307219164 | validation: 0.1451401214476824]
	TIME [epoch: 11.7 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1333556713564713		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.1333556713564713 | validation: 0.16553313645354453]
	TIME [epoch: 11.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12226572489438169		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.12226572489438169 | validation: 0.11207347187051328]
	TIME [epoch: 11.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10347391368965045		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.10347391368965045 | validation: 0.18550265302956293]
	TIME [epoch: 11.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12541593357084832		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.12541593357084832 | validation: 0.1286720048845886]
	TIME [epoch: 11.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15129573616975422		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.15129573616975422 | validation: 0.13475658726093642]
	TIME [epoch: 11.7 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12165998734930099		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.12165998734930099 | validation: 0.12311087972288587]
	TIME [epoch: 11.7 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11180108604766956		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.11180108604766956 | validation: 0.13315617833533444]
	TIME [epoch: 11.7 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11505676177899975		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.11505676177899975 | validation: 0.1172271163412009]
	TIME [epoch: 11.7 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14028088613885847		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.14028088613885847 | validation: 0.13335430721249986]
	TIME [epoch: 11.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12193020459034565		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.12193020459034565 | validation: 0.12308735543681568]
	TIME [epoch: 11.7 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10413476509820463		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.10413476509820463 | validation: 0.12656826305827795]
	TIME [epoch: 11.7 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1614164923085655		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.1614164923085655 | validation: 0.12785303091019073]
	TIME [epoch: 11.7 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14374252964203338		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.14374252964203338 | validation: 0.12057134016369096]
	TIME [epoch: 11.7 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11802216790072409		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.11802216790072409 | validation: 0.1715240864386125]
	TIME [epoch: 11.7 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12523078207792832		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.12523078207792832 | validation: 0.1402884533187489]
	TIME [epoch: 11.7 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13954470725452345		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.13954470725452345 | validation: 0.11760819440411634]
	TIME [epoch: 11.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10757907713742079		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.10757907713742079 | validation: 0.14852308964515473]
	TIME [epoch: 11.7 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11577534015267474		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.11577534015267474 | validation: 0.12595167629962845]
	TIME [epoch: 11.7 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1362433057416792		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.1362433057416792 | validation: 0.1317038943383267]
	TIME [epoch: 403 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10438594687264575		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.10438594687264575 | validation: 0.13869299808788627]
	TIME [epoch: 25.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13495838582304623		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.13495838582304623 | validation: 0.12703939407093615]
	TIME [epoch: 25.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12405574165831354		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.12405574165831354 | validation: 0.11949388790660945]
	TIME [epoch: 25.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12595880452353933		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.12595880452353933 | validation: 0.12887711784366163]
	TIME [epoch: 25.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10366634696092195		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.10366634696092195 | validation: 0.13015899987184928]
	TIME [epoch: 25.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13178107751205928		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.13178107751205928 | validation: 0.11848097981357833]
	TIME [epoch: 25.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12164060141667785		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.12164060141667785 | validation: 0.18401613664733701]
	TIME [epoch: 25.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11048137138959271		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.11048137138959271 | validation: 0.1279227369757589]
	TIME [epoch: 25.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1271269622896005		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.1271269622896005 | validation: 0.13589480374110346]
	TIME [epoch: 25.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13206540879484419		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.13206540879484419 | validation: 0.20715610291384287]
	TIME [epoch: 25.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12429898173634478		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.12429898173634478 | validation: 0.15318408032243008]
	TIME [epoch: 25.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11319445199196396		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.11319445199196396 | validation: 0.11945798075244735]
	TIME [epoch: 25.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13899998951579803		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.13899998951579803 | validation: 0.13207962486397032]
	TIME [epoch: 25.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1276276433429308		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.1276276433429308 | validation: 0.11435618187900191]
	TIME [epoch: 25.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11287600495399641		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.11287600495399641 | validation: 0.12543798815807572]
	TIME [epoch: 25.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11245182309246585		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.11245182309246585 | validation: 0.12106539171345843]
	TIME [epoch: 25.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11667188740418438		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.11667188740418438 | validation: 0.21098688726368597]
	TIME [epoch: 25.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1270608894064698		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.1270608894064698 | validation: 0.14663438287241587]
	TIME [epoch: 25.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12058024871517124		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.12058024871517124 | validation: 0.12503819248973314]
	TIME [epoch: 25.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12195868190406428		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.12195868190406428 | validation: 0.1951020126806125]
	TIME [epoch: 25.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11677485382374025		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.11677485382374025 | validation: 0.14126352772716588]
	TIME [epoch: 25.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11498185768544715		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.11498185768544715 | validation: 0.1394137387998004]
	TIME [epoch: 25.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11080746112698946		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.11080746112698946 | validation: 0.13176871001893573]
	TIME [epoch: 25.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12272164431508673		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.12272164431508673 | validation: 0.11676289978856198]
	TIME [epoch: 25.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10413167841589964		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.10413167841589964 | validation: 0.13383207622571944]
	TIME [epoch: 25.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13020451362405822		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.13020451362405822 | validation: 0.1540946269004813]
	TIME [epoch: 25.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12242670642165168		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.12242670642165168 | validation: 0.14050557172804296]
	TIME [epoch: 25.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11838814810922546		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.11838814810922546 | validation: 0.13062369772140597]
	TIME [epoch: 25.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10989254211127848		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.10989254211127848 | validation: 0.11630350592991238]
	TIME [epoch: 25.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10994495235913004		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.10994495235913004 | validation: 0.17026681451685793]
	TIME [epoch: 25.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12071982392723535		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.12071982392723535 | validation: 0.16384921540982472]
	TIME [epoch: 25.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12619472477611987		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.12619472477611987 | validation: 0.12977119810481352]
	TIME [epoch: 25.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11169482204901124		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.11169482204901124 | validation: 0.11662524460864661]
	TIME [epoch: 25.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10863877933194457		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.10863877933194457 | validation: 0.11499129983702258]
	TIME [epoch: 25.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11105750034913488		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.11105750034913488 | validation: 0.1352469186287255]
	TIME [epoch: 25.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10587932017112664		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.10587932017112664 | validation: 0.17877150267023553]
	TIME [epoch: 25.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12000869027833488		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.12000869027833488 | validation: 0.11586772881514106]
	TIME [epoch: 25.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14961622247586467		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.14961622247586467 | validation: 0.1347522686653434]
	TIME [epoch: 25.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11647052865033107		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.11647052865033107 | validation: 0.11403371847513016]
	TIME [epoch: 25.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10753466771504068		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.10753466771504068 | validation: 0.1440573751801989]
	TIME [epoch: 25.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10631998434226105		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.10631998434226105 | validation: 0.10947182775655842]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11857919499502836		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.11857919499502836 | validation: 0.11186844663789752]
	TIME [epoch: 25.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10903498904422121		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.10903498904422121 | validation: 0.15777145352391447]
	TIME [epoch: 25.2 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11564435547978054		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.11564435547978054 | validation: 0.1288910387825547]
	TIME [epoch: 25.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10442403801531557		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.10442403801531557 | validation: 0.11756030015629791]
	TIME [epoch: 25.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10341345032557259		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.10341345032557259 | validation: 0.11355504188005267]
	TIME [epoch: 25.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10189035760712997		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.10189035760712997 | validation: 0.13934328949567082]
	TIME [epoch: 25.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11861297307015484		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.11861297307015484 | validation: 0.11703590697553728]
	TIME [epoch: 25.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11686440295924244		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.11686440295924244 | validation: 0.13089810406795177]
	TIME [epoch: 25.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09608747503731184		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.09608747503731184 | validation: 0.1667710630654869]
	TIME [epoch: 25.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11212591358149643		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.11212591358149643 | validation: 0.13545319989363475]
	TIME [epoch: 25.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11037090410363995		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.11037090410363995 | validation: 0.11907639443589496]
	TIME [epoch: 25.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11579092314654055		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.11579092314654055 | validation: 0.13294863475033109]
	TIME [epoch: 25.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11720554401944364		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.11720554401944364 | validation: 0.12328486108345196]
	TIME [epoch: 25.2 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09218941793725673		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.09218941793725673 | validation: 0.12236068660882218]
	TIME [epoch: 25.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11283091337646131		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.11283091337646131 | validation: 0.18087685421464303]
	TIME [epoch: 25.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12126179794223135		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.12126179794223135 | validation: 0.13674117444957873]
	TIME [epoch: 25.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10219070142607602		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.10219070142607602 | validation: 0.12610811023500268]
	TIME [epoch: 25.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09900587652277618		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.09900587652277618 | validation: 0.12884331500615964]
	TIME [epoch: 25.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09814503655071191		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.09814503655071191 | validation: 0.1330013379622994]
	TIME [epoch: 25.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10813732091344433		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.10813732091344433 | validation: 0.1220135429422421]
	TIME [epoch: 25.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11475445454062311		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.11475445454062311 | validation: 0.0973640421353287]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09624877392953246		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.09624877392953246 | validation: 0.12431125232245]
	TIME [epoch: 25.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12275897031300043		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.12275897031300043 | validation: 0.11884753781004184]
	TIME [epoch: 25.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09827559883729912		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.09827559883729912 | validation: 0.12002070728354203]
	TIME [epoch: 25.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09852265835562932		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.09852265835562932 | validation: 0.14764923859880047]
	TIME [epoch: 25.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11081303963381417		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.11081303963381417 | validation: 0.15810250101340861]
	TIME [epoch: 25.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1023319781438207		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.1023319781438207 | validation: 0.11663593635844151]
	TIME [epoch: 25.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10318412850505408		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.10318412850505408 | validation: 0.14603803558469197]
	TIME [epoch: 25.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11408892734369742		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.11408892734369742 | validation: 0.12387693300332779]
	TIME [epoch: 25.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10385661495978776		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.10385661495978776 | validation: 0.15643078577034716]
	TIME [epoch: 25.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10995829187530161		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.10995829187530161 | validation: 0.09634507258675509]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09469777138327654		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.09469777138327654 | validation: 0.11814487816325803]
	TIME [epoch: 25.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12930759043884615		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.12930759043884615 | validation: 0.10786207588057956]
	TIME [epoch: 25.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09722692504097123		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.09722692504097123 | validation: 0.130049154162839]
	TIME [epoch: 25.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1094578113717239		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.1094578113717239 | validation: 0.11975336857960435]
	TIME [epoch: 25.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10746075048946209		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.10746075048946209 | validation: 0.15295200130033665]
	TIME [epoch: 25.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10358522154026703		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.10358522154026703 | validation: 0.10703105418694944]
	TIME [epoch: 25.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0933479258912427		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.0933479258912427 | validation: 0.11218818655802339]
	TIME [epoch: 25.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10573420356613422		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.10573420356613422 | validation: 0.14767528756641798]
	TIME [epoch: 25.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10111967587748164		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.10111967587748164 | validation: 0.11978274589321752]
	TIME [epoch: 25.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1101305099637414		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.1101305099637414 | validation: 0.12255361724792935]
	TIME [epoch: 25.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10477986095290936		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.10477986095290936 | validation: 0.12077750964705547]
	TIME [epoch: 25.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09484279915193254		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.09484279915193254 | validation: 0.14109087372535112]
	TIME [epoch: 25.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10485164701077451		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.10485164701077451 | validation: 0.11635916394669105]
	TIME [epoch: 25.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09915625354087246		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.09915625354087246 | validation: 0.15274295366104623]
	TIME [epoch: 25.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11368147504264842		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.11368147504264842 | validation: 0.12629041117527082]
	TIME [epoch: 25.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10437875796698477		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.10437875796698477 | validation: 0.11005235739840935]
	TIME [epoch: 25.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1125261424274478		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.1125261424274478 | validation: 0.11723140870952128]
	TIME [epoch: 25.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09869780073670242		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.09869780073670242 | validation: 0.10700593003054298]
	TIME [epoch: 25.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08942412727575796		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.08942412727575796 | validation: 0.11501417679995601]
	TIME [epoch: 25.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10422697208545864		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.10422697208545864 | validation: 0.11755620459201216]
	TIME [epoch: 25.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09758402979949593		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.09758402979949593 | validation: 0.13035394237466175]
	TIME [epoch: 25.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10329939317596813		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.10329939317596813 | validation: 0.1669011313072656]
	TIME [epoch: 25.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1114534502831214		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.1114534502831214 | validation: 0.11695614050926814]
	TIME [epoch: 25.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0889237484223033		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.0889237484223033 | validation: 0.11438119036032017]
	TIME [epoch: 25.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09841080468121877		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.09841080468121877 | validation: 0.11550057276826403]
	TIME [epoch: 25.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09471879743554545		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.09471879743554545 | validation: 0.13548430565916106]
	TIME [epoch: 25.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11121071240373992		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.11121071240373992 | validation: 0.11795982581021532]
	TIME [epoch: 25.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10203228599557833		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.10203228599557833 | validation: 0.1179461471650877]
	TIME [epoch: 25.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10090907245586422		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.10090907245586422 | validation: 0.1028353359712375]
	TIME [epoch: 25.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09439312418518725		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.09439312418518725 | validation: 0.1079553854878261]
	TIME [epoch: 25.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10030725505667645		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.10030725505667645 | validation: 0.099439691083221]
	TIME [epoch: 25.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09326833060583642		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.09326833060583642 | validation: 0.110475000087114]
	TIME [epoch: 25.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10045904000554948		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.10045904000554948 | validation: 0.1062074938227287]
	TIME [epoch: 25.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0914827412773407		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0914827412773407 | validation: 0.10095968762652742]
	TIME [epoch: 25.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10292464647814423		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.10292464647814423 | validation: 0.12334078243578854]
	TIME [epoch: 25.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10697445344868937		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.10697445344868937 | validation: 0.10968189747424023]
	TIME [epoch: 25.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08995750231374239		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.08995750231374239 | validation: 0.12677126972092373]
	TIME [epoch: 25.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10427755048553893		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.10427755048553893 | validation: 0.14009360492397643]
	TIME [epoch: 25.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10325773477090046		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.10325773477090046 | validation: 0.1115266814293631]
	TIME [epoch: 25.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11328248419484162		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.11328248419484162 | validation: 0.1480483186583405]
	TIME [epoch: 25.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09873954642486445		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.09873954642486445 | validation: 0.09723553689030665]
	TIME [epoch: 25.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09061032594856022		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.09061032594856022 | validation: 0.14175989048620402]
	TIME [epoch: 25.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10883884801870394		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.10883884801870394 | validation: 0.1384899072010679]
	TIME [epoch: 25.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10197064566405967		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.10197064566405967 | validation: 0.09562302720286801]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08573865695636146		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.08573865695636146 | validation: 0.0923189882775021]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09290697521679148		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.09290697521679148 | validation: 0.1269008410813804]
	TIME [epoch: 25.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10788150284357356		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.10788150284357356 | validation: 0.1082908618199267]
	TIME [epoch: 25.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08441932169655436		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.08441932169655436 | validation: 0.10603177216971411]
	TIME [epoch: 25.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10562669663013875		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.10562669663013875 | validation: 0.10990111343221565]
	TIME [epoch: 25.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09244132757010834		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.09244132757010834 | validation: 0.09574870104959834]
	TIME [epoch: 25.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10724017401981537		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.10724017401981537 | validation: 0.10658661672754854]
	TIME [epoch: 25.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09132613017442465		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.09132613017442465 | validation: 0.10017898073255313]
	TIME [epoch: 25.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10102781921695825		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.10102781921695825 | validation: 0.10461927452709174]
	TIME [epoch: 25.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09615073497591026		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.09615073497591026 | validation: 0.09928015308744953]
	TIME [epoch: 25.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09272580788567565		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.09272580788567565 | validation: 0.10670347866271268]
	TIME [epoch: 25.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08563515217851568		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.08563515217851568 | validation: 0.11451154406379481]
	TIME [epoch: 25.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10746871502168692		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.10746871502168692 | validation: 0.10370002312247381]
	TIME [epoch: 25.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09764911322937868		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.09764911322937868 | validation: 0.1026179305742396]
	TIME [epoch: 25.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09418748637089279		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.09418748637089279 | validation: 0.1144977303915633]
	TIME [epoch: 25.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10681367458401601		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.10681367458401601 | validation: 0.12216607148204485]
	TIME [epoch: 25.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0944794696551587		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0944794696551587 | validation: 0.10232479071144668]
	TIME [epoch: 25.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09320215391434833		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.09320215391434833 | validation: 0.10040574180623686]
	TIME [epoch: 25.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0891552707883295		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.0891552707883295 | validation: 0.09846942622883231]
	TIME [epoch: 25.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08702920591846217		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.08702920591846217 | validation: 0.10498470609089475]
	TIME [epoch: 25.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09947637974061346		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.09947637974061346 | validation: 0.10380098570415115]
	TIME [epoch: 25.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08972122166934798		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.08972122166934798 | validation: 0.09938976973797416]
	TIME [epoch: 25.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10080360184528925		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.10080360184528925 | validation: 0.11817194385251151]
	TIME [epoch: 25.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08642293071870283		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.08642293071870283 | validation: 0.09950724814747773]
	TIME [epoch: 25.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08772490911375871		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.08772490911375871 | validation: 0.10682123220876222]
	TIME [epoch: 25.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08963829849408864		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.08963829849408864 | validation: 0.1049261156466011]
	TIME [epoch: 25.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0849020614268983		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.0849020614268983 | validation: 0.12176845505736877]
	TIME [epoch: 25.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08392534486139318		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.08392534486139318 | validation: 0.12807055778734366]
	TIME [epoch: 25.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1025216221989207		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.1025216221989207 | validation: 0.10607942017074383]
	TIME [epoch: 25.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10121736653225452		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.10121736653225452 | validation: 0.09414524023005894]
	TIME [epoch: 25.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08471753955955387		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.08471753955955387 | validation: 0.12168402456541338]
	TIME [epoch: 25.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09269570336378612		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.09269570336378612 | validation: 0.10802337774380483]
	TIME [epoch: 25.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08441833649405063		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.08441833649405063 | validation: 0.10571919172778]
	TIME [epoch: 25.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09371406989263831		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.09371406989263831 | validation: 0.15637806414478056]
	TIME [epoch: 25.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09739101849273571		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.09739101849273571 | validation: 0.08982355749300344]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08220937615291478		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.08220937615291478 | validation: 0.10055790644229139]
	TIME [epoch: 25.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09158653768375524		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.09158653768375524 | validation: 0.11163449999649219]
	TIME [epoch: 25.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09087527368097142		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.09087527368097142 | validation: 0.1087017052722756]
	TIME [epoch: 25.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09016876618687607		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.09016876618687607 | validation: 0.11144279241202182]
	TIME [epoch: 25.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0978119419846469		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.0978119419846469 | validation: 0.09056155411031903]
	TIME [epoch: 25.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.085687298682693		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.085687298682693 | validation: 0.09772336100316852]
	TIME [epoch: 25.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09334464979516782		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.09334464979516782 | validation: 0.09802470998054802]
	TIME [epoch: 25.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09126613469915729		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.09126613469915729 | validation: 0.09718672218164856]
	TIME [epoch: 25.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08974520562759815		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.08974520562759815 | validation: 0.09948815381518655]
	TIME [epoch: 25.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09198976436996917		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.09198976436996917 | validation: 0.10440888895332961]
	TIME [epoch: 25.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09060776506978271		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.09060776506978271 | validation: 0.09758127106057807]
	TIME [epoch: 25.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08473749299762828		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.08473749299762828 | validation: 0.0973571449045362]
	TIME [epoch: 25.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0862053696214177		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.0862053696214177 | validation: 0.09380340028632217]
	TIME [epoch: 25.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07965170937852978		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.07965170937852978 | validation: 0.09744323131780369]
	TIME [epoch: 25.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09081369440963105		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.09081369440963105 | validation: 0.18704884597897795]
	TIME [epoch: 25.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10157856365583971		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.10157856365583971 | validation: 0.12486077059156822]
	TIME [epoch: 25.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08684006938583438		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.08684006938583438 | validation: 0.10129614745699062]
	TIME [epoch: 25.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0817067070913891		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.0817067070913891 | validation: 0.0917596502017366]
	TIME [epoch: 25.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08618736650499542		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.08618736650499542 | validation: 0.09730892415445966]
	TIME [epoch: 25.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08715476752976573		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.08715476752976573 | validation: 0.12343864076455677]
	TIME [epoch: 25.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09374742338230174		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.09374742338230174 | validation: 0.13261302904707922]
	TIME [epoch: 25.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09252338546700549		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.09252338546700549 | validation: 0.10265011875276472]
	TIME [epoch: 25.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09069047645086245		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.09069047645086245 | validation: 0.0910270059137606]
	TIME [epoch: 25.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08942848477629492		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.08942848477629492 | validation: 0.10912034414115707]
	TIME [epoch: 25.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08449404982314904		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.08449404982314904 | validation: 0.12770040813071049]
	TIME [epoch: 25.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08601627229021136		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.08601627229021136 | validation: 0.11001232084595913]
	TIME [epoch: 25.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09214393212772215		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.09214393212772215 | validation: 0.09263836477558605]
	TIME [epoch: 25.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08632517932111666		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.08632517932111666 | validation: 0.08830091934128903]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08870785878635196		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.08870785878635196 | validation: 0.10227775456260615]
	TIME [epoch: 25.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08036578356513241		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.08036578356513241 | validation: 0.10676547058280618]
	TIME [epoch: 25.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09232173464794391		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.09232173464794391 | validation: 0.0926923729316542]
	TIME [epoch: 25.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08118776247769753		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.08118776247769753 | validation: 0.1356094147683363]
	TIME [epoch: 25.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09529010374913216		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.09529010374913216 | validation: 0.09891112315315859]
	TIME [epoch: 25.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08390075484861709		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.08390075484861709 | validation: 0.10747444922261093]
	TIME [epoch: 25.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09353449710604686		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.09353449710604686 | validation: 0.10752741345293287]
	TIME [epoch: 25.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.083715769823291		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.083715769823291 | validation: 0.09907377085122718]
	TIME [epoch: 25.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07525699470798358		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.07525699470798358 | validation: 0.09506417969478688]
	TIME [epoch: 25.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08512318368989816		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.08512318368989816 | validation: 0.09946802474300234]
	TIME [epoch: 25.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08414697581351177		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.08414697581351177 | validation: 0.1534308913152232]
	TIME [epoch: 25.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10220565640177526		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.10220565640177526 | validation: 0.1087875784952595]
	TIME [epoch: 25.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09164977801483275		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.09164977801483275 | validation: 0.11755264556893325]
	TIME [epoch: 25.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0800987012354102		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.0800987012354102 | validation: 0.08917217440238459]
	TIME [epoch: 25.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08654365364520647		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.08654365364520647 | validation: 0.09691600785990247]
	TIME [epoch: 25.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08361775051931011		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.08361775051931011 | validation: 0.10707820679145892]
	TIME [epoch: 25.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08164070334310353		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.08164070334310353 | validation: 0.10929659154642021]
	TIME [epoch: 25.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08972345244366152		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.08972345244366152 | validation: 0.12293652095135862]
	TIME [epoch: 25.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09343940298776049		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.09343940298776049 | validation: 0.09509027530581837]
	TIME [epoch: 25.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09126135899998523		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.09126135899998523 | validation: 0.09728638984459534]
	TIME [epoch: 25.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09178305470942055		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.09178305470942055 | validation: 0.12806133736969758]
	TIME [epoch: 25.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10049897331829134		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.10049897331829134 | validation: 0.0961335812470191]
	TIME [epoch: 25.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1008645146070515		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.1008645146070515 | validation: 0.09635859593456822]
	TIME [epoch: 25.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07928338018573494		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.07928338018573494 | validation: 0.08689502086485565]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08130712000689612		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.08130712000689612 | validation: 0.08347227651698526]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07949734035638929		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.07949734035638929 | validation: 0.09031956957891417]
	TIME [epoch: 25.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08664318998771262		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.08664318998771262 | validation: 0.10332292900990994]
	TIME [epoch: 25.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08690175351815707		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.08690175351815707 | validation: 0.11568709893550898]
	TIME [epoch: 25.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0855091880353416		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0855091880353416 | validation: 0.08663430393372232]
	TIME [epoch: 25.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08611881099482782		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.08611881099482782 | validation: 0.10812656657392247]
	TIME [epoch: 25.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08264060506775382		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.08264060506775382 | validation: 0.08572086974002022]
	TIME [epoch: 25.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07947701088820117		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.07947701088820117 | validation: 0.11567967375923845]
	TIME [epoch: 25.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08914539015726541		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.08914539015726541 | validation: 0.1384894172668547]
	TIME [epoch: 25.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08704819884770333		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.08704819884770333 | validation: 0.08272070396773047]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07686532928457597		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.07686532928457597 | validation: 0.09095103916544012]
	TIME [epoch: 25.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08607887316329928		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.08607887316329928 | validation: 0.09335231438199317]
	TIME [epoch: 25.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08032256737339244		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.08032256737339244 | validation: 0.09068927449371898]
	TIME [epoch: 25.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08727784926629477		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.08727784926629477 | validation: 0.1073206608186057]
	TIME [epoch: 25.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0894549170446067		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0894549170446067 | validation: 0.0996429281144941]
	TIME [epoch: 25.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07787207780510091		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.07787207780510091 | validation: 0.09298671169927536]
	TIME [epoch: 25.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08245126494835531		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.08245126494835531 | validation: 0.09882171590037464]
	TIME [epoch: 25.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07879962039267141		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.07879962039267141 | validation: 0.11282774999493728]
	TIME [epoch: 25.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08591214008563802		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.08591214008563802 | validation: 0.1109144515510462]
	TIME [epoch: 25.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08740843508142308		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.08740843508142308 | validation: 0.10040902650326619]
	TIME [epoch: 25.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07968808030546598		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.07968808030546598 | validation: 0.1040393554942225]
	TIME [epoch: 25.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08082628428156795		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.08082628428156795 | validation: 0.08733069702744098]
	TIME [epoch: 25.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08084479715953923		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.08084479715953923 | validation: 0.0986060882898585]
	TIME [epoch: 25.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08587302894344623		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.08587302894344623 | validation: 0.09274573558316795]
	TIME [epoch: 25.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08739102751545955		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.08739102751545955 | validation: 0.08436822889651366]
	TIME [epoch: 25.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07729517512362455		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.07729517512362455 | validation: 0.1088977570756004]
	TIME [epoch: 25.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08615078362014683		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.08615078362014683 | validation: 0.09157622602875706]
	TIME [epoch: 25.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0868747453260643		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.0868747453260643 | validation: 0.10624290599634803]
	TIME [epoch: 25.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08176521407967992		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.08176521407967992 | validation: 0.10767140041334831]
	TIME [epoch: 25.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07755779587123261		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.07755779587123261 | validation: 0.1119013233502249]
	TIME [epoch: 25.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08215199792330656		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.08215199792330656 | validation: 0.08641734276610769]
	TIME [epoch: 25.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08288074036025919		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.08288074036025919 | validation: 0.08887029746295036]
	TIME [epoch: 25.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07729282040683857		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.07729282040683857 | validation: 0.08505521839672561]
	TIME [epoch: 25.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07935662301456718		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.07935662301456718 | validation: 0.10138235659358953]
	TIME [epoch: 25.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08048236630412241		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.08048236630412241 | validation: 0.08602711514268958]
	TIME [epoch: 25.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08529878330377369		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.08529878330377369 | validation: 0.14645782109026434]
	TIME [epoch: 25.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0910156364302344		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0910156364302344 | validation: 0.0956054700443155]
	TIME [epoch: 25.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07515337993721727		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.07515337993721727 | validation: 0.08862467999206106]
	TIME [epoch: 25.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07879987610521492		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.07879987610521492 | validation: 0.11172788870577585]
	TIME [epoch: 25.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0816187324095361		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.0816187324095361 | validation: 0.08948657169804584]
	TIME [epoch: 25.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07996222308828643		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.07996222308828643 | validation: 0.11970032215833179]
	TIME [epoch: 25.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08270630348640946		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.08270630348640946 | validation: 0.09720875712519161]
	TIME [epoch: 25.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08400536859190927		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.08400536859190927 | validation: 0.09926052752321177]
	TIME [epoch: 25.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08883812493910925		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.08883812493910925 | validation: 0.08704542307719652]
	TIME [epoch: 25.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08003677123347493		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.08003677123347493 | validation: 0.08599767063962496]
	TIME [epoch: 25.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07407377389400914		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.07407377389400914 | validation: 0.08736758331355096]
	TIME [epoch: 25.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08133548645742501		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.08133548645742501 | validation: 0.10774343652575794]
	TIME [epoch: 25.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08066091856393633		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.08066091856393633 | validation: 0.10001622980593775]
	TIME [epoch: 25.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07920416283145708		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.07920416283145708 | validation: 0.09038021434501649]
	TIME [epoch: 25.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08248902847464093		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.08248902847464093 | validation: 0.08526317251922291]
	TIME [epoch: 25.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0770542561234229		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0770542561234229 | validation: 0.10126176183086283]
	TIME [epoch: 25.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07678072813261633		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.07678072813261633 | validation: 0.08469713028148892]
	TIME [epoch: 25.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07562819728544239		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.07562819728544239 | validation: 0.08368583438442899]
	TIME [epoch: 25.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0777446351308356		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0777446351308356 | validation: 0.08431761924959527]
	TIME [epoch: 25.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07920621939634273		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.07920621939634273 | validation: 0.08781355856833686]
	TIME [epoch: 25.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07914807490365061		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.07914807490365061 | validation: 0.09372290634838545]
	TIME [epoch: 25.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07973464077555881		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.07973464077555881 | validation: 0.090956085732377]
	TIME [epoch: 25.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07552256289711207		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.07552256289711207 | validation: 0.0828213068725501]
	TIME [epoch: 25.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07616509178058573		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.07616509178058573 | validation: 0.08300602713159344]
	TIME [epoch: 25.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07595742675827104		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.07595742675827104 | validation: 0.08733305846400163]
	TIME [epoch: 25.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07776145449635144		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.07776145449635144 | validation: 0.09374718603118037]
	TIME [epoch: 25.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07926564226749606		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.07926564226749606 | validation: 0.10226384613053985]
	TIME [epoch: 25.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07660146400858876		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.07660146400858876 | validation: 0.10381003401506465]
	TIME [epoch: 25.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07868664394735034		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.07868664394735034 | validation: 0.08332720130442291]
	TIME [epoch: 25.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07925912959104053		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.07925912959104053 | validation: 0.0907432229109962]
	TIME [epoch: 25.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07323267897483882		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.07323267897483882 | validation: 0.09180209370329633]
	TIME [epoch: 25.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07776047245335795		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.07776047245335795 | validation: 0.08159972329024323]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07359876012602362		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.07359876012602362 | validation: 0.10333708769156377]
	TIME [epoch: 25.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08065406715694909		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.08065406715694909 | validation: 0.0931159858735752]
	TIME [epoch: 25.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07702051205588942		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.07702051205588942 | validation: 0.07940715997055811]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0754178044425945		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.0754178044425945 | validation: 0.08263321076995406]
	TIME [epoch: 25.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07667606736363		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.07667606736363 | validation: 0.10645626183563377]
	TIME [epoch: 25.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0784169445500962		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0784169445500962 | validation: 0.08476225406423066]
	TIME [epoch: 25.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07599381966795007		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.07599381966795007 | validation: 0.09967722707897098]
	TIME [epoch: 25.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08245311087067952		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.08245311087067952 | validation: 0.08454527146179805]
	TIME [epoch: 25.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07339857085581174		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.07339857085581174 | validation: 0.08636636796483975]
	TIME [epoch: 25.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07143906850028896		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.07143906850028896 | validation: 0.09068146683645667]
	TIME [epoch: 25.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0785496591439547		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0785496591439547 | validation: 0.09796693573068074]
	TIME [epoch: 25.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07260246234052048		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.07260246234052048 | validation: 0.0831518750489306]
	TIME [epoch: 25.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0788604714739602		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0788604714739602 | validation: 0.07896216754471741]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07614519075072734		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.07614519075072734 | validation: 0.08201781027260316]
	TIME [epoch: 25.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07467181485586798		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.07467181485586798 | validation: 0.07758897575783874]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07544715682213585		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.07544715682213585 | validation: 0.09260145570421946]
	TIME [epoch: 25.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07627120762577823		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.07627120762577823 | validation: 0.09049670268606408]
	TIME [epoch: 25.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07399436371416596		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.07399436371416596 | validation: 0.0916561939440832]
	TIME [epoch: 25.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0785711097041327		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.0785711097041327 | validation: 0.09478716965911155]
	TIME [epoch: 25.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07256435564148005		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.07256435564148005 | validation: 0.10151495363895019]
	TIME [epoch: 25.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07098559649751977		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.07098559649751977 | validation: 0.08296105278857047]
	TIME [epoch: 25.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07484190970424362		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.07484190970424362 | validation: 0.0797009220319431]
	TIME [epoch: 25.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0759571279667479		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.0759571279667479 | validation: 0.09533938208239356]
	TIME [epoch: 25.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07733386809340512		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.07733386809340512 | validation: 0.09629209957485355]
	TIME [epoch: 25.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07675420309663605		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.07675420309663605 | validation: 0.0869209534599352]
	TIME [epoch: 25.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07821442991146244		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.07821442991146244 | validation: 0.08606663355057664]
	TIME [epoch: 25.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07448618807776189		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.07448618807776189 | validation: 0.09124287858351057]
	TIME [epoch: 25.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07106139233380662		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.07106139233380662 | validation: 0.08209071382988184]
	TIME [epoch: 25.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07152646595647173		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.07152646595647173 | validation: 0.10914097956436714]
	TIME [epoch: 25.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08550353721170406		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.08550353721170406 | validation: 0.09072102997458217]
	TIME [epoch: 25.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07314833082542528		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.07314833082542528 | validation: 0.10793923022261309]
	TIME [epoch: 25.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0864616109544978		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.0864616109544978 | validation: 0.09019928362902811]
	TIME [epoch: 25.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07154592281059126		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.07154592281059126 | validation: 0.0980945560432109]
	TIME [epoch: 25.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07145405842034995		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.07145405842034995 | validation: 0.08785361127411223]
	TIME [epoch: 25.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07904052085424772		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.07904052085424772 | validation: 0.14325732506481087]
	TIME [epoch: 25.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08880761682389579		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.08880761682389579 | validation: 0.0927713865899513]
	TIME [epoch: 25.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08027541155448359		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.08027541155448359 | validation: 0.08906049042397589]
	TIME [epoch: 25.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07346264216369		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.07346264216369 | validation: 0.08238274992022251]
	TIME [epoch: 25.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07356542727581343		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.07356542727581343 | validation: 0.09444315302572222]
	TIME [epoch: 25.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0758266782833752		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0758266782833752 | validation: 0.09804495904758212]
	TIME [epoch: 25.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07785883263402142		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.07785883263402142 | validation: 0.08799602960812754]
	TIME [epoch: 25.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07416065637673136		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.07416065637673136 | validation: 0.08088667867711478]
	TIME [epoch: 25.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06832630676503526		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.06832630676503526 | validation: 0.08872595216697954]
	TIME [epoch: 25.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07810184380158856		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.07810184380158856 | validation: 0.091634549465398]
	TIME [epoch: 25.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07149497169047833		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.07149497169047833 | validation: 0.10694954461460887]
	TIME [epoch: 25.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07771110603422857		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.07771110603422857 | validation: 0.08710472561937019]
	TIME [epoch: 25.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07814547825570509		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.07814547825570509 | validation: 0.08970102263917512]
	TIME [epoch: 25.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.074817099357612		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.074817099357612 | validation: 0.08700371230381286]
	TIME [epoch: 25.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07042938100765261		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.07042938100765261 | validation: 0.08288445633987637]
	TIME [epoch: 25.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07029533660629811		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.07029533660629811 | validation: 0.07938563080485245]
	TIME [epoch: 25.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07385305746357504		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.07385305746357504 | validation: 0.08475777248555375]
	TIME [epoch: 25.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07364451041245496		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.07364451041245496 | validation: 0.08232932145783277]
	TIME [epoch: 25.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07225284864972706		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.07225284864972706 | validation: 0.08257530695306571]
	TIME [epoch: 25.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06967454157782296		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.06967454157782296 | validation: 0.08569544548283234]
	TIME [epoch: 25.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0730910835046945		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.0730910835046945 | validation: 0.08776692257502844]
	TIME [epoch: 25.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0733916265592463		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0733916265592463 | validation: 0.11489566268079338]
	TIME [epoch: 25.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07900108247744912		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.07900108247744912 | validation: 0.08302494318667622]
	TIME [epoch: 25.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07320644429298545		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.07320644429298545 | validation: 0.08319280253203921]
	TIME [epoch: 25.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07614519496190714		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.07614519496190714 | validation: 0.09395943291346436]
	TIME [epoch: 25.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07095932054688477		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.07095932054688477 | validation: 0.08878492115720907]
	TIME [epoch: 25.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07416496050486603		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.07416496050486603 | validation: 0.07972171370563429]
	TIME [epoch: 25.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07034848783826808		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.07034848783826808 | validation: 0.09025179945980613]
	TIME [epoch: 25.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06888112742899852		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.06888112742899852 | validation: 0.08936038304789254]
	TIME [epoch: 25.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0704616806165035		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0704616806165035 | validation: 0.08176349527083257]
	TIME [epoch: 25.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07950774430459548		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.07950774430459548 | validation: 0.10404839270434485]
	TIME [epoch: 25.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0727974556768497		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0727974556768497 | validation: 0.087193679435104]
	TIME [epoch: 25.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07480896450412815		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.07480896450412815 | validation: 0.09644757564383033]
	TIME [epoch: 25.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07008232412163687		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.07008232412163687 | validation: 0.086865941541151]
	TIME [epoch: 25.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07028866747836016		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.07028866747836016 | validation: 0.09059894450883348]
	TIME [epoch: 25.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08029983501988985		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.08029983501988985 | validation: 0.08756843569917075]
	TIME [epoch: 25.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07310992151454637		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.07310992151454637 | validation: 0.08890300744032673]
	TIME [epoch: 25.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07397503951791796		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.07397503951791796 | validation: 0.10006375554999838]
	TIME [epoch: 25.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07290186310475275		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.07290186310475275 | validation: 0.09397914886918816]
	TIME [epoch: 25.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07540097021196406		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.07540097021196406 | validation: 0.0959843329015696]
	TIME [epoch: 25.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07251936994266878		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.07251936994266878 | validation: 0.09972199903432893]
	TIME [epoch: 25.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0703965784040608		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0703965784040608 | validation: 0.088757564316471]
	TIME [epoch: 25.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07194872870629268		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.07194872870629268 | validation: 0.08098431637153826]
	TIME [epoch: 25.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07082391060762483		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.07082391060762483 | validation: 0.08784872546514129]
	TIME [epoch: 25.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06826088854341963		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.06826088854341963 | validation: 0.08494850781702633]
	TIME [epoch: 25.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0715070542943014		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0715070542943014 | validation: 0.08741495783552553]
	TIME [epoch: 25.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07443820597556154		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.07443820597556154 | validation: 0.08611009744833184]
	TIME [epoch: 25.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07277757764877574		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.07277757764877574 | validation: 0.09378588481207392]
	TIME [epoch: 25.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07016057854042805		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.07016057854042805 | validation: 0.09357299160897753]
	TIME [epoch: 25.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07186219584599059		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.07186219584599059 | validation: 0.09105332296511606]
	TIME [epoch: 25.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07386230787241313		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.07386230787241313 | validation: 0.09708470200591593]
	TIME [epoch: 25.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0707531887684323		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0707531887684323 | validation: 0.08376256925078168]
	TIME [epoch: 25.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07186733751466333		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.07186733751466333 | validation: 0.07919955056472025]
	TIME [epoch: 25.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06993181122554726		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.06993181122554726 | validation: 0.08681239484330602]
	TIME [epoch: 25.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07126071838756423		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.07126071838756423 | validation: 0.08822277480294848]
	TIME [epoch: 25.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06754269638075036		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.06754269638075036 | validation: 0.09147940421256942]
	TIME [epoch: 25.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07256155945218451		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.07256155945218451 | validation: 0.08461409907545153]
	TIME [epoch: 25.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0681700546070304		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0681700546070304 | validation: 0.08760063100671737]
	TIME [epoch: 25.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07268550788275284		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.07268550788275284 | validation: 0.08275394831724159]
	TIME [epoch: 25.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07160268727420648		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.07160268727420648 | validation: 0.09062924628077534]
	TIME [epoch: 25.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07263047277960334		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.07263047277960334 | validation: 0.08590285470020897]
	TIME [epoch: 25.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06795643373561011		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.06795643373561011 | validation: 0.08238073068032645]
	TIME [epoch: 25.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07053786017972778		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.07053786017972778 | validation: 0.07487188617288809]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0701656654166649		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0701656654166649 | validation: 0.07610633713316936]
	TIME [epoch: 25.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06884781029238107		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.06884781029238107 | validation: 0.09970516036433752]
	TIME [epoch: 25.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06928127400273827		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.06928127400273827 | validation: 0.08119266780910618]
	TIME [epoch: 25.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07137403186856915		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.07137403186856915 | validation: 0.08144700021069362]
	TIME [epoch: 25.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07382162051655314		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.07382162051655314 | validation: 0.08211655350833119]
	TIME [epoch: 25.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0711207664877077		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.0711207664877077 | validation: 0.09025682997193811]
	TIME [epoch: 25.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06809140181247154		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.06809140181247154 | validation: 0.07770671412945673]
	TIME [epoch: 25.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07415932697620672		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.07415932697620672 | validation: 0.08747970696014756]
	TIME [epoch: 25.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06863880138123866		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.06863880138123866 | validation: 0.08290797451106038]
	TIME [epoch: 25.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06999279363193195		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.06999279363193195 | validation: 0.08290648780149432]
	TIME [epoch: 25.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06763881629275391		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.06763881629275391 | validation: 0.08536567001772478]
	TIME [epoch: 25.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07036673872760152		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.07036673872760152 | validation: 0.09268376853017726]
	TIME [epoch: 25.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06977637897961271		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.06977637897961271 | validation: 0.07448650722572474]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06602832631635391		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.06602832631635391 | validation: 0.09156256929186582]
	TIME [epoch: 25.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06915015161590375		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.06915015161590375 | validation: 0.07861056447832676]
	TIME [epoch: 25.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06779925015145427		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.06779925015145427 | validation: 0.0909490949907264]
	TIME [epoch: 25.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07578309481065407		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.07578309481065407 | validation: 0.07943160070620676]
	TIME [epoch: 25.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06869958129906135		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.06869958129906135 | validation: 0.07855973611302774]
	TIME [epoch: 25.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06911082324504492		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.06911082324504492 | validation: 0.07495178132387914]
	TIME [epoch: 25.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.071280561340822		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.071280561340822 | validation: 0.07610133920897638]
	TIME [epoch: 25.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06823871571888125		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.06823871571888125 | validation: 0.08248912259324856]
	TIME [epoch: 25.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0691938905874663		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.0691938905874663 | validation: 0.09031659791535276]
	TIME [epoch: 25.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07194298189623961		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.07194298189623961 | validation: 0.08071562899971788]
	TIME [epoch: 25.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06945446844224869		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.06945446844224869 | validation: 0.08208245161543996]
	TIME [epoch: 25.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07069698132883634		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.07069698132883634 | validation: 0.07577818653163926]
	TIME [epoch: 25.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07339671092757212		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.07339671092757212 | validation: 0.08837027245400036]
	TIME [epoch: 25.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0718155881104129		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0718155881104129 | validation: 0.07739049612183233]
	TIME [epoch: 25.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06754898890104374		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.06754898890104374 | validation: 0.07922290714155045]
	TIME [epoch: 25.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07199454738959342		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.07199454738959342 | validation: 0.08208239780825757]
	TIME [epoch: 25.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06716780821839498		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.06716780821839498 | validation: 0.07415323880622891]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06996122845425082		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.06996122845425082 | validation: 0.0846080688676199]
	TIME [epoch: 25.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06609383523237089		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.06609383523237089 | validation: 0.07767194130344422]
	TIME [epoch: 25.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06597180432871451		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.06597180432871451 | validation: 0.09265409709638635]
	TIME [epoch: 25.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07172884308999444		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.07172884308999444 | validation: 0.07802549669235881]
	TIME [epoch: 25.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07134560737961156		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.07134560737961156 | validation: 0.0815269649204928]
	TIME [epoch: 25.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06819838118926434		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.06819838118926434 | validation: 0.07641182771315061]
	TIME [epoch: 25.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06897767587250625		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.06897767587250625 | validation: 0.07531552695281199]
	TIME [epoch: 25.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06946976064361754		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.06946976064361754 | validation: 0.07901006779176466]
	TIME [epoch: 25.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06742966465392217		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.06742966465392217 | validation: 0.08799482726645769]
	TIME [epoch: 25.2 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07085664641861733		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.07085664641861733 | validation: 0.10093390900061942]
	TIME [epoch: 25.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07128366134409132		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.07128366134409132 | validation: 0.07440211057885832]
	TIME [epoch: 25.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06462168068621642		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.06462168068621642 | validation: 0.07904762506825203]
	TIME [epoch: 25.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0639554030413168		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0639554030413168 | validation: 0.07657162429728853]
	TIME [epoch: 25.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06713511046130366		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.06713511046130366 | validation: 0.07245349632083625]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_912.pth
	Model improved!!!
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06779814105216869		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.06779814105216869 | validation: 0.07115024749040755]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_913.pth
	Model improved!!!
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06858780763329418		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.06858780763329418 | validation: 0.08161982622460624]
	TIME [epoch: 26 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06701678118396232		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.06701678118396232 | validation: 0.07383812527135886]
	TIME [epoch: 25.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06548408944877959		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.06548408944877959 | validation: 0.0786679518753609]
	TIME [epoch: 25.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06750298853874445		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.06750298853874445 | validation: 0.08237308046952282]
	TIME [epoch: 25.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06771857036486954		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.06771857036486954 | validation: 0.07590854535634675]
	TIME [epoch: 25.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06598776677112077		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.06598776677112077 | validation: 0.08016783762120643]
	TIME [epoch: 25.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06673569410006233		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.06673569410006233 | validation: 0.08616364370633824]
	TIME [epoch: 25.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07106081171393684		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.07106081171393684 | validation: 0.08792073119755536]
	TIME [epoch: 25.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0708379613473406		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.0708379613473406 | validation: 0.08312549656748422]
	TIME [epoch: 25.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06466752130826654		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.06466752130826654 | validation: 0.08470388523885329]
	TIME [epoch: 25.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0676420077095256		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0676420077095256 | validation: 0.0752006865661278]
	TIME [epoch: 25.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06709021553010155		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.06709021553010155 | validation: 0.07950467811958267]
	TIME [epoch: 25.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06626110532770473		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.06626110532770473 | validation: 0.07219037635506181]
	TIME [epoch: 25.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0677491700847663		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0677491700847663 | validation: 0.07504145564508533]
	TIME [epoch: 25.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06502727303735975		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.06502727303735975 | validation: 0.08026284365946654]
	TIME [epoch: 25.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06711401349689733		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.06711401349689733 | validation: 0.08194745007224491]
	TIME [epoch: 25.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07739321603243865		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.07739321603243865 | validation: 0.0870920810940714]
	TIME [epoch: 25.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.072262021361374		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.072262021361374 | validation: 0.06964948487976447]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_931.pth
	Model improved!!!
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06695840869781336		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.06695840869781336 | validation: 0.0734727389417307]
	TIME [epoch: 25.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06430646476203389		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.06430646476203389 | validation: 0.08156399062820002]
	TIME [epoch: 25.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06581478097346746		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.06581478097346746 | validation: 0.07696434994736565]
	TIME [epoch: 25.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06513127504397012		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.06513127504397012 | validation: 0.07723487909345557]
	TIME [epoch: 25.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07547380161111789		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.07547380161111789 | validation: 0.0800633101867183]
	TIME [epoch: 25.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06690884171187125		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.06690884171187125 | validation: 0.07304735526318229]
	TIME [epoch: 25.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06625856354586945		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.06625856354586945 | validation: 0.07565003259075868]
	TIME [epoch: 25.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06783940521557343		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.06783940521557343 | validation: 0.07299043560081951]
	TIME [epoch: 25.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06398581813359354		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.06398581813359354 | validation: 0.07723799410362203]
	TIME [epoch: 25.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06345161736004758		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.06345161736004758 | validation: 0.07331209773752274]
	TIME [epoch: 25.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06684206479304901		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.06684206479304901 | validation: 0.07947595298619034]
	TIME [epoch: 25.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07182975364814785		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.07182975364814785 | validation: 0.07243322046790818]
	TIME [epoch: 25.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0658378419489053		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0658378419489053 | validation: 0.08492921532411868]
	TIME [epoch: 25.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06470475808719774		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.06470475808719774 | validation: 0.06944703638567902]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0678140793548018		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0678140793548018 | validation: 0.07610244332351015]
	TIME [epoch: 25.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06688405250871421		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.06688405250871421 | validation: 0.0778476509949042]
	TIME [epoch: 25.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388565900969279		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.06388565900969279 | validation: 0.07582610777022605]
	TIME [epoch: 25.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06577376187510058		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.06577376187510058 | validation: 0.07635550384868153]
	TIME [epoch: 25.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0677786793046274		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0677786793046274 | validation: 0.0838711016339907]
	TIME [epoch: 25.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06757143194648144		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.06757143194648144 | validation: 0.07281712086271533]
	TIME [epoch: 25.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06413406558618033		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.06413406558618033 | validation: 0.07615682277840888]
	TIME [epoch: 25.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0652826785165654		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0652826785165654 | validation: 0.07403369571533723]
	TIME [epoch: 25.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07155845993958487		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.07155845993958487 | validation: 0.078353680805503]
	TIME [epoch: 25.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06426100746886626		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.06426100746886626 | validation: 0.07205662840671842]
	TIME [epoch: 25.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06552167214476487		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.06552167214476487 | validation: 0.0861903947304935]
	TIME [epoch: 25.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06663068452857548		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.06663068452857548 | validation: 0.07303845372162579]
	TIME [epoch: 25.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06323839762702232		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.06323839762702232 | validation: 0.06970119681804365]
	TIME [epoch: 25.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06611492915508849		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.06611492915508849 | validation: 0.07595078626372666]
	TIME [epoch: 25.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06552979293090655		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.06552979293090655 | validation: 0.0843316979858551]
	TIME [epoch: 25.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06392324625803124		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.06392324625803124 | validation: 0.07159996480652606]
	TIME [epoch: 25.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0724768828561749		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0724768828561749 | validation: 0.06959858037301084]
	TIME [epoch: 25.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06456217805500894		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.06456217805500894 | validation: 0.06887203272127827]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06325761404728501		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.06325761404728501 | validation: 0.08648682958185169]
	TIME [epoch: 25.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06626014980385918		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.06626014980385918 | validation: 0.08309923499087604]
	TIME [epoch: 25.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06521109223175457		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.06521109223175457 | validation: 0.07708493147617838]
	TIME [epoch: 25.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06648716314468144		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.06648716314468144 | validation: 0.07467556012622475]
	TIME [epoch: 25.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06531241119930664		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.06531241119930664 | validation: 0.07951785217440312]
	TIME [epoch: 25.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06554505465193114		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.06554505465193114 | validation: 0.08182064143930473]
	TIME [epoch: 25.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0647189971318809		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0647189971318809 | validation: 0.07607028880031191]
	TIME [epoch: 25.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06448722629623889		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.06448722629623889 | validation: 0.07802188109956032]
	TIME [epoch: 25.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06569397627455435		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.06569397627455435 | validation: 0.07545777952994978]
	TIME [epoch: 25.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0637770922578223		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0637770922578223 | validation: 0.07661002706081063]
	TIME [epoch: 25.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06355196525843912		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.06355196525843912 | validation: 0.07281590771493479]
	TIME [epoch: 25.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06606762428549777		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.06606762428549777 | validation: 0.07454306905083433]
	TIME [epoch: 25.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06417723244973937		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.06417723244973937 | validation: 0.07729798749867346]
	TIME [epoch: 25.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06162936299086368		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.06162936299086368 | validation: 0.07251539503711692]
	TIME [epoch: 25.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06678333518378879		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.06678333518378879 | validation: 0.0773735292004927]
	TIME [epoch: 25.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06172982818752278		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.06172982818752278 | validation: 0.07125723498488833]
	TIME [epoch: 25.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06156896654801618		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.06156896654801618 | validation: 0.07967089230360999]
	TIME [epoch: 25.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06589087368227245		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.06589087368227245 | validation: 0.07438737364250488]
	TIME [epoch: 25.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0620641819592204		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0620641819592204 | validation: 0.07710536854732986]
	TIME [epoch: 25.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06600922388765423		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.06600922388765423 | validation: 0.08058135672687078]
	TIME [epoch: 25.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655025569091719		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0655025569091719 | validation: 0.07650455361629954]
	TIME [epoch: 25.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06447218789647158		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.06447218789647158 | validation: 0.07210516142725504]
	TIME [epoch: 25.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06129895112279244		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.06129895112279244 | validation: 0.08129052653345178]
	TIME [epoch: 25.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062206667687666736		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.062206667687666736 | validation: 0.07804810229139736]
	TIME [epoch: 25.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06379548435518191		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.06379548435518191 | validation: 0.07062131531478581]
	TIME [epoch: 25.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06350976648569388		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.06350976648569388 | validation: 0.07000140706656192]
	TIME [epoch: 25.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06216349392344773		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.06216349392344773 | validation: 0.08416983324154714]
	TIME [epoch: 25.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06671907653187395		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.06671907653187395 | validation: 0.07023321281610934]
	TIME [epoch: 25.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06323532561943804		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.06323532561943804 | validation: 0.0708540642393597]
	TIME [epoch: 25.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06170222810798056		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.06170222810798056 | validation: 0.07202523388179291]
	TIME [epoch: 25.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06397972677582844		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.06397972677582844 | validation: 0.07757517930786072]
	TIME [epoch: 25.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06554672605523541		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.06554672605523541 | validation: 0.07810502932467914]
	TIME [epoch: 25.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06278201695257206		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.06278201695257206 | validation: 0.07642560718675234]
	TIME [epoch: 25.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06561620442120969		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.06561620442120969 | validation: 0.07583199965945348]
	TIME [epoch: 25.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06650030625431383		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.06650030625431383 | validation: 0.07621002946796208]
	TIME [epoch: 25.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06126137830317351		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.06126137830317351 | validation: 0.07518588920393485]
	TIME [epoch: 25.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06284164458533982		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.06284164458533982 | validation: 0.076487993433147]
	TIME [epoch: 25.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06161481646527006		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.06161481646527006 | validation: 0.08214553301435804]
	TIME [epoch: 400 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06488125843626226		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.06488125843626226 | validation: 0.06628957909695478]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061766684772277786		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.061766684772277786 | validation: 0.07222569895634515]
	TIME [epoch: 53.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06165906253585431		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.06165906253585431 | validation: 0.07327858183681983]
	TIME [epoch: 53.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06251784220372707		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.06251784220372707 | validation: 0.07617587809969434]
	TIME [epoch: 53.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06313684633864704		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.06313684633864704 | validation: 0.0708084153454053]
	TIME [epoch: 53.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0609446593087762		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0609446593087762 | validation: 0.07194266122623805]
	TIME [epoch: 53.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0597229231929982		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0597229231929982 | validation: 0.06954097897510408]
	TIME [epoch: 53.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0615794638139123		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.0615794638139123 | validation: 0.07286677297311644]
	TIME [epoch: 53.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06237239133533006		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.06237239133533006 | validation: 0.07269658515031158]
	TIME [epoch: 53.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06144157786364818		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.06144157786364818 | validation: 0.07495012810563081]
	TIME [epoch: 53.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06237659311568977		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.06237659311568977 | validation: 0.07419522006289526]
	TIME [epoch: 53.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0605248800448476		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0605248800448476 | validation: 0.07180333514528528]
	TIME [epoch: 53.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.063907990083648		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.063907990083648 | validation: 0.07647186412351831]
	TIME [epoch: 53.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06322137543975187		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.06322137543975187 | validation: 0.07545904432658887]
	TIME [epoch: 53.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06398310071438174		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.06398310071438174 | validation: 0.06834126676518128]
	TIME [epoch: 53.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06543201590020566		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.06543201590020566 | validation: 0.06518630915516434]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1017.pth
	Model improved!!!
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06117200716346913		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.06117200716346913 | validation: 0.07211442357079324]
	TIME [epoch: 53.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664460070144538		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0664460070144538 | validation: 0.0736223128175654]
	TIME [epoch: 53.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336432331744161		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.06336432331744161 | validation: 0.07113990089391306]
	TIME [epoch: 53.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06038576297138823		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.06038576297138823 | validation: 0.06695029720142742]
	TIME [epoch: 53.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062073714089746074		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.062073714089746074 | validation: 0.07516772955614531]
	TIME [epoch: 53.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06027298334069357		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.06027298334069357 | validation: 0.07074893100422308]
	TIME [epoch: 53.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061397709452212595		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.061397709452212595 | validation: 0.07156709756856845]
	TIME [epoch: 53.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05974794694710243		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.05974794694710243 | validation: 0.07373606587152781]
	TIME [epoch: 53.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060837615705330333		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.060837615705330333 | validation: 0.07466643804991617]
	TIME [epoch: 53.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06395826596059723		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.06395826596059723 | validation: 0.06921381881414371]
	TIME [epoch: 53.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06172596779510935		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.06172596779510935 | validation: 0.07428705076543321]
	TIME [epoch: 53.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06271369531958372		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.06271369531958372 | validation: 0.07638944209008874]
	TIME [epoch: 53.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06303115342781263		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.06303115342781263 | validation: 0.0811099342778872]
	TIME [epoch: 53.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0636922139469418		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0636922139469418 | validation: 0.06671917803097507]
	TIME [epoch: 53.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913488880367884		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.05913488880367884 | validation: 0.06736210245339]
	TIME [epoch: 53.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06097228563049885		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.06097228563049885 | validation: 0.06898818894982706]
	TIME [epoch: 53.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06218925955730374		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.06218925955730374 | validation: 0.07249528027530679]
	TIME [epoch: 53.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06414743038305493		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.06414743038305493 | validation: 0.07089402188911523]
	TIME [epoch: 53.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061817289009198945		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.061817289009198945 | validation: 0.06961825483660908]
	TIME [epoch: 53.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06193299290249184		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.06193299290249184 | validation: 0.0700503458798907]
	TIME [epoch: 53.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0597979999616123		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0597979999616123 | validation: 0.067282706775435]
	TIME [epoch: 53.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06052521833302691		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.06052521833302691 | validation: 0.06986396263865782]
	TIME [epoch: 53.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06046102561709718		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.06046102561709718 | validation: 0.07220855524455459]
	TIME [epoch: 53.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05944895789863018		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.05944895789863018 | validation: 0.07116078001718903]
	TIME [epoch: 53.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06378605557346453		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.06378605557346453 | validation: 0.06838384569396452]
	TIME [epoch: 53.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06065865317151041		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.06065865317151041 | validation: 0.07608427471311625]
	TIME [epoch: 53.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0611682993361792		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0611682993361792 | validation: 0.06830898222203166]
	TIME [epoch: 53.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06229389369912912		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.06229389369912912 | validation: 0.07154232229680568]
	TIME [epoch: 53.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060226076052291605		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.060226076052291605 | validation: 0.06960324071811355]
	TIME [epoch: 53.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059468335322025834		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.059468335322025834 | validation: 0.0734536932086585]
	TIME [epoch: 53.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06149535832700617		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.06149535832700617 | validation: 0.06805003855246013]
	TIME [epoch: 53.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05948815522281225		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.05948815522281225 | validation: 0.06645492406992351]
	TIME [epoch: 53.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06300944978340911		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.06300944978340911 | validation: 0.07650369446989597]
	TIME [epoch: 53.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06572950375832641		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.06572950375832641 | validation: 0.07048753116676265]
	TIME [epoch: 53.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06310041160389883		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.06310041160389883 | validation: 0.06765602759390812]
	TIME [epoch: 53.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0589024414647071		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0589024414647071 | validation: 0.06614407061616417]
	TIME [epoch: 53.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05807928338814153		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.05807928338814153 | validation: 0.0756666566333227]
	TIME [epoch: 53.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0613671697218019		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0613671697218019 | validation: 0.07661926667148172]
	TIME [epoch: 53.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060664651452554244		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.060664651452554244 | validation: 0.07235977775985603]
	TIME [epoch: 53.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06187650801704941		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.06187650801704941 | validation: 0.06793987657957808]
	TIME [epoch: 53.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05957660679485792		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.05957660679485792 | validation: 0.06844758552174565]
	TIME [epoch: 53.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058208991312310984		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.058208991312310984 | validation: 0.0676477426390343]
	TIME [epoch: 53.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06082783919992993		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.06082783919992993 | validation: 0.06688754500477284]
	TIME [epoch: 53.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06105873268969897		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.06105873268969897 | validation: 0.07014256532278115]
	TIME [epoch: 53.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05944053668026836		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.05944053668026836 | validation: 0.07205782514967174]
	TIME [epoch: 53.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059967358263792084		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.059967358263792084 | validation: 0.08233039976892081]
	TIME [epoch: 53.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06091394110279461		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.06091394110279461 | validation: 0.07337412157093583]
	TIME [epoch: 53.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059477192367641936		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.059477192367641936 | validation: 0.07615884927314112]
	TIME [epoch: 53.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06364886773532027		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.06364886773532027 | validation: 0.07322078205173228]
	TIME [epoch: 53.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06123068772567677		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.06123068772567677 | validation: 0.07251608006499813]
	TIME [epoch: 53.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059654130773497925		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.059654130773497925 | validation: 0.07090462886548378]
	TIME [epoch: 53.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060330353623113994		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.060330353623113994 | validation: 0.0722740417697255]
	TIME [epoch: 53.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05894040350054945		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.05894040350054945 | validation: 0.0740752112944438]
	TIME [epoch: 53.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060274705349750164		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.060274705349750164 | validation: 0.07072681950991175]
	TIME [epoch: 53.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05996476568486811		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.05996476568486811 | validation: 0.07206788205221057]
	TIME [epoch: 53.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05937786978037139		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.05937786978037139 | validation: 0.07399286648462645]
	TIME [epoch: 53.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060801958304092754		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.060801958304092754 | validation: 0.06890221844205727]
	TIME [epoch: 54.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06006715718860931		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.06006715718860931 | validation: 0.07171319761933072]
	TIME [epoch: 53.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05998099574879294		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.05998099574879294 | validation: 0.07584978576566477]
	TIME [epoch: 53.8 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0622697305081879		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0622697305081879 | validation: 0.06705596749551415]
	TIME [epoch: 54 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0595028950320276		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0595028950320276 | validation: 0.06781755519134788]
	TIME [epoch: 53.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05874985328426971		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.05874985328426971 | validation: 0.06874637545190287]
	TIME [epoch: 53.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05879157011516905		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.05879157011516905 | validation: 0.07024037746060041]
	TIME [epoch: 53.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05755508497815795		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.05755508497815795 | validation: 0.07377666971921004]
	TIME [epoch: 53.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05910566328650482		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.05910566328650482 | validation: 0.07211438702502629]
	TIME [epoch: 53.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06049886397283628		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.06049886397283628 | validation: 0.06984484620643351]
	TIME [epoch: 53.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05959963333150218		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.05959963333150218 | validation: 0.06551216481831901]
	TIME [epoch: 53.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05856771893507019		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.05856771893507019 | validation: 0.07754485033786837]
	TIME [epoch: 54 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0595518823724439		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0595518823724439 | validation: 0.07030025227595278]
	TIME [epoch: 53.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059545902316211034		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.059545902316211034 | validation: 0.08122360543113227]
	TIME [epoch: 53.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06001202355072724		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.06001202355072724 | validation: 0.07067678130654734]
	TIME [epoch: 53.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0587001159214009		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0587001159214009 | validation: 0.0667454234371765]
	TIME [epoch: 53.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059591060463802		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.059591060463802 | validation: 0.07132459247891487]
	TIME [epoch: 53.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060570401649737336		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.060570401649737336 | validation: 0.06634469664494341]
	TIME [epoch: 54.1 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05816078086923204		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.05816078086923204 | validation: 0.0670530276315545]
	TIME [epoch: 53.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05647959756880443		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.05647959756880443 | validation: 0.06647490362959813]
	TIME [epoch: 53.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057436679370845026		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.057436679370845026 | validation: 0.07118449273395054]
	TIME [epoch: 53.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060750900339157504		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.060750900339157504 | validation: 0.06611555196798533]
	TIME [epoch: 53.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05896614891407004		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.05896614891407004 | validation: 0.06880477162150403]
	TIME [epoch: 53.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05668128328291625		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.05668128328291625 | validation: 0.07311327962475415]
	TIME [epoch: 53.8 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05746699630322063		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.05746699630322063 | validation: 0.07078242334277177]
	TIME [epoch: 53.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058208860666654		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.058208860666654 | validation: 0.07336765331267775]
	TIME [epoch: 53.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995063976125718		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.05995063976125718 | validation: 0.06478497237777582]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1100.pth
	Model improved!!!
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057376038119158104		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.057376038119158104 | validation: 0.07050364416429397]
	TIME [epoch: 53.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057096641318812216		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.057096641318812216 | validation: 0.06527843807740581]
	TIME [epoch: 53.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06014586560919376		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.06014586560919376 | validation: 0.06493445528810905]
	TIME [epoch: 54 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05943689466541162		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.05943689466541162 | validation: 0.07918898277298221]
	TIME [epoch: 53.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958791390201044		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.05958791390201044 | validation: 0.06742225291259629]
	TIME [epoch: 53.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059248608031913966		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.059248608031913966 | validation: 0.06901280762386038]
	TIME [epoch: 53.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05763734317635848		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.05763734317635848 | validation: 0.07159299908974422]
	TIME [epoch: 53.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913337701042121		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.05913337701042121 | validation: 0.06514956009537898]
	TIME [epoch: 53.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958217704699561		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.05958217704699561 | validation: 0.06971855262198684]
	TIME [epoch: 53.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058008256483188775		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.058008256483188775 | validation: 0.06603175609906102]
	TIME [epoch: 53.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059066790504178485		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.059066790504178485 | validation: 0.06337490079080238]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059985039489156705		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.059985039489156705 | validation: 0.07049536346653056]
	TIME [epoch: 53.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05788283563673751		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.05788283563673751 | validation: 0.07184977693693037]
	TIME [epoch: 53.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05801339509797773		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.05801339509797773 | validation: 0.06375514019368976]
	TIME [epoch: 53.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05767411444089961		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.05767411444089961 | validation: 0.06889465957749619]
	TIME [epoch: 53.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868398378048391		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.05868398378048391 | validation: 0.06967503803398753]
	TIME [epoch: 53.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062310461563711325		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.062310461563711325 | validation: 0.06710915158353575]
	TIME [epoch: 53.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05728598335136732		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.05728598335136732 | validation: 0.065484273014752]
	TIME [epoch: 53.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05724302794163673		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.05724302794163673 | validation: 0.06815906460147961]
	TIME [epoch: 53.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05723116293249288		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.05723116293249288 | validation: 0.06841972964892432]
	TIME [epoch: 53.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058128691248778674		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.058128691248778674 | validation: 0.06157032185996594]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05827866809873887		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.05827866809873887 | validation: 0.06645608331165506]
	TIME [epoch: 53.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05781800715613189		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.05781800715613189 | validation: 0.06822659698131618]
	TIME [epoch: 53.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057871113227667005		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.057871113227667005 | validation: 0.06523649187146062]
	TIME [epoch: 53.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055762580646292643		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.055762580646292643 | validation: 0.06619395447777414]
	TIME [epoch: 53.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05686524833659311		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.05686524833659311 | validation: 0.06865162992494986]
	TIME [epoch: 53.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057139944264997536		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.057139944264997536 | validation: 0.07062502244588034]
	TIME [epoch: 53.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0586722575282065		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0586722575282065 | validation: 0.06584278928113232]
	TIME [epoch: 53.9 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058586853179048916		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.058586853179048916 | validation: 0.06673605241510389]
	TIME [epoch: 53.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05741594154006882		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.05741594154006882 | validation: 0.06532931964028608]
	TIME [epoch: 53.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057816522579993306		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.057816522579993306 | validation: 0.06682411502206238]
	TIME [epoch: 53.8 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056065652614322406		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.056065652614322406 | validation: 0.0689331138869441]
	TIME [epoch: 53.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06017069216846001		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.06017069216846001 | validation: 0.0655909128390286]
	TIME [epoch: 53.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05652367529550623		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.05652367529550623 | validation: 0.06688565101752475]
	TIME [epoch: 53.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05943646479228304		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.05943646479228304 | validation: 0.06398557131244681]
	TIME [epoch: 53.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822574262122766		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.05822574262122766 | validation: 0.06346190684346587]
	TIME [epoch: 53.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055876161311978735		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.055876161311978735 | validation: 0.06584760596067762]
	TIME [epoch: 53.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05805558261427998		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.05805558261427998 | validation: 0.06549904452203315]
	TIME [epoch: 53.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058421814875674986		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.058421814875674986 | validation: 0.06798283309935785]
	TIME [epoch: 53.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057685632466662015		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.057685632466662015 | validation: 0.06890889005770065]
	TIME [epoch: 53.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055809644641452764		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.055809644641452764 | validation: 0.06481196876190858]
	TIME [epoch: 53.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056254197832032525		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.056254197832032525 | validation: 0.06364858297242368]
	TIME [epoch: 53.8 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05653601025302504		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.05653601025302504 | validation: 0.06687525705869778]
	TIME [epoch: 53.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058001321352988834		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.058001321352988834 | validation: 0.061064173963634255]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1144.pth
	Model improved!!!
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05896138792309697		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.05896138792309697 | validation: 0.06472665586306714]
	TIME [epoch: 53.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056690407413448954		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.056690407413448954 | validation: 0.06566551958242044]
	TIME [epoch: 53.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05639187338579255		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.05639187338579255 | validation: 0.06809509775906167]
	TIME [epoch: 53.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05595967630660305		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.05595967630660305 | validation: 0.0674928783366801]
	TIME [epoch: 53.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056562162766090066		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.056562162766090066 | validation: 0.06396395997969967]
	TIME [epoch: 53.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056078006522661164		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.056078006522661164 | validation: 0.06962281505891509]
	TIME [epoch: 53.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06076009775065211		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.06076009775065211 | validation: 0.07558974816771244]
	TIME [epoch: 53.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0567602504049342		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.0567602504049342 | validation: 0.07537843394623664]
	TIME [epoch: 53.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05810579330835927		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.05810579330835927 | validation: 0.06615224205489362]
	TIME [epoch: 53.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05614571854091325		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.05614571854091325 | validation: 0.06425096228171163]
	TIME [epoch: 53.7 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06265581689990163		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.06265581689990163 | validation: 0.06078703283943288]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1155.pth
	Model improved!!!
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05847446742848331		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.05847446742848331 | validation: 0.057239640650665335]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1156.pth
	Model improved!!!
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055597689405286095		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.055597689405286095 | validation: 0.06515803608616579]
	TIME [epoch: 53.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05577402883938801		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.05577402883938801 | validation: 0.0650101926093925]
	TIME [epoch: 53.7 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0567940984509903		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0567940984509903 | validation: 0.06518959405877764]
	TIME [epoch: 53.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05443216015341989		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.05443216015341989 | validation: 0.06338535330606984]
	TIME [epoch: 53.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05731149953296502		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.05731149953296502 | validation: 0.06327911884284128]
	TIME [epoch: 53.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05585493145925888		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.05585493145925888 | validation: 0.06372447867246507]
	TIME [epoch: 53.7 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05494643828165431		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.05494643828165431 | validation: 0.06443724574059834]
	TIME [epoch: 53.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056852523396281565		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.056852523396281565 | validation: 0.06521671872019388]
	TIME [epoch: 53.7 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056826970889198324		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.056826970889198324 | validation: 0.06847868024988055]
	TIME [epoch: 53.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058481019920666566		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.058481019920666566 | validation: 0.0644838330411056]
	TIME [epoch: 53.7 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05758342082726945		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.05758342082726945 | validation: 0.07190805013760215]
	TIME [epoch: 53.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05650731291926061		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.05650731291926061 | validation: 0.0657615884173133]
	TIME [epoch: 53.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05504221527862932		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.05504221527862932 | validation: 0.06329031339459004]
	TIME [epoch: 53.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054398737575032885		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.054398737575032885 | validation: 0.06643886187328846]
	TIME [epoch: 53.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05584115543527856		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.05584115543527856 | validation: 0.0674922290152692]
	TIME [epoch: 53.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05709025518758627		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.05709025518758627 | validation: 0.06461688751344898]
	TIME [epoch: 53.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05683031076622186		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.05683031076622186 | validation: 0.06783408770996252]
	TIME [epoch: 53.7 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05980866077099894		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.05980866077099894 | validation: 0.0638169499350367]
	TIME [epoch: 53.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05680510160552889		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.05680510160552889 | validation: 0.06838702534143802]
	TIME [epoch: 53.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055030822975758564		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.055030822975758564 | validation: 0.06550880635269951]
	TIME [epoch: 53.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0540996226122217		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.0540996226122217 | validation: 0.06838323866417167]
	TIME [epoch: 53.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05655183940743902		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.05655183940743902 | validation: 0.06272001493352093]
	TIME [epoch: 53.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05450693965692823		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.05450693965692823 | validation: 0.06719714251421963]
	TIME [epoch: 53.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05781836899039325		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.05781836899039325 | validation: 0.07012650339561166]
	TIME [epoch: 53.7 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05624999663397047		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.05624999663397047 | validation: 0.06221032784302129]
	TIME [epoch: 53.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0569459271131144		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0569459271131144 | validation: 0.06101353777902606]
	TIME [epoch: 53.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05404706584644053		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.05404706584644053 | validation: 0.06588639217709119]
	TIME [epoch: 53.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05669252286565208		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.05669252286565208 | validation: 0.06569187825350037]
	TIME [epoch: 53.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055364965064187886		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.055364965064187886 | validation: 0.06358413528423229]
	TIME [epoch: 53.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05378886540904216		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.05378886540904216 | validation: 0.06542634132801889]
	TIME [epoch: 53.7 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0575702380533257		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0575702380533257 | validation: 0.06402217377850905]
	TIME [epoch: 53.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05513439714190341		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.05513439714190341 | validation: 0.06719175237410954]
	TIME [epoch: 53.7 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055907800124947664		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.055907800124947664 | validation: 0.06095210330564067]
	TIME [epoch: 53.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054743550769174296		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.054743550769174296 | validation: 0.06261285642742191]
	TIME [epoch: 53.7 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05602307672032675		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.05602307672032675 | validation: 0.059854203737123274]
	TIME [epoch: 53.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05430831256791767		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.05430831256791767 | validation: 0.06763838553399225]
	TIME [epoch: 53.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056370590997187		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.056370590997187 | validation: 0.05839922963465702]
	TIME [epoch: 53.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05505053520915492		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.05505053520915492 | validation: 0.06472694540738841]
	TIME [epoch: 53.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0558042633615105		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.0558042633615105 | validation: 0.06244332362769637]
	TIME [epoch: 53.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055573093581071806		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.055573093581071806 | validation: 0.06471158130831726]
	TIME [epoch: 53.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05683461598272929		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.05683461598272929 | validation: 0.06339061565031534]
	TIME [epoch: 53.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055519926983162785		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.055519926983162785 | validation: 0.06563158005004271]
	TIME [epoch: 53.7 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05426054039843074		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.05426054039843074 | validation: 0.0695091597233715]
	TIME [epoch: 53.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05439280037631544		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.05439280037631544 | validation: 0.06317552964312026]
	TIME [epoch: 53.7 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05519507113784884		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.05519507113784884 | validation: 0.06770002706835372]
	TIME [epoch: 53.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0559727332886355		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0559727332886355 | validation: 0.06847528200632312]
	TIME [epoch: 53.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056321652710419175		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.056321652710419175 | validation: 0.06865573455910591]
	TIME [epoch: 53.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056296931522495056		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.056296931522495056 | validation: 0.06253986576597952]
	TIME [epoch: 53.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05590197241016717		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.05590197241016717 | validation: 0.06822966169377573]
	TIME [epoch: 53.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05496985593620042		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.05496985593620042 | validation: 0.06641697418471222]
	TIME [epoch: 53.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05462460398248212		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.05462460398248212 | validation: 0.07031028069067716]
	TIME [epoch: 53.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054727714984754636		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.054727714984754636 | validation: 0.07221348795827998]
	TIME [epoch: 53.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05869850313429777		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.05869850313429777 | validation: 0.06265085132769993]
	TIME [epoch: 53.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054391451903620824		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.054391451903620824 | validation: 0.06089401623841691]
	TIME [epoch: 53.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05458934534178158		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.05458934534178158 | validation: 0.06491165268149814]
	TIME [epoch: 53.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05546611916393325		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.05546611916393325 | validation: 0.06585918456357845]
	TIME [epoch: 53.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055713896554595035		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.055713896554595035 | validation: 0.06127187908228104]
	TIME [epoch: 53.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055344983683170146		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.055344983683170146 | validation: 0.06348042391969783]
	TIME [epoch: 53.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05525692752248412		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.05525692752248412 | validation: 0.06564455887571485]
	TIME [epoch: 53.7 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05473502208166369		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.05473502208166369 | validation: 0.06550015768679777]
	TIME [epoch: 53.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05460671174171064		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.05460671174171064 | validation: 0.06378753307761414]
	TIME [epoch: 53.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05421763794393024		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.05421763794393024 | validation: 0.06591871324017357]
	TIME [epoch: 53.7 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05566076205301048		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.05566076205301048 | validation: 0.07029136914876188]
	TIME [epoch: 53.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05429475739265134		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.05429475739265134 | validation: 0.060707483152845736]
	TIME [epoch: 53.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05382196736470836		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.05382196736470836 | validation: 0.06362063365732501]
	TIME [epoch: 53.6 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0542576548243047		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.0542576548243047 | validation: 0.06470097938121533]
	TIME [epoch: 53.6 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05402915333979191		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.05402915333979191 | validation: 0.06257785686063504]
	TIME [epoch: 53.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05897886953929471		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.05897886953929471 | validation: 0.06397356259145898]
	TIME [epoch: 53.6 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05796273217322877		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.05796273217322877 | validation: 0.06015293265926894]
	TIME [epoch: 53.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054656451649991444		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.054656451649991444 | validation: 0.05643278167352206]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1226.pth
	Model improved!!!
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05600902885137077		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.05600902885137077 | validation: 0.06258920303013138]
	TIME [epoch: 53.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05833922972082081		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.05833922972082081 | validation: 0.06169908848596527]
	TIME [epoch: 53.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0562726657593455		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.0562726657593455 | validation: 0.0597695503086228]
	TIME [epoch: 53.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055484511630238396		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.055484511630238396 | validation: 0.06078648522048228]
	TIME [epoch: 53.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05380168499901686		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.05380168499901686 | validation: 0.05928595014760275]
	TIME [epoch: 53.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05406429931995586		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.05406429931995586 | validation: 0.05986613445896022]
	TIME [epoch: 53.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05268192003543557		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.05268192003543557 | validation: 0.06125454484181863]
	TIME [epoch: 53.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05432471451354328		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.05432471451354328 | validation: 0.05674597176791603]
	TIME [epoch: 53.7 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05448426626318238		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.05448426626318238 | validation: 0.06072742427802834]
	TIME [epoch: 53.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05467109870593162		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.05467109870593162 | validation: 0.05909259245490929]
	TIME [epoch: 53.7 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054370128325085004		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.054370128325085004 | validation: 0.06550816597351644]
	TIME [epoch: 53.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054035476065664334		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.054035476065664334 | validation: 0.06071685114909876]
	TIME [epoch: 53.7 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05510066742245902		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.05510066742245902 | validation: 0.062764811281325]
	TIME [epoch: 53.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0547510559541728		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0547510559541728 | validation: 0.06767321018304487]
	TIME [epoch: 53.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053325643243835916		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.053325643243835916 | validation: 0.06411404526785142]
	TIME [epoch: 53.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053832525061368014		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.053832525061368014 | validation: 0.061160677798841115]
	TIME [epoch: 53.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053757425056738		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.053757425056738 | validation: 0.06117854108537139]
	TIME [epoch: 53.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0559707945547778		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.0559707945547778 | validation: 0.057843914645387906]
	TIME [epoch: 53.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053453087917012754		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.053453087917012754 | validation: 0.06096808797015067]
	TIME [epoch: 53.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053169695876948225		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.053169695876948225 | validation: 0.05859835669890276]
	TIME [epoch: 53.7 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05394264043298709		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.05394264043298709 | validation: 0.06516924814837582]
	TIME [epoch: 53.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05321379185901295		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.05321379185901295 | validation: 0.06526982353127495]
	TIME [epoch: 53.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05477096973433225		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.05477096973433225 | validation: 0.06636074656151256]
	TIME [epoch: 53.7 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05462532587021836		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.05462532587021836 | validation: 0.06636773596302699]
	TIME [epoch: 53.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05273294256806885		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.05273294256806885 | validation: 0.06024134253000485]
	TIME [epoch: 53.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053055264701397695		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.053055264701397695 | validation: 0.05885167658583583]
	TIME [epoch: 53.7 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05360246492530858		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.05360246492530858 | validation: 0.06096111357340807]
	TIME [epoch: 53.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05524554341986077		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.05524554341986077 | validation: 0.060223932883423606]
	TIME [epoch: 53.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05250979789636032		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.05250979789636032 | validation: 0.06533783574024685]
	TIME [epoch: 53.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05412065467538579		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.05412065467538579 | validation: 0.05934892667342123]
	TIME [epoch: 53.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052598592321784404		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.052598592321784404 | validation: 0.06020207977011222]
	TIME [epoch: 53.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052802042783645156		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.052802042783645156 | validation: 0.05717118515432126]
	TIME [epoch: 53.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05396118653075515		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.05396118653075515 | validation: 0.06645175697856115]
	TIME [epoch: 53.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05398482205602932		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.05398482205602932 | validation: 0.06079822452840357]
	TIME [epoch: 53.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05259905064670228		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.05259905064670228 | validation: 0.06029451012426452]
	TIME [epoch: 53.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053764572666817605		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.053764572666817605 | validation: 0.0599858972080574]
	TIME [epoch: 53.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05739796209827515		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.05739796209827515 | validation: 0.06150062825383154]
	TIME [epoch: 53.7 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053157386380802016		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.053157386380802016 | validation: 0.06104787146873159]
	TIME [epoch: 53.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0539634361382266		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0539634361382266 | validation: 0.0641990580108435]
	TIME [epoch: 53.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05482552338753592		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.05482552338753592 | validation: 0.062019460982490254]
	TIME [epoch: 53.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053782332205672204		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.053782332205672204 | validation: 0.06178444572671733]
	TIME [epoch: 53.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05404936062124113		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.05404936062124113 | validation: 0.060052175010729555]
	TIME [epoch: 53.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05309953054417983		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.05309953054417983 | validation: 0.060798053836533306]
	TIME [epoch: 53.7 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052007157174903715		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.052007157174903715 | validation: 0.06025689038759412]
	TIME [epoch: 53.7 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054184764079370326		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.054184764079370326 | validation: 0.06279779065044891]
	TIME [epoch: 53.6 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053161433667351335		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.053161433667351335 | validation: 0.06244651578369448]
	TIME [epoch: 53.7 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05325632281390185		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.05325632281390185 | validation: 0.05907720088776349]
	TIME [epoch: 53.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053122944444095745		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.053122944444095745 | validation: 0.06127804918942845]
	TIME [epoch: 53.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052704318597676436		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.052704318597676436 | validation: 0.059961151783351686]
	TIME [epoch: 53.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05215978179614836		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.05215978179614836 | validation: 0.05807715054861269]
	TIME [epoch: 53.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05297138470949521		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.05297138470949521 | validation: 0.06347005672621774]
	TIME [epoch: 53.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051949064787783696		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.051949064787783696 | validation: 0.06163347562272421]
	TIME [epoch: 53.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05248427264353751		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.05248427264353751 | validation: 0.0643281675795925]
	TIME [epoch: 53.7 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0527515449967907		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0527515449967907 | validation: 0.06383502791440043]
	TIME [epoch: 53.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0534694388811109		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.0534694388811109 | validation: 0.061490325104061536]
	TIME [epoch: 53.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053541370825400274		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.053541370825400274 | validation: 0.06179648525828708]
	TIME [epoch: 53.7 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05515290136762763		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.05515290136762763 | validation: 0.07311545340899026]
	TIME [epoch: 53.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05306584427028152		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.05306584427028152 | validation: 0.061678588919232176]
	TIME [epoch: 53.7 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05264821155748513		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.05264821155748513 | validation: 0.06331840458702542]
	TIME [epoch: 53.7 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05274411133775688		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.05274411133775688 | validation: 0.06180951608525631]
	TIME [epoch: 53.7 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053623177795922995		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.053623177795922995 | validation: 0.06364767335315391]
	TIME [epoch: 53.7 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05365091783692472		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.05365091783692472 | validation: 0.06300001336363173]
	TIME [epoch: 53.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053318808286377604		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.053318808286377604 | validation: 0.06177451016432426]
	TIME [epoch: 53.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053633639690836746		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.053633639690836746 | validation: 0.06734541505798552]
	TIME [epoch: 53.7 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05217668095702945		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.05217668095702945 | validation: 0.06289190040616788]
	TIME [epoch: 53.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05134489482601595		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.05134489482601595 | validation: 0.06414751876031757]
	TIME [epoch: 53.7 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05540803511995864		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.05540803511995864 | validation: 0.06055005731469746]
	TIME [epoch: 53.7 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05293936102039168		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.05293936102039168 | validation: 0.06206863387247637]
	TIME [epoch: 53.7 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052469987927472955		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.052469987927472955 | validation: 0.06161822243234242]
	TIME [epoch: 53.7 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05229645241662318		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.05229645241662318 | validation: 0.06132043176962022]
	TIME [epoch: 53.7 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052079831898812504		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.052079831898812504 | validation: 0.06513617191850642]
	TIME [epoch: 53.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052319810832864876		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.052319810832864876 | validation: 0.06456552964406712]
	TIME [epoch: 53.7 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052384182633498845		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.052384182633498845 | validation: 0.06427388113800156]
	TIME [epoch: 53.7 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05198368769275231		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.05198368769275231 | validation: 0.05875330808521306]
	TIME [epoch: 53.7 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05346505587577968		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.05346505587577968 | validation: 0.06558958176523955]
	TIME [epoch: 53.7 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053198717763759354		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.053198717763759354 | validation: 0.06636620109108246]
	TIME [epoch: 53.7 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05290046162598212		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.05290046162598212 | validation: 0.06410934986459962]
	TIME [epoch: 53.7 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052933477267380145		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.052933477267380145 | validation: 0.06037243614347376]
	TIME [epoch: 53.7 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05342796162332776		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.05342796162332776 | validation: 0.06163644951215917]
	TIME [epoch: 53.7 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05203902862065861		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.05203902862065861 | validation: 0.06358947578856777]
	TIME [epoch: 53.7 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05231276763117676		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.05231276763117676 | validation: 0.06198413217945151]
	TIME [epoch: 53.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055416165297708694		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.055416165297708694 | validation: 0.06076070733319201]
	TIME [epoch: 53.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05242528890985137		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.05242528890985137 | validation: 0.06251077029583552]
	TIME [epoch: 53.7 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05167908417316737		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.05167908417316737 | validation: 0.06379575964030283]
	TIME [epoch: 53.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05274521539710887		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.05274521539710887 | validation: 0.06137721637743197]
	TIME [epoch: 53.7 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05277624905546632		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.05277624905546632 | validation: 0.06224280338823114]
	TIME [epoch: 53.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051742258656082604		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.051742258656082604 | validation: 0.060415738431780615]
	TIME [epoch: 53.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05161300097586435		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.05161300097586435 | validation: 0.05661935113529724]
	TIME [epoch: 53.7 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05480583251340075		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.05480583251340075 | validation: 0.06061328883161439]
	TIME [epoch: 53.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052748139194049186		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.052748139194049186 | validation: 0.05878009024441062]
	TIME [epoch: 53.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05210226790271373		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.05210226790271373 | validation: 0.06297277891538339]
	TIME [epoch: 53.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052176372169795704		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.052176372169795704 | validation: 0.05850199679008877]
	TIME [epoch: 53.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05157231976311029		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.05157231976311029 | validation: 0.062284909256401504]
	TIME [epoch: 53.7 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05299188648493415		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.05299188648493415 | validation: 0.05889573081821747]
	TIME [epoch: 53.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05159999245146718		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.05159999245146718 | validation: 0.061564812685467055]
	TIME [epoch: 53.7 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052402270397938294		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.052402270397938294 | validation: 0.06296009272175906]
	TIME [epoch: 53.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051327310832731565		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.051327310832731565 | validation: 0.061777427614817956]
	TIME [epoch: 53.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05158455023381364		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.05158455023381364 | validation: 0.06801787107313431]
	TIME [epoch: 53.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052671020912123265		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.052671020912123265 | validation: 0.06275229959585184]
	TIME [epoch: 53.7 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05164834703217464		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.05164834703217464 | validation: 0.0651984185322578]
	TIME [epoch: 53.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05271386683614303		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.05271386683614303 | validation: 0.06062131396872794]
	TIME [epoch: 53.7 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_3_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_3_v_mmd1_1327.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 37493.111 seconds.
