Args:
Namespace(name='model_phi1_4a_distortion_v2_4_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.019716218, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2942282993

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.061904397343387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.061904397343387 | validation: 4.585632645109981]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9235219879442385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9235219879442385 | validation: 4.49637026508723]
	TIME [epoch: 0.763 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.653626723664201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.653626723664201 | validation: 4.75938950102106]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.647032175726286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.647032175726286 | validation: 4.026191901340504]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.116804935193545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.116804935193545 | validation: 3.727976207178201]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.038714914702732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038714914702732 | validation: 4.168157147379086]
	TIME [epoch: 0.694 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8955893532061383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8955893532061383 | validation: 3.7061154020301417]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.666195158436589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.666195158436589 | validation: 3.6617581295391464]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5088614262979707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5088614262979707 | validation: 3.184822673247754]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.298342023309658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.298342023309658 | validation: 3.6863304501808063]
	TIME [epoch: 0.698 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2354787673596705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2354787673596705 | validation: 2.4296486562009765]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4648086720987696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4648086720987696 | validation: 3.314269043807145]
	TIME [epoch: 0.697 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2634446842852665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2634446842852665 | validation: 3.4529215647330873]
	TIME [epoch: 0.695 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1443268111540994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1443268111540994 | validation: 2.6883761309479985]
	TIME [epoch: 0.694 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7850448971680364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7850448971680364 | validation: 2.5072416677384117]
	TIME [epoch: 0.695 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.70753437984984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.70753437984984 | validation: 2.7396793952829603]
	TIME [epoch: 0.696 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6568833139332004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6568833139332004 | validation: 2.5137495305556166]
	TIME [epoch: 0.697 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.638184757298237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.638184757298237 | validation: 2.5989274600989276]
	TIME [epoch: 0.697 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0779496433554416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0779496433554416 | validation: 2.931368880300624]
	TIME [epoch: 0.695 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9418011967025484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9418011967025484 | validation: 2.5860642492357937]
	TIME [epoch: 0.697 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.508038938086906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.508038938086906 | validation: 2.2270194300702064]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5360397311896152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5360397311896152 | validation: 2.0248304333032823]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4811479466072313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4811479466072313 | validation: 2.1842473483836975]
	TIME [epoch: 0.696 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3196674037432774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3196674037432774 | validation: 1.9995432511699087]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.234823008732502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.234823008732502 | validation: 1.7545968071181641]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.206256515694247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.206256515694247 | validation: 1.897123965458485]
	TIME [epoch: 0.696 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1942943808956232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1942943808956232 | validation: 1.9229375990557778]
	TIME [epoch: 0.697 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1959963795343755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1959963795343755 | validation: 1.8761220929997298]
	TIME [epoch: 0.697 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.123556072277597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.123556072277597 | validation: 1.669548519511272]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.023216278645946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.023216278645946 | validation: 1.757783097298199]
	TIME [epoch: 0.698 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9922175516000975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9922175516000975 | validation: 1.606793728898952]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.965675209239687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.965675209239687 | validation: 1.7016708316319402]
	TIME [epoch: 0.699 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9498685790004033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9498685790004033 | validation: 1.6280137284758789]
	TIME [epoch: 0.697 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.951355215897295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.951355215897295 | validation: 1.664595177947543]
	TIME [epoch: 0.697 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.950175520877238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.950175520877238 | validation: 1.597483152550834]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.944843573224451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.944843573224451 | validation: 1.700015606861777]
	TIME [epoch: 0.698 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9453369602342807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9453369602342807 | validation: 1.4844730379569662]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9810970196756879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9810970196756879 | validation: 1.8898097409688228]
	TIME [epoch: 0.698 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.039574877391487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.039574877391487 | validation: 1.540107351741299]
	TIME [epoch: 0.696 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9589033973702261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9589033973702261 | validation: 1.6675108164264594]
	TIME [epoch: 0.697 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9322052272459116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9322052272459116 | validation: 1.6252268455852275]
	TIME [epoch: 0.697 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9217701526773794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9217701526773794 | validation: 1.5717828078258766]
	TIME [epoch: 0.697 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9234382274887216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9234382274887216 | validation: 1.7023891265966116]
	TIME [epoch: 0.695 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9296861383712038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9296861383712038 | validation: 1.5458148837689312]
	TIME [epoch: 0.697 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9238295907283316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9238295907283316 | validation: 1.7440650513504616]
	TIME [epoch: 0.699 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9391801294500293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9391801294500293 | validation: 1.5090877665989872]
	TIME [epoch: 0.696 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.930845901844771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.930845901844771 | validation: 1.7477659466863231]
	TIME [epoch: 0.696 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9316035207730116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9316035207730116 | validation: 1.5102855862116789]
	TIME [epoch: 0.697 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9181828971961414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9181828971961414 | validation: 1.7299455719784156]
	TIME [epoch: 0.698 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9192750659805802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9192750659805802 | validation: 1.5367521077528716]
	TIME [epoch: 0.696 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9131228623885954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9131228623885954 | validation: 1.7096572370879182]
	TIME [epoch: 0.697 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9127666397839465		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.9127666397839465 | validation: 1.5239166414160765]
	TIME [epoch: 0.698 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.909337565984674		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.909337565984674 | validation: 1.785045731352466]
	TIME [epoch: 0.701 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9271000544263726		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.9271000544263726 | validation: 1.481842763503393]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9403053185097456		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.9403053185097456 | validation: 1.8376868152553116]
	TIME [epoch: 0.698 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.961671414948138		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.961671414948138 | validation: 1.6895997219527807]
	TIME [epoch: 0.698 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9023514932879568		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.9023514932879568 | validation: 1.462453215711889]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9149625388745002		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.9149625388745002 | validation: 1.8584368240138738]
	TIME [epoch: 0.696 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9569177419994392		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.9569177419994392 | validation: 1.5773694959910023]
	TIME [epoch: 0.696 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8963412782494669		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.8963412782494669 | validation: 1.6664531866335712]
	TIME [epoch: 0.698 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8890509416218333		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.8890509416218333 | validation: 1.6268772250201533]
	TIME [epoch: 0.697 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8913054403118832		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.8913054403118832 | validation: 1.6768105581644321]
	TIME [epoch: 0.701 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8899626038601278		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.8899626038601278 | validation: 1.6031993769581783]
	TIME [epoch: 0.696 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8770511033259092		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.8770511033259092 | validation: 1.7020870357691553]
	TIME [epoch: 0.696 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8744989499915432		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.8744989499915432 | validation: 1.5675510739708072]
	TIME [epoch: 0.695 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8861072563398689		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.8861072563398689 | validation: 1.8028492381559698]
	TIME [epoch: 0.696 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9168933209444003		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.9168933209444003 | validation: 1.5425515418913138]
	TIME [epoch: 0.697 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9079009056210496		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.9079009056210496 | validation: 1.7921930958900754]
	TIME [epoch: 0.699 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.908880957557168		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.908880957557168 | validation: 1.591314817409099]
	TIME [epoch: 0.697 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8666123666157177		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.8666123666157177 | validation: 1.6954638431526974]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8706531331371363		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8706531331371363 | validation: 1.58831665361812]
	TIME [epoch: 0.697 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8728244696345702		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.8728244696345702 | validation: 1.7888054409513314]
	TIME [epoch: 0.698 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.901597333490335		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.901597333490335 | validation: 1.5310213814005644]
	TIME [epoch: 0.696 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8899441980045464		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.8899441980045464 | validation: 1.8095112575648864]
	TIME [epoch: 0.696 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8977310674668253		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.8977310674668253 | validation: 1.5464599483577395]
	TIME [epoch: 0.697 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8689387078821365		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.8689387078821365 | validation: 1.746761496006135]
	TIME [epoch: 0.698 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8708689495565278		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.8708689495565278 | validation: 1.5924480306401774]
	TIME [epoch: 0.697 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8900526842732996		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.8900526842732996 | validation: 1.8135906350686082]
	TIME [epoch: 0.697 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9076755411750792		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.9076755411750792 | validation: 1.6023240641242928]
	TIME [epoch: 0.697 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8949604272437182		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.8949604272437182 | validation: 1.7592324276559042]
	TIME [epoch: 0.697 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8749428402689285		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.8749428402689285 | validation: 1.5518032763961747]
	TIME [epoch: 0.696 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8645023006104864		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.8645023006104864 | validation: 1.7456692091549806]
	TIME [epoch: 0.698 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8793813752589337		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.8793813752589337 | validation: 1.5710775885228714]
	TIME [epoch: 0.697 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8622814096903062		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.8622814096903062 | validation: 1.7812729180244256]
	TIME [epoch: 0.697 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8746189809654528		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.8746189809654528 | validation: 1.4835026542891363]
	TIME [epoch: 0.696 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8696406784002855		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.8696406784002855 | validation: 1.885619023465292]
	TIME [epoch: 0.696 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9040394328287624		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.9040394328287624 | validation: 1.506885951252949]
	TIME [epoch: 0.696 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8713197544691782		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8713197544691782 | validation: 1.728486150890387]
	TIME [epoch: 0.696 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8520814321333827		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.8520814321333827 | validation: 1.6086614062877538]
	TIME [epoch: 0.696 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8454827931130577		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.8454827931130577 | validation: 1.667504693985]
	TIME [epoch: 0.696 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8592009859183338		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.8592009859183338 | validation: 1.8786064961547404]
	TIME [epoch: 0.696 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9778074910595593		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.9778074910595593 | validation: 1.7409299018865427]
	TIME [epoch: 0.697 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8921976048262326		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.8921976048262326 | validation: 1.6287026908149445]
	TIME [epoch: 0.696 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8501945618960915		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.8501945618960915 | validation: 1.6872584842261173]
	TIME [epoch: 0.697 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.85637253746129		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.85637253746129 | validation: 1.6834714048061414]
	TIME [epoch: 0.697 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8446441866391738		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.8446441866391738 | validation: 1.5495406876133122]
	TIME [epoch: 0.696 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8403983238588058		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.8403983238588058 | validation: 1.8176861423592436]
	TIME [epoch: 0.695 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8596593226904259		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.8596593226904259 | validation: 1.3166324121675275]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.940315377297041		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.940315377297041 | validation: 1.9998537254508044]
	TIME [epoch: 0.695 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9555336148988884		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.9555336148988884 | validation: 1.6820402234812475]
	TIME [epoch: 0.692 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8427559086110432		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.8427559086110432 | validation: 1.4391658296132692]
	TIME [epoch: 0.697 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8815717392419697		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.8815717392419697 | validation: 1.763898332437087]
	TIME [epoch: 0.696 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8504283865534223		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8504283865534223 | validation: 1.6712573324946363]
	TIME [epoch: 0.699 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8271094466387365		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.8271094466387365 | validation: 1.5155614702124458]
	TIME [epoch: 0.695 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8306204122844514		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.8306204122844514 | validation: 1.7179894828754252]
	TIME [epoch: 0.696 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8259101763116516		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.8259101763116516 | validation: 1.5846368946628655]
	TIME [epoch: 0.698 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8203984019071215		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.8203984019071215 | validation: 1.6485007317228064]
	TIME [epoch: 0.695 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8045933108059706		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.8045933108059706 | validation: 1.6389849276869577]
	TIME [epoch: 0.695 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8175415026276727		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.8175415026276727 | validation: 1.7138559297092941]
	TIME [epoch: 0.695 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.95374362084716		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.95374362084716 | validation: 1.9413008310185018]
	TIME [epoch: 0.696 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9560829128514152		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.9560829128514152 | validation: 1.5630335829741822]
	TIME [epoch: 0.696 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8122444780362588		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.8122444780362588 | validation: 1.706900148712213]
	TIME [epoch: 0.695 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8581500950497596		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.8581500950497596 | validation: 1.4976658503920655]
	TIME [epoch: 0.695 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8186928337544657		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.8186928337544657 | validation: 1.79783825209461]
	TIME [epoch: 0.694 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8456825465503766		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.8456825465503766 | validation: 1.4576884400324852]
	TIME [epoch: 0.696 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.830309861187923		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.830309861187923 | validation: 1.7677359256082903]
	TIME [epoch: 0.696 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.819195818597594		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.819195818597594 | validation: 1.4882502716853958]
	TIME [epoch: 0.696 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8144309640712961		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.8144309640712961 | validation: 1.8477562093435118]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8387978936116023		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.8387978936116023 | validation: 1.3296686517543768]
	TIME [epoch: 0.696 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8581502318512109		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.8581502318512109 | validation: 1.8227243964010276]
	TIME [epoch: 0.695 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8390610855630625		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.8390610855630625 | validation: 1.4328464798449838]
	TIME [epoch: 0.696 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8104958857005187		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.8104958857005187 | validation: 1.6327162426958646]
	TIME [epoch: 0.695 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.784484632352166		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.784484632352166 | validation: 1.6111467980379979]
	TIME [epoch: 0.695 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7806253838286814		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.7806253838286814 | validation: 1.5324371026139372]
	TIME [epoch: 0.696 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7802030828943014		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.7802030828943014 | validation: 1.7417315063993692]
	TIME [epoch: 0.696 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.800743933101317		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.800743933101317 | validation: 1.295919079893059]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.877267193526014		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.877267193526014 | validation: 2.0298501835497835]
	TIME [epoch: 0.692 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9555702776935846		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.9555702776935846 | validation: 1.3487337828800818]
	TIME [epoch: 0.69 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8191419572411838		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.8191419572411838 | validation: 1.6334503116478851]
	TIME [epoch: 0.691 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8068448951365177		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.8068448951365177 | validation: 1.6468391034040146]
	TIME [epoch: 0.695 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7778741968661786		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.7778741968661786 | validation: 1.5334892437818006]
	TIME [epoch: 0.695 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.74804955936525		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.74804955936525 | validation: 1.596490726545336]
	TIME [epoch: 0.696 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.74304072268135		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.74304072268135 | validation: 1.5259233620552441]
	TIME [epoch: 0.694 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7459791549415833		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.7459791549415833 | validation: 1.7513051721047703]
	TIME [epoch: 0.691 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7664003962257837		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.7664003962257837 | validation: 1.2329213233136576]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9379807979793202		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.9379807979793202 | validation: 2.0548150079570315]
	TIME [epoch: 0.691 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9656612994357447		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.9656612994357447 | validation: 1.320456256794719]
	TIME [epoch: 0.694 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7661402313111338		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.7661402313111338 | validation: 1.4301870347309689]
	TIME [epoch: 0.693 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7282878282877148		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.7282878282877148 | validation: 1.6793682412985242]
	TIME [epoch: 0.694 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7396585652657615		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.7396585652657615 | validation: 1.4152212087671332]
	TIME [epoch: 0.69 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6992575589510557		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.6992575589510557 | validation: 1.5148203230685424]
	TIME [epoch: 0.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6770145681573256		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.6770145681573256 | validation: 1.5603852652291565]
	TIME [epoch: 0.688 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6673492121116948		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.6673492121116948 | validation: 1.3708073621913448]
	TIME [epoch: 0.689 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.682571335282583		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.682571335282583 | validation: 1.938258299580467]
	TIME [epoch: 0.695 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8211554755397885		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.8211554755397885 | validation: 1.1092143983909064]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2024464574654683		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 2.2024464574654683 | validation: 1.418982774564015]
	TIME [epoch: 0.696 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.635351269176677		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.635351269176677 | validation: 1.9022930908165578]
	TIME [epoch: 0.694 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.842535124233405		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.842535124233405 | validation: 1.2309081498620673]
	TIME [epoch: 0.694 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7303680013849		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.7303680013849 | validation: 1.4400907075381566]
	TIME [epoch: 0.694 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6268244340343574		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.6268244340343574 | validation: 1.6210721201187355]
	TIME [epoch: 0.694 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6489105417884915		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.6489105417884915 | validation: 1.3104739731991841]
	TIME [epoch: 0.693 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6337212777462793		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.6337212777462793 | validation: 1.5622261095804806]
	TIME [epoch: 0.693 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5960507091726717		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.5960507091726717 | validation: 1.312521506518905]
	TIME [epoch: 0.694 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5746867439004624		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5746867439004624 | validation: 1.7184002381642138]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6305323133458216		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.6305323133458216 | validation: 1.0413144159355363]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.942390419298518		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.942390419298518 | validation: 1.809968972793417]
	TIME [epoch: 0.694 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7000114160971365		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.7000114160971365 | validation: 1.216553982923693]
	TIME [epoch: 0.695 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5828449551032793		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.5828449551032793 | validation: 1.462935528996045]
	TIME [epoch: 0.694 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5243213540924234		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.5243213540924234 | validation: 1.3933055849439517]
	TIME [epoch: 0.693 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4916208737521317		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.4916208737521317 | validation: 1.25141239791206]
	TIME [epoch: 0.694 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.480898077834084		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.480898077834084 | validation: 1.6263887792108727]
	TIME [epoch: 0.695 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5362677224539323		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.5362677224539323 | validation: 1.0362309357306718]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.849436774464471		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.849436774464471 | validation: 1.8016094228844217]
	TIME [epoch: 0.694 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6735696621605314		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.6735696621605314 | validation: 1.13541031302185]
	TIME [epoch: 0.693 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6459160256517305		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.6459160256517305 | validation: 1.5074160003821344]
	TIME [epoch: 0.694 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4884739351363576		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.4884739351363576 | validation: 1.2435635069207345]
	TIME [epoch: 0.693 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.406328583010813		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.406328583010813 | validation: 1.3165127376818608]
	TIME [epoch: 0.693 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.386627081778869		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.386627081778869 | validation: 1.289399880333591]
	TIME [epoch: 0.694 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3649400886406016		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3649400886406016 | validation: 1.2651717139599858]
	TIME [epoch: 0.693 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3380912762070745		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3380912762070745 | validation: 1.1969207178555983]
	TIME [epoch: 0.693 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3057298294858208		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3057298294858208 | validation: 1.3350476800538607]
	TIME [epoch: 0.695 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106054892552348		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3106054892552348 | validation: 1.0693539900270876]
	TIME [epoch: 0.695 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8751087396984116		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.8751087396984116 | validation: 2.521070516076424]
	TIME [epoch: 0.694 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4913885430308076		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 2.4913885430308076 | validation: 1.272921776632262]
	TIME [epoch: 0.693 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2762347553174607		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2762347553174607 | validation: 1.0453103043899865]
	TIME [epoch: 0.694 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7022609019287007		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.7022609019287007 | validation: 1.6165602197623525]
	TIME [epoch: 0.693 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5104689187761398		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.5104689187761398 | validation: 1.1594653434890376]
	TIME [epoch: 0.693 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3171382123726574		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3171382123726574 | validation: 1.2375093630460428]
	TIME [epoch: 0.692 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26397179749151		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.26397179749151 | validation: 1.2387997701637752]
	TIME [epoch: 0.697 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2544965980659086		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.2544965980659086 | validation: 1.1473406729226492]
	TIME [epoch: 0.693 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.245406970096931		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.245406970096931 | validation: 1.3311539046316754]
	TIME [epoch: 0.693 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2576772190346674		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.2576772190346674 | validation: 1.0312678528582704]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.509625335814314		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.509625335814314 | validation: 1.861234744617736]
	TIME [epoch: 0.698 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8256629590275015		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.8256629590275015 | validation: 1.0697976468986707]
	TIME [epoch: 0.693 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3332329759910004		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.3332329759910004 | validation: 1.259759938716864]
	TIME [epoch: 0.691 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2209340942951676		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2209340942951676 | validation: 1.1664409509247562]
	TIME [epoch: 0.695 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1946549615247593		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1946549615247593 | validation: 1.1524656317682413]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1799527540903467		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1799527540903467 | validation: 1.1751383090687957]
	TIME [epoch: 0.692 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171847828504943		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.171847828504943 | validation: 1.0971401569331722]
	TIME [epoch: 0.692 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1802666487358096		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1802666487358096 | validation: 1.5626451907408614]
	TIME [epoch: 0.693 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.443924259654636		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.443924259654636 | validation: 1.2166088644418724]
	TIME [epoch: 0.693 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0774211604293633		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 2.0774211604293633 | validation: 1.401122416336583]
	TIME [epoch: 0.693 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2415383009445708		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.2415383009445708 | validation: 1.1488780089447208]
	TIME [epoch: 0.692 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1546119308889777		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1546119308889777 | validation: 1.1094578646489544]
	TIME [epoch: 0.695 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155996560910286		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.155996560910286 | validation: 1.3228415621233676]
	TIME [epoch: 0.693 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2072741216821081		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2072741216821081 | validation: 1.0473568642443367]
	TIME [epoch: 0.693 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4558873610412997		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.4558873610412997 | validation: 1.687192602380523]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5841327070653375		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.5841327070653375 | validation: 1.0399179727021834]
	TIME [epoch: 0.694 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2174793553762502		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2174793553762502 | validation: 1.2119658550433225]
	TIME [epoch: 0.693 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1388907199770057		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1388907199770057 | validation: 1.1048911897754032]
	TIME [epoch: 0.693 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1244041390844464		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.1244041390844464 | validation: 1.2070632773337129]
	TIME [epoch: 173 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254130695661178		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1254130695661178 | validation: 1.034022381824558]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1993905942409775		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1993905942409775 | validation: 1.6286123107449286]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5273058926722543		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.5273058926722543 | validation: 1.0096243767705666]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130455062285028		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.4130455062285028 | validation: 1.3889365778501064]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2416881046102572		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2416881046102572 | validation: 1.0459084921554096]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1529172796741616		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1529172796741616 | validation: 1.2335752020182353]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1312702185865786		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1312702185865786 | validation: 1.0247561347804879]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1677962636044055		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1677962636044055 | validation: 1.376722370605108]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268647445874022		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.268647445874022 | validation: 0.9742469237691008]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3271805252249127		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.3271805252249127 | validation: 1.4372635888972647]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2914805169684331		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2914805169684331 | validation: 1.0162546476244696]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1428651773592795		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1428651773592795 | validation: 1.233654073565792]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1061472046052567		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1061472046052567 | validation: 1.0264594868092674]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0953203953386208		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.0953203953386208 | validation: 1.2806268153536824]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1325917210302279		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.1325917210302279 | validation: 0.9929671500276567]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2356524906346638		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2356524906346638 | validation: 1.5343344466907114]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3973406809380438		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.3973406809380438 | validation: 0.9952580789487393]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1393602364226771		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.1393602364226771 | validation: 1.1815832833067348]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0687701353544734		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0687701353544734 | validation: 1.040739406236077]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0548591177573925		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.0548591177573925 | validation: 1.2523348532003886]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0925250691101238		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.0925250691101238 | validation: 0.9871337511014595]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.249350994337129		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.249350994337129 | validation: 1.5504158273640616]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4354028711803841		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.4354028711803841 | validation: 1.0162504500984093]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0842514707263158		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.0842514707263158 | validation: 1.1027193584425674]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016795214200904		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.016795214200904 | validation: 1.1129119874977658]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0114581343760254		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.0114581343760254 | validation: 1.037237665158556]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0272092129425696		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.0272092129425696 | validation: 1.3044373462912038]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1432827973832826		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1432827973832826 | validation: 0.9761886577969779]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4511270463235741		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.4511270463235741 | validation: 1.49385879379117]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.334559909248671		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.334559909248671 | validation: 1.0712129539553352]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0265040586746261		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.0265040586746261 | validation: 1.0104113572161377]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030621598293973		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.030621598293973 | validation: 1.273922826500307]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163701735388807		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1163701735388807 | validation: 0.9801293363678909]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20381230908087		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.20381230908087 | validation: 1.3863781240391417]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2295185811086449		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2295185811086449 | validation: 1.0135614043100054]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0459984724207405		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.0459984724207405 | validation: 1.1249498804345721]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.997505188703247		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.997505188703247 | validation: 1.0559955860617993]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896213629857092		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9896213629857092 | validation: 1.087789402941923]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796222260614421		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9796222260614421 | validation: 1.0123492043465343]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9742619118190823		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9742619118190823 | validation: 1.2076288167643559]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0362563106145897		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.0362563106145897 | validation: 0.9625084463444331]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31209928917633		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.31209928917633 | validation: 1.5523859551724042]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4273961816717537		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.4273961816717537 | validation: 1.0862075294967348]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864949358894798		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9864949358894798 | validation: 0.9726870478676647]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0765812675421098		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0765812675421098 | validation: 1.403681007996619]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2571199062594811		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.2571199062594811 | validation: 0.9768416314620872]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371587560798075		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0371587560798075 | validation: 1.1432568198499775]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785164546870021		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.9785164546870021 | validation: 0.9876390035638534]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589830178961557		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9589830178961557 | validation: 1.1379230276266197]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.962280012282026		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.962280012282026 | validation: 0.9540952490196107]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0004938447405345		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0004938447405345 | validation: 1.2995381313655383]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128010362976957		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.128010362976957 | validation: 0.9329340629513546]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1673761382886685		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.1673761382886685 | validation: 1.3021038633391113]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133978061668515		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.133978061668515 | validation: 0.9899800630148885]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9621609220490973		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9621609220490973 | validation: 1.056888221038468]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259330135212249		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9259330135212249 | validation: 1.050263854703647]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220075771884879		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9220075771884879 | validation: 0.9949542007970795]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199086891527103		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.9199086891527103 | validation: 1.0967048139320756]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9393677812376532		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.9393677812376532 | validation: 0.9317007062912634]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040836822075686		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.040836822075686 | validation: 1.4957101754322364]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3513459382080453		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.3513459382080453 | validation: 0.980842802181053]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978110179879737		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.978110179879737 | validation: 1.0237570089403263]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936330557615909		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8936330557615909 | validation: 1.0428704132669029]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036279593741554		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9036279593741554 | validation: 0.9697148488724292]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280695642145957		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9280695642145957 | validation: 1.320841854177177]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319572407247862		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.1319572407247862 | validation: 0.9496597668954732]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.229007332917358		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.229007332917358 | validation: 1.2909214016122679]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0848899604268183		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0848899604268183 | validation: 1.0136499587747203]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191411752561871		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.9191411752561871 | validation: 0.9810016951337103]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940725160374068		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8940725160374068 | validation: 1.1444998305595913]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9755546999091315		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9755546999091315 | validation: 0.9117734738875035]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1620270581266434		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.1620270581266434 | validation: 1.3842802057407813]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1956286131659635		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1956286131659635 | validation: 1.054407287444526]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440182543732891		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.9440182543732891 | validation: 0.9594671401286003]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301700547002003		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9301700547002003 | validation: 1.1792271930230378]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9942801102416158		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9942801102416158 | validation: 0.9116426945564314]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0522365954070751		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.0522365954070751 | validation: 1.2540843246827986]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0532405141261527		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.0532405141261527 | validation: 0.9638682300095432]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249332533433546		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.9249332533433546 | validation: 1.0413353100053893]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782283726396019		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8782283726396019 | validation: 0.9899060906881311]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703396045909448		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8703396045909448 | validation: 0.9956796508272]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667796926266386		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8667796926266386 | validation: 1.0157745729409224]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684590724864151		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8684590724864151 | validation: 0.9828258560097898]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854485796595797		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.854485796595797 | validation: 1.0343525091998191]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706538424621394		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8706538424621394 | validation: 0.8954070684831028]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9965822202785469		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.9965822202785469 | validation: 1.548465548693081]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4131550035342895		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.4131550035342895 | validation: 1.0998667597447889]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9678142254002501		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.9678142254002501 | validation: 0.958980157668237]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.090361698620576		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.090361698620576 | validation: 1.3186008271962277]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370395429038862		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.1370395429038862 | validation: 0.9516954540540127]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159684023269925		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.9159684023269925 | validation: 0.9959012910002422]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616877483110877		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8616877483110877 | validation: 0.9752122428795471]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509182609041736		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8509182609041736 | validation: 0.9864748167243317]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481211615694505		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.8481211615694505 | validation: 0.9515739789334724]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84271722434572		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.84271722434572 | validation: 1.008283393426949]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468312926724917		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8468312926724917 | validation: 0.8780205629992994]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9731221949870446		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.9731221949870446 | validation: 1.4900047712886426]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3445848188585825		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.3445848188585825 | validation: 1.049001405611027]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9185639053716323		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.9185639053716323 | validation: 0.8814946129728256]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0139214181726834		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0139214181726834 | validation: 1.3352702784426373]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1737951689908634		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1737951689908634 | validation: 0.937592839167132]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624427208284939		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8624427208284939 | validation: 0.9269282777663386]
	TIME [epoch: 1.37 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546405360888136		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.8546405360888136 | validation: 1.1434723540518807]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9509658647528614		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.9509658647528614 | validation: 0.8735400795465884]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9953104840166597		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.9953104840166597 | validation: 1.2160957363882403]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0180625086797017		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.0180625086797017 | validation: 0.9492699459331471]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646652514320945		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8646652514320945 | validation: 0.9586795074338899]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290426313246166		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8290426313246166 | validation: 0.9970629274786748]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368920902392156		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8368920902392156 | validation: 0.8990083533592724]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656210587893515		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8656210587893515 | validation: 1.178058720134919]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9894601712942148		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.9894601712942148 | validation: 0.8992055711628053]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784461244820192		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.9784461244820192 | validation: 1.128801709794491]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232525606286813		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.9232525606286813 | validation: 0.8852246544568699]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804899500655138		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8804899500655138 | validation: 1.136336725396135]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945997501309339		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.945997501309339 | validation: 0.8609648511755869]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9383914007472829		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.9383914007472829 | validation: 1.1582931590982186]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9737694597809642		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.9737694597809642 | validation: 0.9084863246566158]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622326147717062		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8622326147717062 | validation: 0.9842429262644704]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192518682682968		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.8192518682682968 | validation: 0.9274393414865115]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240855755745927		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8240855755745927 | validation: 1.006323265867328]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298650733409093		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8298650733409093 | validation: 0.8745230204311071]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818682772160683		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.8818682772160683 | validation: 1.2566657713301161]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075458209278101		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.075458209278101 | validation: 0.8780592185329426]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102548374340903		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.9102548374340903 | validation: 1.0506359056941992]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732969604524694		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8732969604524694 | validation: 0.871232902199234]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727110968409365		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8727110968409365 | validation: 1.1328640943320387]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504683416912013		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.9504683416912013 | validation: 0.8818543802226106]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080389427453932		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.9080389427453932 | validation: 1.1062867173455864]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095488404814296		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.9095488404814296 | validation: 0.8538964343779029]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846251707652514		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8846251707652514 | validation: 1.1261465860295885]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050687984105616		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.9050687984105616 | validation: 0.8633882864341281]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885836717400565		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.885836717400565 | validation: 1.0995582840752065]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9106940628324116		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.9106940628324116 | validation: 0.8897591272286536]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8615976152426317		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.8615976152426317 | validation: 1.039654160058319]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504256425867592		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8504256425867592 | validation: 0.8659577403730744]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856368060347148		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.856368060347148 | validation: 1.1454136111893416]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9416337149400539		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.9416337149400539 | validation: 0.8903924711944127]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738942933435498		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8738942933435498 | validation: 0.9958516739640022]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156757440062393		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.8156757440062393 | validation: 0.8778238689415248]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200137908772069		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8200137908772069 | validation: 1.0648966993150595]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773708704246808		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.8773708704246808 | validation: 0.863809675364541]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312380517448862		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.9312380517448862 | validation: 1.1374619798959544]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9416411134639512		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.9416411134639512 | validation: 0.8726685604320823]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475369857137682		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.8475369857137682 | validation: 1.0453203959190516]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531244880540328		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8531244880540328 | validation: 0.8456711511008204]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86586787258289		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.86586787258289 | validation: 1.1278841683332566]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412444964408809		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9412444964408809 | validation: 0.8949287551992976]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845496035994551		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.845496035994551 | validation: 0.9975868339598245]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151602032028257		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.8151602032028257 | validation: 0.857695613029025]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236856156380752		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.8236856156380752 | validation: 1.096328969208655]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221560411138338		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.9221560411138338 | validation: 0.8782422814031996]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887951529012602		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.887951529012602 | validation: 1.0376627049274896]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417793065471534		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.8417793065471534 | validation: 0.828948485099442]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711157823198652		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.8711157823198652 | validation: 1.1315968436337087]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606739372683555		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.9606739372683555 | validation: 0.9112458372840667]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500611680870858		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.8500611680870858 | validation: 0.9256357059378995]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830189037120912		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7830189037120912 | validation: 0.9420075287700804]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802608960241962		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.802608960241962 | validation: 0.8669168830806604]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127144058533854		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.8127144058533854 | validation: 1.1594598272840182]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938778097786749		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.938778097786749 | validation: 0.8856948034395451]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085761782678565		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9085761782678565 | validation: 1.018767934464188]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221586749463393		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.8221586749463393 | validation: 0.8339464746218948]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379729155030057		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.8379729155030057 | validation: 1.164746444484344]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662435954448525		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.9662435954448525 | validation: 0.8838778051337098]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607704944126744		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.8607704944126744 | validation: 0.9956868139990542]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125125033168067		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.8125125033168067 | validation: 0.8435567445770542]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030246150321508		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.8030246150321508 | validation: 1.049891927379489]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605634070963986		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.8605634070963986 | validation: 0.8329019687963272]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123944707820019		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9123944707820019 | validation: 1.1294298565671033]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183299785447524		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.9183299785447524 | validation: 0.9063067676861412]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076183910358605		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.8076183910358605 | validation: 0.8887221814639578]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833607805557281		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.7833607805557281 | validation: 1.0163961948774005]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373769093851323		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.8373769093851323 | validation: 0.8194090078953015]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9095709064707892		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.9095709064707892 | validation: 1.1630134842397652]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9570072949259003		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.9570072949259003 | validation: 0.93998286832722]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308306667927482		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.8308306667927482 | validation: 0.8524762980176231]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014831792738872		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.8014831792738872 | validation: 1.0799956844525156]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053593349586939		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.9053593349586939 | validation: 0.81530133029317]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8948951703575422		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.8948951703575422 | validation: 1.0696037719678075]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648350445896316		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.8648350445896316 | validation: 0.9044844448997191]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943851843314489		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7943851843314489 | validation: 0.8900132165094888]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721392347033195		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7721392347033195 | validation: 0.9711509563888043]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897110810418955		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7897110810418955 | validation: 0.8146341954402463]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352904819475967		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.8352904819475967 | validation: 1.1552759959462795]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486830855741656		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9486830855741656 | validation: 0.8947944606704775]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137543929304983		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.8137543929304983 | validation: 0.8902428621621001]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715770297443334		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.7715770297443334 | validation: 0.9425301928267796]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785781863114812		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.785781863114812 | validation: 0.8287776048244933]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240069351173116		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.8240069351173116 | validation: 1.1419489997414403]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9411775183655655		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.9411775183655655 | validation: 0.9067660124203318]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422570679878328		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.8422570679878328 | validation: 0.8985678376953334]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769135712637948		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.7769135712637948 | validation: 1.0070019836004673]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824788359218315		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.824788359218315 | validation: 0.7812611491479349]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9567678944084294		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.9567678944084294 | validation: 1.2112109661246384]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9851870669225654		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.9851870669225654 | validation: 0.9584045573833303]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331402030866665		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.8331402030866665 | validation: 0.8190623526587513]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356809503844578		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.8356809503844578 | validation: 1.0931145712096246]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012374188400254		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9012374188400254 | validation: 0.8142022791979171]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048807181696248		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.8048807181696248 | validation: 0.9730138172772836]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963409320430642		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.7963409320430642 | validation: 0.8616569363310757]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786172785293948		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.786172785293948 | validation: 0.9736742736838258]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908106758762082		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.7908106758762082 | validation: 0.8200032570775658]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011321072953852		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.8011321072953852 | validation: 1.0387246842638072]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426565613008355		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.8426565613008355 | validation: 0.8384345643929575]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8242112709372208		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.8242112709372208 | validation: 1.0149275493712497]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102407647908635		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.8102407647908635 | validation: 0.8187831529938046]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037367822477461		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.8037367822477461 | validation: 1.0114817846947315]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827347396136921		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.827347396136921 | validation: 0.8138839642634518]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292009659035937		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.8292009659035937 | validation: 1.0534560192304454]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546127518947485		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.8546127518947485 | validation: 0.8524575182001389]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821082769503422		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7821082769503422 | validation: 0.9001754331153524]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550380469480601		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7550380469480601 | validation: 0.8771282863200578]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534668025940812		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.7534668025940812 | validation: 0.8759921316376182]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597455982939806		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.7597455982939806 | validation: 0.9690210214024682]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893985431881636		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7893985431881636 | validation: 0.797054876306023]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8615234948938816		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.8615234948938816 | validation: 1.1495104590734797]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.961146685290907		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.961146685290907 | validation: 0.8827906110343204]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107514036536019		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.8107514036536019 | validation: 0.8317812741895394]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760101349537272		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.7760101349537272 | validation: 1.0885633456750161]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080449807432908		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9080449807432908 | validation: 0.7961675243637875]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208735458276022		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.8208735458276022 | validation: 0.9847240556779566]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969442557027421		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.7969442557027421 | validation: 0.8521331491926745]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625282958173487		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.7625282958173487 | validation: 0.9327778728640038]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759937506809153		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.759937506809153 | validation: 0.8053937392182219]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871967383857432		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.7871967383857432 | validation: 1.0677631484780856]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800187456566008		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.8800187456566008 | validation: 0.8652584885749536]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064230822154164		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.8064230822154164 | validation: 0.881856182490202]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530617133483034		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.7530617133483034 | validation: 0.9403031794087007]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762988896254993		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.7762988896254993 | validation: 0.7668350693966496]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798576577897879		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.8798576577897879 | validation: 1.132055228078272]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9285383681103123		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9285383681103123 | validation: 0.8856275345845532]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914650603265289		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.7914650603265289 | validation: 0.7904369356963964]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922254489813633		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.7922254489813633 | validation: 1.0631268378025407]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808688337834154		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.8808688337834154 | validation: 0.8007886977767632]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947327833719681		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7947327833719681 | validation: 0.9528467950782066]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747527171529462		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.7747527171529462 | validation: 0.8612399914493308]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526650847903037		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.7526650847903037 | validation: 0.9077947372539721]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578099942770617		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.7578099942770617 | validation: 0.8161937260420496]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657845317291052		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.7657845317291052 | validation: 1.0149251383272873]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156977528649867		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.8156977528649867 | validation: 0.8190327453355496]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8053720725447172		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.8053720725447172 | validation: 0.9884464287001926]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849747578601677		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.7849747578601677 | validation: 0.7826381442284872]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848786814971439		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.7848786814971439 | validation: 1.0151263093916398]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8175986776959764		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.8175986776959764 | validation: 0.8014386482182385]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858129381191507		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.7858129381191507 | validation: 0.9707581477287403]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771588615194696		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.771588615194696 | validation: 0.8096411145089069]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629322762331185		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.7629322762331185 | validation: 0.9771287313641225]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886188248346039		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.7886188248346039 | validation: 0.7833736989423041]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957754671901747		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.7957754671901747 | validation: 0.9996905384127963]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810054985732653		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.810054985732653 | validation: 0.7992911140727585]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660691562471179		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.7660691562471179 | validation: 0.911298606512802]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568701444022125		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.7568701444022125 | validation: 0.785115456298304]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732304356365403		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7732304356365403 | validation: 1.0152844865418753]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192698834393969		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.8192698834393969 | validation: 0.802121442644806]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847058793624896		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.7847058793624896 | validation: 0.9472490281183799]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678233834254039		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.7678233834254039 | validation: 0.7814296265519858]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696584295115543		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.7696584295115543 | validation: 0.9680490368517447]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899473924510483		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.7899473924510483 | validation: 0.7958228901730845]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701624911573443		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.7701624911573443 | validation: 0.9379607921680368]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651372672079643		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.7651372672079643 | validation: 0.7866190154425479]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719212390352793		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.7719212390352793 | validation: 0.959318511459723]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870513454237077		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.7870513454237077 | validation: 0.7957790526817172]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610573066711477		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7610573066711477 | validation: 0.956903237249327]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627751657832227		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.7627751657832227 | validation: 0.782224059899466]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653550907906148		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.7653550907906148 | validation: 0.985582502440217]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963575127502043		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.7963575127502043 | validation: 0.8012043652182824]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631809480958003		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.7631809480958003 | validation: 0.9320654706585849]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525776591290947		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.7525776591290947 | validation: 0.7824397626417043]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587255780448896		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.7587255780448896 | validation: 0.9396631589329438]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735962961695759		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.7735962961695759 | validation: 0.7852551598178887]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671810939877358		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.7671810939877358 | validation: 0.9695390688053127]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827852177183137		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.7827852177183137 | validation: 0.7790155853963407]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547971904928398		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.7547971904928398 | validation: 0.9364151393494625]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622192159364712		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.7622192159364712 | validation: 0.7691850859189091]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651245284606563		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.7651245284606563 | validation: 0.9706763301278873]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987184928240606		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.7987184928240606 | validation: 0.783899378310414]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471956931350968		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.7471956931350968 | validation: 0.904282063922836]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733866662776711		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.733866662776711 | validation: 0.823453704572854]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295201847066122		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.7295201847066122 | validation: 0.8956966092329859]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372057640311787		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.7372057640311787 | validation: 0.8053528585358466]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245471813589104		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.7245471813589104 | validation: 0.950273229890586]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506380372187758		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.7506380372187758 | validation: 0.7582710073478547]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021494681820868		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.8021494681820868 | validation: 1.0547376799011197]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596769792449174		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.8596769792449174 | validation: 0.8352425516655644]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249257907269694		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.7249257907269694 | validation: 0.7940101570709045]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243066966609125		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.7243066966609125 | validation: 0.9668881840521802]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818114318042518		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.7818114318042518 | validation: 0.7790119797020822]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774711098513025		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.7774711098513025 | validation: 0.9534502674665082]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829754578355013		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.7829754578355013 | validation: 0.7950956863739895]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356696333793992		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.7356696333793992 | validation: 0.8781232609666635]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263922845844679		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.7263922845844679 | validation: 0.7669535622939091]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457414035715209		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.7457414035715209 | validation: 0.9656758893809873]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911425598091754		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.7911425598091754 | validation: 0.7807670950177772]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748768789467854		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.748768789467854 | validation: 0.888246149769429]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321847365911899		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.7321847365911899 | validation: 0.7631779816424193]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359049962158036		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.7359049962158036 | validation: 0.9580986110149315]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835522743834445		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.7835522743834445 | validation: 0.7382049699006465]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587960575944186		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.7587960575944186 | validation: 0.933710217459495]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7604839344261971		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.7604839344261971 | validation: 0.7576390637349747]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248367727921032		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.7248367727921032 | validation: 0.8931997595911216]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327962595173404		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.7327962595173404 | validation: 0.7745633353260759]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201727771983372		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.7201727771983372 | validation: 0.8911225738651215]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352458163979532		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.7352458163979532 | validation: 0.783938739812545]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410861580496194		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.7410861580496194 | validation: 0.915140095277954]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525455909426532		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.7525455909426532 | validation: 0.8051671739697643]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179168828368411		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.7179168828368411 | validation: 0.8706240819633391]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055308663149089		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.7055308663149089 | validation: 0.7842431094731261]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158373202582979		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.7158373202582979 | validation: 0.9341462998714705]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744134485603121		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.744134485603121 | validation: 0.7476158879070548]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950097983157225		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.7950097983157225 | validation: 1.026995339224209]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322495985270848		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.8322495985270848 | validation: 0.8460734282062028]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268739660760746		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.7268739660760746 | validation: 0.7456184452535535]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325098451846549		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.7325098451846549 | validation: 0.9592965879121422]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962631788643089		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.7962631788643089 | validation: 0.7516764135423801]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7342183412908042		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.7342183412908042 | validation: 0.8778613655461307]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308728945534807		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.7308728945534807 | validation: 0.789666771317084]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145255247353904		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.7145255247353904 | validation: 0.8530287346297272]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715362513690382		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.715362513690382 | validation: 0.7762355178698911]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116421324695634		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.7116421324695634 | validation: 0.919182470367034]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535766303944018		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.7535766303944018 | validation: 0.7455233335266905]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307883250901855		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.7307883250901855 | validation: 0.9177755371585894]
	TIME [epoch: 2.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333021453685566		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.7333021453685566 | validation: 0.7326441974383869]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439293747829472		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.7439293747829472 | validation: 0.9484810164431092]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626397887086305		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.7626397887086305 | validation: 0.760517926205541]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094585324192693		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.7094585324192693 | validation: 0.8473543743276123]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016193519792501		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.7016193519792501 | validation: 0.7587430216396217]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700370734247455		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.700370734247455 | validation: 0.8940631406515616]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421616632403542		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.7421616632403542 | validation: 0.7622419686552798]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485544915932073		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.7485544915932073 | validation: 0.8946195765682154]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570996240966221		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.7570996240966221 | validation: 0.7577399522156174]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042194145842092		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.7042194145842092 | validation: 0.8310147585384817]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7000583952242595		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.7000583952242595 | validation: 0.7327146633308752]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162392423045807		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.7162392423045807 | validation: 0.929771872619855]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504156299851684		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.7504156299851684 | validation: 0.6992138507271909]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578595710132738		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.7578595710132738 | validation: 0.9369700769459318]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754126498328014		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.754126498328014 | validation: 0.7807675516530692]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932718121405134		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.6932718121405134 | validation: 0.7949321555040276]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925110358902898		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.6925110358902898 | validation: 0.83777387163533]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693996619249942		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.693996619249942 | validation: 0.7386276198125781]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009515390557906		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.7009515390557906 | validation: 0.9247795514797957]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530441802450614		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.7530441802450614 | validation: 0.7367059371270828]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317385502110793		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.7317385502110793 | validation: 0.8604838109891699]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706908469595932		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.706908469595932 | validation: 0.7275134956935467]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953421437251873		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.6953421437251873 | validation: 0.875851075770796]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158006786749881		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.7158006786749881 | validation: 0.7713711122002153]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7044033893166979		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.7044033893166979 | validation: 0.8330854259622161]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934854444048112		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.6934854444048112 | validation: 0.8276537964177293]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177139729465989		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.7177139729465989 | validation: 0.8180105352271101]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033387860148396		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.7033387860148396 | validation: 0.8194733614110579]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921098139240257		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.6921098139240257 | validation: 0.707843064800163]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185488913666334		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.7185488913666334 | validation: 0.9680603402239303]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243046516233264		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.8243046516233264 | validation: 0.8025810192465187]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390409967929915		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.7390409967929915 | validation: 0.7605426202487631]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386278534215268		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.7386278534215268 | validation: 0.9290684907481186]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703999575606721		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.7703999575606721 | validation: 0.7108203223470364]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933026785058488		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.6933026785058488 | validation: 0.8338136423971007]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924236569762195		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.6924236569762195 | validation: 0.7200437632088814]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7139986474203485		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.7139986474203485 | validation: 0.886638079075352]
	TIME [epoch: 2.67 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718866436995254		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.718866436995254 | validation: 0.7198477666884218]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.702884325374722		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.702884325374722 | validation: 0.8407969911004947]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951349768372608		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.6951349768372608 | validation: 0.7243688972489141]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023650861315172		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.7023650861315172 | validation: 0.8551557949145252]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120356058931243		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.7120356058931243 | validation: 0.7039888212579859]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993886522781853		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.6993886522781853 | validation: 0.8848818198912382]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281335943905693		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.7281335943905693 | validation: 0.7390668675764817]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906259109730585		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.6906259109730585 | validation: 0.819157215720702]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945319002749736		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.6945319002749736 | validation: 0.7783989132366518]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040645181076107		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.7040645181076107 | validation: 0.7762653344458728]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034404012824901		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.7034404012824901 | validation: 0.8547573961902799]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109243754192102		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.7109243754192102 | validation: 0.7298176644973817]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871711340286776		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.6871711340286776 | validation: 0.8556123639728828]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968891011185115		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.6968891011185115 | validation: 0.676527939211624]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248989932804307		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.7248989932804307 | validation: 0.916975399142248]
	TIME [epoch: 2.67 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516773832426242		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.7516773832426242 | validation: 0.7306513688773842]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813003125805258		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.6813003125805258 | validation: 0.7636522305624608]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6696391403855139		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.6696391403855139 | validation: 0.7708494855781831]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695223106687334		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.6695223106687334 | validation: 0.7505109604298931]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649282002005313		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.6649282002005313 | validation: 0.7704901374611136]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680661887396248		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.6680661887396248 | validation: 0.7387921525497139]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718538583552439		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.6718538583552439 | validation: 0.8485727926785831]
	TIME [epoch: 2.67 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221175065955623		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.7221175065955623 | validation: 0.7354114319126206]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691614623749666		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.7691614623749666 | validation: 0.8760339922048601]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215083680830875		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.7215083680830875 | validation: 0.7328096930578543]
	TIME [epoch: 2.67 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799501858502521		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.6799501858502521 | validation: 0.8374345139793118]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958159876966619		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.6958159876966619 | validation: 0.7873782390202131]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861866239549642		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.6861866239549642 | validation: 0.723605347634954]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743106359070649		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.6743106359070649 | validation: 0.8506164233032101]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963553744646		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.6963553744646 | validation: 0.6420209992580913]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021208651699165		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8021208651699165 | validation: 0.9185074860819052]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485942300161711		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.7485942300161711 | validation: 0.7972423078538882]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838905870838525		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.6838905870838525 | validation: 0.660946835099644]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010356808403672		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.7010356808403672 | validation: 0.8657298326892082]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246071935150172		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.7246071935150172 | validation: 0.7397357827388366]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706009206112629		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.6706009206112629 | validation: 0.6717648299064005]
	TIME [epoch: 2.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906299726961709		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.6906299726961709 | validation: 0.859080133546439]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094542739325173		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.7094542739325173 | validation: 0.7269595730901977]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67155766679805		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.67155766679805 | validation: 0.7367814514310247]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622272660532539		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.6622272660532539 | validation: 0.7461021882685654]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718773061590997		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.6718773061590997 | validation: 0.7858447729178785]
	TIME [epoch: 2.67 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867057403495797		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.6867057403495797 | validation: 0.7751770267025954]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037795445610714		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.7037795445610714 | validation: 0.8114681946440747]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883387410761737		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.6883387410761737 | validation: 0.7118851098346323]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714283735833716		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.6714283735833716 | validation: 0.8116526091272898]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731158298500225		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.6731158298500225 | validation: 0.6935810438788024]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748020560745279		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.6748020560745279 | validation: 0.8184090128265654]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782285910125367		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.6782285910125367 | validation: 0.6564504155398139]
	TIME [epoch: 2.67 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127962110854944		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.7127962110854944 | validation: 0.8923959291267233]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403855840249002		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.7403855840249002 | validation: 0.7323813233092302]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796055837059115		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.6796055837059115 | validation: 0.7589743751181479]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678272641055996		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.6678272641055996 | validation: 0.7079780150706525]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676982764573135		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.676982764573135 | validation: 0.858785030081635]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7163693002000902		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.7163693002000902 | validation: 0.7154903464482931]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023889866258358		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.7023889866258358 | validation: 0.7753092019040166]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760513119336031		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.6760513119336031 | validation: 0.7535404963914866]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570705552139774		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.6570705552139774 | validation: 0.7462323844703587]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559254934334277		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.6559254934334277 | validation: 0.7608572673173838]
	TIME [epoch: 2.68 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562675367004962		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.6562675367004962 | validation: 0.7075012272942519]
	TIME [epoch: 2.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622476634159645		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.6622476634159645 | validation: 0.7841582495988966]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832734980703126		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.6832734980703126 | validation: 0.7492320672647256]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959982937001092		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.6959982937001092 | validation: 0.831885939357178]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097153950449403		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.7097153950449403 | validation: 0.7421934699873796]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600752344017662		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.6600752344017662 | validation: 0.6905670577374035]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563758936415729		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.6563758936415729 | validation: 0.8389309931419465]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869644891954737		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.6869644891954737 | validation: 0.6496104155819943]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357182290231464		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.7357182290231464 | validation: 0.9100752600446804]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419681567135851		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.7419681567135851 | validation: 0.7810635948375444]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684015258155654		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.6684015258155654 | validation: 0.6376248946908448]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726205413574734		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.726205413574734 | validation: 0.8454235122783719]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005699447634308		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.7005699447634308 | validation: 0.7085326493619979]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562322801828215		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.6562322801828215 | validation: 0.6937744274390181]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658929227698085		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.658929227698085 | validation: 0.8162204556158893]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6770302665574633		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.6770302665574633 | validation: 0.6717418510574066]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663214958997117		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.6663214958997117 | validation: 0.8003519003116631]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799815732700918		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.6799815732700918 | validation: 0.765604540955187]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849072587603103		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.6849072587603103 | validation: 0.7223926420863221]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787545355856838		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.6787545355856838 | validation: 0.8319350815575149]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176705941643217		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.7176705941643217 | validation: 0.7389221002330304]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764543200325658		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.6764543200325658 | validation: 0.7453939790393067]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653591862889997		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.6653591862889997 | validation: 0.7327986371572095]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6539469540927739		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.6539469540927739 | validation: 0.7650142170990549]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544768970439344		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.6544768970439344 | validation: 0.7065810952364413]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570927550233944		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.6570927550233944 | validation: 0.7771034135751287]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6539379251474422		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.6539379251474422 | validation: 0.6889445606306691]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709277849626885		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.6709277849626885 | validation: 0.8586149817308716]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996657592809606		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.6996657592809606 | validation: 0.6702395923915138]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871973795599757		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.6871973795599757 | validation: 0.8194752430273321]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6794193726268921		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.6794193726268921 | validation: 0.6954399951195942]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655748908512862		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.655748908512862 | validation: 0.743121558068266]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747786024640083		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.6747786024640083 | validation: 0.7480674176350259]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685649098658115		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.6685649098658115 | validation: 0.8078673084165695]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047564018088418		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.7047564018088418 | validation: 0.7321011765139755]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690084652777842		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.6690084652777842 | validation: 0.7311476415176914]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.651382217735069		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.651382217735069 | validation: 0.7474908763744064]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559156631693509		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.6559156631693509 | validation: 0.7524963429819594]
	TIME [epoch: 2.67 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518381288396252		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.6518381288396252 | validation: 0.7265566142145631]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554558965768055		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.6554558965768055 | validation: 0.6892831933997372]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6578942845815022		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.6578942845815022 | validation: 0.825552819958054]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990539345066625		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.6990539345066625 | validation: 0.6469551705483428]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109680436975623		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.7109680436975623 | validation: 0.8428947671362942]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699442099259083		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.699442099259083 | validation: 0.7260787233624584]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540531844669636		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.6540531844669636 | validation: 0.6318076976385743]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935645088545064		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.6935645088545064 | validation: 0.8576290723845225]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026672205699207		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.7026672205699207 | validation: 0.7022775617902991]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561472174888493		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.6561472174888493 | validation: 0.7510125289058142]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630744236778355		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.6630744236778355 | validation: 0.7241548472409018]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682243365267199		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.6682243365267199 | validation: 0.7340211252890577]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569752889619375		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.6569752889619375 | validation: 0.7116382012482758]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504325100747732		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.6504325100747732 | validation: 0.775493895297859]
	TIME [epoch: 2.67 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526738159768679		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.6526738159768679 | validation: 0.6813267751042513]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495311498037734		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.6495311498037734 | validation: 0.7542888154867864]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552464699283707		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.6552464699283707 | validation: 0.6383808106142101]
	TIME [epoch: 2.68 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666431735220942		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.666431735220942 | validation: 0.8865277148843588]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307030080263701		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.7307030080263701 | validation: 0.7662587501729652]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691651733648376		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.6691651733648376 | validation: 0.6577594521398763]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663564917158914		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.663564917158914 | validation: 0.8097921430292594]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793245481662848		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.6793245481662848 | validation: 0.6480642816414446]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573770680731768		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.6573770680731768 | validation: 0.6816138012819537]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6479225712900031		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.6479225712900031 | validation: 0.7393869848157417]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472364428786094		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.6472364428786094 | validation: 0.6464667355202457]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6697192292267178		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.6697192292267178 | validation: 0.7957382968455967]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880302569364909		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.6880302569364909 | validation: 0.7554643468486063]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686551908462702		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.6686551908462702 | validation: 0.6766430082850601]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765991274577755		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.6765991274577755 | validation: 0.7678523337154851]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656417825342844		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.656417825342844 | validation: 0.6577971694858559]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541872230879884		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.6541872230879884 | validation: 0.7664853479137836]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553285408785396		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.6553285408785396 | validation: 0.70776245044332]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550231065761024		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.6550231065761024 | validation: 0.7941570404577791]
	TIME [epoch: 2.67 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786148483628776		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.6786148483628776 | validation: 0.6963791811517728]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629444556125007		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.6629444556125007 | validation: 0.6929440268399225]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498042848146619		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.6498042848146619 | validation: 0.711588622360892]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438161583213756		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.6438161583213756 | validation: 0.7031116400728341]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361422836343704		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.6361422836343704 | validation: 0.7132623385510816]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402450170276237		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.6402450170276237 | validation: 0.6981517223827174]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6435218467185578		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.6435218467185578 | validation: 0.7555101452682291]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556820415943599		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.6556820415943599 | validation: 0.6499081349530487]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687072152383084		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.687072152383084 | validation: 0.8403485464859632]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156005731661143		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.7156005731661143 | validation: 0.7201320857312485]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510771533428462		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.6510771533428462 | validation: 0.6584815535774102]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658164747181389		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.658164747181389 | validation: 0.811429799605794]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6801444135125075		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.6801444135125075 | validation: 0.650604538436129]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661542329479297		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.661542329479297 | validation: 0.7687421992997487]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544548581999443		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.6544548581999443 | validation: 0.7018445823067881]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409342878772237		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.6409342878772237 | validation: 0.7127232826329025]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644103669913649		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.644103669913649 | validation: 0.7766149663996835]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636722544573831		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.6636722544573831 | validation: 0.705591825439303]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652616127916039		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.652616127916039 | validation: 0.6661525717591562]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421064844406524		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.6421064844406524 | validation: 0.7736942420062533]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637261227843589		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.6637261227843589 | validation: 0.6127778419889242]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6794528762989529		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.6794528762989529 | validation: 0.7819050987951313]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604523323949861		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.6604523323949861 | validation: 0.7169685512895234]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429295119830556		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.6429295119830556 | validation: 0.5789363017678343]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7164217591240865		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.7164217591240865 | validation: 0.8160662379139678]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936147092309404		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.6936147092309404 | validation: 0.7212054833301095]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424959265353499		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.6424959265353499 | validation: 0.6409361575072632]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6715316436048394		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.6715316436048394 | validation: 0.7714230742221831]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490421802143161		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.6490421802143161 | validation: 0.686900160001004]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640462068305283		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.640462068305283 | validation: 0.6755355035465649]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407862555680464		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.6407862555680464 | validation: 0.7326617313652577]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6456522734234176		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.6456522734234176 | validation: 0.6867617647968101]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404545869990029		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.6404545869990029 | validation: 0.7149352834052873]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363323031424046		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.6363323031424046 | validation: 0.6862136699136889]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345325539138836		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.6345325539138836 | validation: 0.7178729580119103]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384216460903963		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.6384216460903963 | validation: 0.736296713485002]
	TIME [epoch: 2.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589915274565971		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.6589915274565971 | validation: 0.6956274499337556]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707130675083371		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.6707130675083371 | validation: 0.7694946646925609]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723976329360479		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.6723976329360479 | validation: 0.6798742751403286]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536450915931241		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.6536450915931241 | validation: 0.7548866565338943]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654939773626019		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.654939773626019 | validation: 0.6511447606643932]
	TIME [epoch: 2.66 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487716130188659		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.6487716130188659 | validation: 0.7589773459733783]
	TIME [epoch: 2.68 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399667449213363		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.6399667449213363 | validation: 0.7286948209766755]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459922725361062		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.6459922725361062 | validation: 0.668591344402177]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530385452937938		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.6530385452937938 | validation: 0.6914818897896049]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367019291491401		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.6367019291491401 | validation: 0.7168987884990723]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338900509540251		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.6338900509540251 | validation: 0.6257076312765435]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683089447157197		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.6683089447157197 | validation: 0.7751687638596345]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731745980144794		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.6731745980144794 | validation: 0.6788658119821102]
	TIME [epoch: 2.66 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395508347830422		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.6395508347830422 | validation: 0.7016763294718197]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365044597030576		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.6365044597030576 | validation: 0.7120198769292068]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322578382468881		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.6322578382468881 | validation: 0.6386283392687777]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416101815871097		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.6416101815871097 | validation: 0.7140301964870522]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462613238635598		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.6462613238635598 | validation: 0.707524740411616]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496031649535209		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.6496031649535209 | validation: 0.7392891366335604]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556527292564844		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.6556527292564844 | validation: 0.7180290273280865]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454153640291252		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.6454153640291252 | validation: 0.6764346932505814]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434338402097391		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.6434338402097391 | validation: 0.7682934895374571]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708074503083183		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.6708074503083183 | validation: 0.670518643363714]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495462221896358		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.6495462221896358 | validation: 0.7003232042793682]
	TIME [epoch: 2.66 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313465748071875		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.6313465748071875 | validation: 0.6605463888655292]
	TIME [epoch: 2.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361145063192849		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.6361145063192849 | validation: 0.723343257503862]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360571031243991		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.6360571031243991 | validation: 0.6332962952024648]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535838859410171		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.6535838859410171 | validation: 0.7674761562502341]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473807669265303		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.6473807669265303 | validation: 0.6397252069731959]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461365202513116		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.6461365202513116 | validation: 0.7010822581464022]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315967959753835		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.6315967959753835 | validation: 0.6421824756545407]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424574968928576		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.6424574968928576 | validation: 0.7547901927252618]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509866671130309		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.6509866671130309 | validation: 0.7075459101443906]
	TIME [epoch: 2.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503960166173761		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.6503960166173761 | validation: 0.7209302293711396]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502248060223489		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.6502248060223489 | validation: 0.6838775458680284]
	TIME [epoch: 2.66 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440794146672368		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.6440794146672368 | validation: 0.7522945276510264]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541959289521162		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.6541959289521162 | validation: 0.6220105003202875]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687540769327873		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.6687540769327873 | validation: 0.73442018751341]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411468194739942		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.6411468194739942 | validation: 0.6400696464092013]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390910121727754		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.6390910121727754 | validation: 0.736312989783306]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397110210647792		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.6397110210647792 | validation: 0.6462401874391377]
	TIME [epoch: 2.66 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358940882321662		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.6358940882321662 | validation: 0.6845050666312905]
	TIME [epoch: 2.66 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366739040397671		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.6366739040397671 | validation: 0.6835226845789086]
	TIME [epoch: 2.66 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290363642059016		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.6290363642059016 | validation: 0.7196925817703758]
	TIME [epoch: 2.66 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367242754874128		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.6367242754874128 | validation: 0.7152882102193096]
	TIME [epoch: 2.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411714169318096		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.6411714169318096 | validation: 0.6730253298696799]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396413071784133		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.6396413071784133 | validation: 0.7012120307696824]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639893505696064		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.639893505696064 | validation: 0.6654362852089813]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311693120433636		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.6311693120433636 | validation: 0.646736173592512]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401411974531536		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.6401411974531536 | validation: 0.7280967095199938]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429243495669138		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.6429243495669138 | validation: 0.6566202471630717]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339556376087824		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.6339556376087824 | validation: 0.6616965891737302]
	TIME [epoch: 2.66 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295972260184958		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.6295972260184958 | validation: 0.777679465455717]
	TIME [epoch: 2.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607400564060074		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.6607400564060074 | validation: 0.657279102260287]
	TIME [epoch: 2.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543791386075881		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.6543791386075881 | validation: 0.7404110725658863]
	TIME [epoch: 2.66 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354923132351518		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.6354923132351518 | validation: 0.6311797866942599]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391085112601723		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.6391085112601723 | validation: 0.7114130324964687]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359407627299514		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.6359407627299514 | validation: 0.6574249895242087]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320179711051279		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.6320179711051279 | validation: 0.6346709744189349]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360258973281999		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.6360258973281999 | validation: 0.7544665267964812]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544158139619529		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.6544158139619529 | validation: 0.6715300221277244]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404735563030921		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.6404735563030921 | validation: 0.6741316000559441]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463512872402187		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.6463512872402187 | validation: 0.7143427209493612]
	TIME [epoch: 2.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546745540947714		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.6546745540947714 | validation: 0.738602552077545]
	TIME [epoch: 2.66 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466846476001161		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.6466846476001161 | validation: 0.6408970193782549]
	TIME [epoch: 2.66 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419752789370798		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.6419752789370798 | validation: 0.7346798495651654]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371537781294031		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.6371537781294031 | validation: 0.6516131225829751]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323457104424417		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.6323457104424417 | validation: 0.6681488948655874]
	TIME [epoch: 2.67 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329727127797281		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.6329727127797281 | validation: 0.6979444013777164]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308286830025723		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.6308286830025723 | validation: 0.6231552422007972]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365088733846248		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.6365088733846248 | validation: 0.6929140139804695]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234289689897305		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.6234289689897305 | validation: 0.7580596815452378]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665239279592115		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.665239279592115 | validation: 0.6808520316421504]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562803836992697		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.6562803836992697 | validation: 0.6571087206502922]
	TIME [epoch: 2.66 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289515731000135		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.6289515731000135 | validation: 0.7579943012840745]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569974224637111		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.6569974224637111 | validation: 0.6244752953815669]
	TIME [epoch: 2.66 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643347325941997		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.643347325941997 | validation: 0.7485904168542362]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440832791770642		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.6440832791770642 | validation: 0.6496492382949968]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286612875236888		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.6286612875236888 | validation: 0.6185504907139617]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345727236901857		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.6345727236901857 | validation: 0.7449453756420175]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466267781959824		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.6466267781959824 | validation: 0.6438388306600699]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372825544612128		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.6372825544612128 | validation: 0.663964770790868]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284861412916876		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.6284861412916876 | validation: 0.6975083539563798]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356044999676336		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.6356044999676336 | validation: 0.6296741994169404]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409705675282531		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.6409705675282531 | validation: 0.7078023926524811]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328410550508516		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.6328410550508516 | validation: 0.6383663308179707]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296672537265201		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.6296672537265201 | validation: 0.7127691093801323]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464515984064773		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.6464515984064773 | validation: 0.6641158764453481]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275966534152153		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.6275966534152153 | validation: 0.6544909994914349]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd4_20250516_213103/states/model_phi1_4a_distortion_v2_4_v_mmd4_813.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1948.746 seconds.
