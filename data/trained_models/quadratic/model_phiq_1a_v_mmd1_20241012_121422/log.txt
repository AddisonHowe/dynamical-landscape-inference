Args:
Namespace(name='model_phiq_1a_v_mmd1', outdir='out/model_training/model_phiq_1a_v_mmd1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[100, 250, 500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2310143386

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.736346386934729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.736346386934729 | validation: 4.46728520794497]
	TIME [epoch: 104 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.553928354506155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.553928354506155 | validation: 4.509081699828873]
	TIME [epoch: 12.8 sec]
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.624627799077606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.624627799077606 | validation: 4.6764782164626]
	TIME [epoch: 12.7 sec]
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.352012114872386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.352012114872386 | validation: 4.328826627005052]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.138648900915458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138648900915458 | validation: 4.024590399155597]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.061781397525654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.061781397525654 | validation: 4.054752317208891]
	TIME [epoch: 12.7 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8943373431489587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8943373431489587 | validation: 3.8104953177539285]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6974476531406277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6974476531406277 | validation: 3.778836699437863]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6813539230392283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6813539230392283 | validation: 3.644631368373566]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3437969856996617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3437969856996617 | validation: 3.465982584509847]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.157212682192111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157212682192111 | validation: 3.142969309257217]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.046667918417077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.046667918417077 | validation: 3.0498123300697304]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.88842415708295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.88842415708295 | validation: 2.8855721777788452]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7230802664723237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7230802664723237 | validation: 2.7460024288466145]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.626811832532719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.626811832532719 | validation: 2.9559365520146175]
	TIME [epoch: 12.7 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.698918948297103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.698918948297103 | validation: 2.6117722600861395]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4187404380391975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4187404380391975 | validation: 2.42809883814859]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.251222690963132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.251222690963132 | validation: 2.2857394258363124]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1809466161948627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1809466161948627 | validation: 2.172187308330244]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1974901606804127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1974901606804127 | validation: 2.1885025386052472]
	TIME [epoch: 12.7 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0702369440796407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0702369440796407 | validation: 2.0429933600578902]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.979137956456994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.979137956456994 | validation: 1.956899860619358]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9770546371128939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9770546371128939 | validation: 2.151818244724327]
	TIME [epoch: 12.6 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9471544981493916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9471544981493916 | validation: 1.860562218020805]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7830461669915605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7830461669915605 | validation: 1.7920129848799369]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8022674307612516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8022674307612516 | validation: 1.788031604411163]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7435514115942319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7435514115942319 | validation: 1.6777665921050247]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6739957621240413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6739957621240413 | validation: 1.6453898145940506]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6835784246514254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6835784246514254 | validation: 1.6360820509018734]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6634193721344401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6634193721344401 | validation: 1.6677626439626976]
	TIME [epoch: 12.7 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5975907680936725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5975907680936725 | validation: 1.5170040159716107]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6175652779465093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6175652779465093 | validation: 1.5802970195334973]
	TIME [epoch: 12.6 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5595824215246854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5595824215246854 | validation: 1.49078064678208]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5132852175011826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5132852175011826 | validation: 1.5059786436934084]
	TIME [epoch: 12.6 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.58287323222213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.58287323222213 | validation: 1.6017588215624454]
	TIME [epoch: 12.6 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5873592967348844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5873592967348844 | validation: 1.5303063359965097]
	TIME [epoch: 12.7 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4915190608972135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4915190608972135 | validation: 1.4490860233782783]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4479332836443541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4479332836443541 | validation: 1.433388192054724]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4688919898999784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4688919898999784 | validation: 1.50435889776546]
	TIME [epoch: 12.7 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4570760590861567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4570760590861567 | validation: 1.4067752682005321]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3905709429225477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3905709429225477 | validation: 1.4601347264425635]
	TIME [epoch: 12.7 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.502802171876547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.502802171876547 | validation: 1.4198494136832465]
	TIME [epoch: 12.7 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.427760511213171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.427760511213171 | validation: 1.4157058462385232]
	TIME [epoch: 12.6 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3638263344991948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3638263344991948 | validation: 1.33538298027645]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.333211855890481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.333211855890481 | validation: 1.3235623425458953]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3379063352497538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3379063352497538 | validation: 1.364588995501221]
	TIME [epoch: 12.6 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.35058391890217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.35058391890217 | validation: 1.342548146951063]
	TIME [epoch: 12.7 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.331349163768846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.331349163768846 | validation: 1.3185219223587659]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3153419851411825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3153419851411825 | validation: 1.3097980775926246]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3116436360071206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3116436360071206 | validation: 1.351049270709625]
	TIME [epoch: 12.6 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3067468281714842		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 1.3067468281714842 | validation: 1.2775046038260014]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.263399790352451		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 1.263399790352451 | validation: 1.2353737642271094]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.310419916646156		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 1.310419916646156 | validation: 1.3292892591763215]
	TIME [epoch: 12.6 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3042603866925524		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 1.3042603866925524 | validation: 1.3195219805620482]
	TIME [epoch: 12.6 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2697312780659575		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.2697312780659575 | validation: 1.2664949094326436]
	TIME [epoch: 12.7 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2522977232415369		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 1.2522977232415369 | validation: 1.2323964605263402]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2503893597509146		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 1.2503893597509146 | validation: 1.20286092028919]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.274655013673277		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 1.274655013673277 | validation: 1.2887268842912005]
	TIME [epoch: 12.7 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.247650932311535		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 1.247650932311535 | validation: 1.2057990131588288]
	TIME [epoch: 12.6 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.239674160632621		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 1.239674160632621 | validation: 1.2614774978848378]
	TIME [epoch: 12.7 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2340738409808374		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 1.2340738409808374 | validation: 1.2216581143980139]
	TIME [epoch: 12.7 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.221959612925791		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 1.221959612925791 | validation: 1.189057936059681]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2075688218977656		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 1.2075688218977656 | validation: 1.2499905278062613]
	TIME [epoch: 12.7 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.243357324096105		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 1.243357324096105 | validation: 1.2371387632607405]
	TIME [epoch: 12.7 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2050708189346169		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 1.2050708189346169 | validation: 1.1491588319653263]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.214654847255792		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 1.214654847255792 | validation: 1.2373315806151974]
	TIME [epoch: 12.7 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1912196280903657		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 1.1912196280903657 | validation: 1.3509778352168071]
	TIME [epoch: 12.7 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2278410382421585		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 1.2278410382421585 | validation: 1.0864140190710103]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2029365874457363		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 1.2029365874457363 | validation: 1.2414499355468347]
	TIME [epoch: 12.6 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.223680463406029		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 1.223680463406029 | validation: 1.2020870467281362]
	TIME [epoch: 12.7 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1919179878763058		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 1.1919179878763058 | validation: 1.1678083530778336]
	TIME [epoch: 12.6 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.171799117165731		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 1.171799117165731 | validation: 1.099309946140023]
	TIME [epoch: 12.6 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1714364277148317		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 1.1714364277148317 | validation: 1.1731171049827094]
	TIME [epoch: 12.7 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1571562913706825		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.1571562913706825 | validation: 1.1312738621041951]
	TIME [epoch: 12.6 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.115652838971004		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 1.115652838971004 | validation: 1.0606813358028293]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.161428327686678		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 1.161428327686678 | validation: 1.1507659412094227]
	TIME [epoch: 12.6 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1156595049843525		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 1.1156595049843525 | validation: 1.0588871578711674]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1327945732088813		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 1.1327945732088813 | validation: 1.0644435718483414]
	TIME [epoch: 12.6 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1027843111698015		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 1.1027843111698015 | validation: 1.089731802177736]
	TIME [epoch: 12.6 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1328288425681694		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 1.1328288425681694 | validation: 1.0713311353890138]
	TIME [epoch: 12.7 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1082940350405726		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 1.1082940350405726 | validation: 1.1285030898386927]
	TIME [epoch: 12.6 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1839071035197644		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 1.1839071035197644 | validation: 1.0646586972754406]
	TIME [epoch: 12.6 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0968355523325444		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 1.0968355523325444 | validation: 1.0937352215590892]
	TIME [epoch: 12.7 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0925086311323953		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 1.0925086311323953 | validation: 1.1846459547824737]
	TIME [epoch: 12.6 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1262939307816882		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.1262939307816882 | validation: 1.0201828386134035]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0505402925385596		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 1.0505402925385596 | validation: 1.1047986164239383]
	TIME [epoch: 12.7 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0831708742898276		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 1.0831708742898276 | validation: 1.207818292767007]
	TIME [epoch: 12.6 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.194578444634		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 1.194578444634 | validation: 1.2271967223380464]
	TIME [epoch: 12.6 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1549663764781484		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 1.1549663764781484 | validation: 1.1924832031048913]
	TIME [epoch: 12.7 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1439352991053808		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.1439352991053808 | validation: 1.331313549623756]
	TIME [epoch: 12.6 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1930618564274345		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 1.1930618564274345 | validation: 1.2022686345461384]
	TIME [epoch: 12.6 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1383011980643511		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.1383011980643511 | validation: 1.0943288346892466]
	TIME [epoch: 12.6 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0919761973135604		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.0919761973135604 | validation: 1.0312095523095839]
	TIME [epoch: 12.6 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0517983346247777		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.0517983346247777 | validation: 0.9882826244103531]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9984745472247235		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 0.9984745472247235 | validation: 1.011257386558526]
	TIME [epoch: 12.6 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0964234570882896		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 1.0964234570882896 | validation: 1.0890034005273277]
	TIME [epoch: 12.7 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0667241477579177		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 1.0667241477579177 | validation: 1.0593360486097498]
	TIME [epoch: 12.6 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.050287855645672		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 1.050287855645672 | validation: 1.0890073019647906]
	TIME [epoch: 12.6 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.055571746638544		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 1.055571746638544 | validation: 1.020312942427387]
	TIME [epoch: 12.7 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.996247211332275		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 0.996247211332275 | validation: 0.9875965141483292]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0469531727419852		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 1.0469531727419852 | validation: 0.9990169056167437]
	TIME [epoch: 115 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9732494820104629		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 0.9732494820104629 | validation: 0.9889086939571696]
	TIME [epoch: 24.6 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0460764936690388		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 1.0460764936690388 | validation: 1.0055057432979122]
	TIME [epoch: 24.3 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9959951015630386		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 0.9959951015630386 | validation: 1.0262917401369127]
	TIME [epoch: 24.4 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1143295628847605		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 1.1143295628847605 | validation: 1.5437971110602835]
	TIME [epoch: 24.4 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.330636900571613		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 1.330636900571613 | validation: 1.465708499519058]
	TIME [epoch: 24.3 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2824009817991202		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 1.2824009817991202 | validation: 1.3210438308046357]
	TIME [epoch: 24.4 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1843812428041054		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 1.1843812428041054 | validation: 1.1917539680978066]
	TIME [epoch: 24.3 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0910672265010266		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 1.0910672265010266 | validation: 1.174849336243899]
	TIME [epoch: 24.4 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0418657065493666		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 1.0418657065493666 | validation: 0.9906106035517199]
	TIME [epoch: 24.4 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9554363179340983		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 0.9554363179340983 | validation: 1.0308686098700446]
	TIME [epoch: 24.4 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0405376181757646		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 1.0405376181757646 | validation: 1.0648750569964445]
	TIME [epoch: 24.4 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9945394097392419		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 0.9945394097392419 | validation: 0.9826100160451654]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.994020857747798		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 0.994020857747798 | validation: 0.9258601018375725]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9428893880449654		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 0.9428893880449654 | validation: 0.9187895103410217]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9238710293623412		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 0.9238710293623412 | validation: 1.2559124964834742]
	TIME [epoch: 24.3 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1010095503290311		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 1.1010095503290311 | validation: 0.9774960966019963]
	TIME [epoch: 24.4 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9367914190214529		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 0.9367914190214529 | validation: 0.8801110966147065]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8838020096321281		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 0.8838020096321281 | validation: 0.9845352618973497]
	TIME [epoch: 24.3 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9786765345272435		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 0.9786765345272435 | validation: 0.9055867930991075]
	TIME [epoch: 24.4 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9560136565375047		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 0.9560136565375047 | validation: 0.8701124654356367]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8706849164376871		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 0.8706849164376871 | validation: 1.108139338809698]
	TIME [epoch: 24.3 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0221966884758424		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 1.0221966884758424 | validation: 1.0902379063971948]
	TIME [epoch: 24.4 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0784464655006172		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 1.0784464655006172 | validation: 0.9989762803869062]
	TIME [epoch: 24.3 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.951221181177797		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 0.951221181177797 | validation: 0.8560469280704555]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8588445259614519		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 0.8588445259614519 | validation: 0.897981794361159]
	TIME [epoch: 24.4 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9383262949864779		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 0.9383262949864779 | validation: 0.9149620702823529]
	TIME [epoch: 24.3 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8844678236328016		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 0.8844678236328016 | validation: 0.8308224627616443]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8674012613304181		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 0.8674012613304181 | validation: 1.120663525618181]
	TIME [epoch: 24.4 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9454650336005636		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 0.9454650336005636 | validation: 0.8910948472838633]
	TIME [epoch: 24.3 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8271656437330469		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.8271656437330469 | validation: 0.7611415170503308]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8084780902886355		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 0.8084780902886355 | validation: 0.8471340117363946]
	TIME [epoch: 24.3 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9246900352908902		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 0.9246900352908902 | validation: 1.014982933051129]
	TIME [epoch: 24.4 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.286348506050957		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 1.286348506050957 | validation: 1.6347228596878227]
	TIME [epoch: 24.4 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.364101146906561		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 1.364101146906561 | validation: 1.4417759137591948]
	TIME [epoch: 24.3 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2242335210857909		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 1.2242335210857909 | validation: 1.2607176289114683]
	TIME [epoch: 24.4 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0918147981669728		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 1.0918147981669728 | validation: 1.140612441910085]
	TIME [epoch: 24.3 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9509073929386694		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 0.9509073929386694 | validation: 0.895647524266858]
	TIME [epoch: 24.3 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8935971836598378		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 0.8935971836598378 | validation: 1.0379776019278077]
	TIME [epoch: 24.4 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.948394235476036		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 0.948394235476036 | validation: 0.9187118228911306]
	TIME [epoch: 24.3 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8576496958019835		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 0.8576496958019835 | validation: 0.8039264544620394]
	TIME [epoch: 24.4 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8426030222581088		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 0.8426030222581088 | validation: 0.961843153321623]
	TIME [epoch: 24.4 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8204282181812267		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 0.8204282181812267 | validation: 0.7775737053241901]
	TIME [epoch: 24.3 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8296701752054844		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 0.8296701752054844 | validation: 1.1050418152395225]
	TIME [epoch: 24.4 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9313669481535481		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 0.9313669481535481 | validation: 0.9807410427520313]
	TIME [epoch: 24.4 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8540341538605059		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 0.8540341538605059 | validation: 0.7640831418450429]
	TIME [epoch: 24.4 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8181433765381156		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 0.8181433765381156 | validation: 0.7901855636736891]
	TIME [epoch: 24.4 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8427259011091396		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 0.8427259011091396 | validation: 0.8247235962838351]
	TIME [epoch: 24.3 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7980509675598902		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 0.7980509675598902 | validation: 0.994318585684951]
	TIME [epoch: 24.4 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8596577654082646		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.8596577654082646 | validation: 0.73306246361314]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7387657614954362		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 0.7387657614954362 | validation: 0.7044821892172419]
	TIME [epoch: 24.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7812377732511798		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 0.7812377732511798 | validation: 0.7865370478501735]
	TIME [epoch: 24.4 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7835522377539241		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 0.7835522377539241 | validation: 0.66433246748193]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8738285639837785		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 0.8738285639837785 | validation: 0.9762814514282303]
	TIME [epoch: 24.3 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8443150094632582		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 0.8443150094632582 | validation: 0.6935113484788658]
	TIME [epoch: 24.4 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8228188757024744		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 0.8228188757024744 | validation: 0.8556206372109615]
	TIME [epoch: 24.3 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8356100945406547		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 0.8356100945406547 | validation: 0.7070749048845415]
	TIME [epoch: 24.4 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7541730243243929		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 0.7541730243243929 | validation: 1.070340906008517]
	TIME [epoch: 24.4 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7453542773537452		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 0.7453542773537452 | validation: 1.1573520072246202]
	TIME [epoch: 24.3 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.009846685785674		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 1.009846685785674 | validation: 1.1453477164090047]
	TIME [epoch: 24.4 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9090214373642013		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 0.9090214373642013 | validation: 0.9239809096104354]
	TIME [epoch: 24.3 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7251572545262107		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 0.7251572545262107 | validation: 0.7084868329220873]
	TIME [epoch: 24.3 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9182668061909358		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 0.9182668061909358 | validation: 1.2111060905100348]
	TIME [epoch: 24.4 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1499321013837225		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 1.1499321013837225 | validation: 1.1544581963429061]
	TIME [epoch: 24.3 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1024643209372085		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 1.1024643209372085 | validation: 1.091079085348036]
	TIME [epoch: 24.4 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0584921341511182		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 1.0584921341511182 | validation: 1.243464465638615]
	TIME [epoch: 24.3 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1051012385211074		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 1.1051012385211074 | validation: 1.1359663225882173]
	TIME [epoch: 24.3 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.926815736021121		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 0.926815736021121 | validation: 0.9922926147154574]
	TIME [epoch: 24.3 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8129684969623255		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.8129684969623255 | validation: 0.7508585308965003]
	TIME [epoch: 24.3 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7122719980681491		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 0.7122719980681491 | validation: 0.9836484470888227]
	TIME [epoch: 24.4 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9370211972476553		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 0.9370211972476553 | validation: 0.9357463953741627]
	TIME [epoch: 24.4 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9137113785994422		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 0.9137113785994422 | validation: 0.7925177059315949]
	TIME [epoch: 24.3 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8026646489954612		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 0.8026646489954612 | validation: 0.9273739821083119]
	TIME [epoch: 24.4 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8863490475599172		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 0.8863490475599172 | validation: 0.9465959564896974]
	TIME [epoch: 24.3 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7079445956034105		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 0.7079445956034105 | validation: 0.7867996169864164]
	TIME [epoch: 24.3 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6801491243412077		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 0.6801491243412077 | validation: 0.6362366188740183]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7034822833966276		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 0.7034822833966276 | validation: 0.7636605363000926]
	TIME [epoch: 24.4 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7910929810467887		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 0.7910929810467887 | validation: 1.0216233960788246]
	TIME [epoch: 24.4 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7406295857353791		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 0.7406295857353791 | validation: 0.7719046078594918]
	TIME [epoch: 24.4 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7834065980698349		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 0.7834065980698349 | validation: 0.9688713043728685]
	TIME [epoch: 24.3 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7572361048468195		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 0.7572361048468195 | validation: 0.6944586383599595]
	TIME [epoch: 24.4 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6482955800710536		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 0.6482955800710536 | validation: 0.7784058007744428]
	TIME [epoch: 24.3 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.669655557234072		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 0.669655557234072 | validation: 0.5994265621634673]
	TIME [epoch: 24.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6732020307384268		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 0.6732020307384268 | validation: 0.5889013502281204]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6039696225694132		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 0.6039696225694132 | validation: 0.5897739757777484]
	TIME [epoch: 24.3 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6907704367865426		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 0.6907704367865426 | validation: 0.9456469402585379]
	TIME [epoch: 24.4 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8225761126156275		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 0.8225761126156275 | validation: 0.8134434519023844]
	TIME [epoch: 24.4 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7995817967517981		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.7995817967517981 | validation: 0.7416457714008498]
	TIME [epoch: 24.3 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6974332870145362		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 0.6974332870145362 | validation: 0.705752273808072]
	TIME [epoch: 24.3 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6708461272088837		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 0.6708461272088837 | validation: 0.5777826331590936]
	TIME [epoch: 24.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7135103403505025		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 0.7135103403505025 | validation: 1.043420554224067]
	TIME [epoch: 24.4 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.052129374949399		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 1.052129374949399 | validation: 0.8614953047881504]
	TIME [epoch: 24.4 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8922742823418361		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 0.8922742823418361 | validation: 0.8169415407308573]
	TIME [epoch: 24.3 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8261535542639561		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 0.8261535542639561 | validation: 0.8945006720916076]
	TIME [epoch: 24.4 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8380712139385422		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 0.8380712139385422 | validation: 0.8222947018592988]
	TIME [epoch: 24.4 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7413116740481116		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 0.7413116740481116 | validation: 0.7243427883541613]
	TIME [epoch: 24.3 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6226171696633404		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 0.6226171696633404 | validation: 1.2300732081602463]
	TIME [epoch: 24.4 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9511793092150286		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 0.9511793092150286 | validation: 0.7427898214199643]
	TIME [epoch: 24.3 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6219819817064202		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 0.6219819817064202 | validation: 0.7030715591670756]
	TIME [epoch: 24.3 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.751132237654289		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 0.751132237654289 | validation: 0.8406206811103307]
	TIME [epoch: 24.4 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7478279912819898		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 0.7478279912819898 | validation: 0.6073711613890159]
	TIME [epoch: 24.3 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5699770664765136		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 0.5699770664765136 | validation: 0.8221029410080919]
	TIME [epoch: 24.4 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6741811570620388		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 0.6741811570620388 | validation: 0.6175980591178976]
	TIME [epoch: 24.3 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6175684896880805		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 0.6175684896880805 | validation: 0.9499812558225611]
	TIME [epoch: 24.3 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6125769240938375		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 0.6125769240938375 | validation: 0.8376920863951811]
	TIME [epoch: 24.4 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6135840746157568		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 0.6135840746157568 | validation: 0.6955562285894092]
	TIME [epoch: 24.3 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7191894363698088		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.7191894363698088 | validation: 0.6094801322787902]
	TIME [epoch: 24.4 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6305858594593897		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 0.6305858594593897 | validation: 0.7543557271016421]
	TIME [epoch: 24.4 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5370316443595895		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 0.5370316443595895 | validation: 0.7251937480791579]
	TIME [epoch: 24.3 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6130144851916483		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 0.6130144851916483 | validation: 0.5562533016465326]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.699821146618002		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 0.699821146618002 | validation: 0.9709798221398365]
	TIME [epoch: 24.4 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8542143179482748		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 0.8542143179482748 | validation: 0.6933167108111367]
	TIME [epoch: 24.3 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5931881017219935		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 0.5931881017219935 | validation: 0.6779878403315611]
	TIME [epoch: 24.4 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790475541180125		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 0.5790475541180125 | validation: 0.6733106799486313]
	TIME [epoch: 24.3 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450644657535738		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 0.5450644657535738 | validation: 0.5812637504455479]
	TIME [epoch: 24.4 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5405108007184274		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 0.5405108007184274 | validation: 0.525562539108652]
	TIME [epoch: 24.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303010764455653		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 0.5303010764455653 | validation: 0.5419030559001996]
	TIME [epoch: 24.3 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5180289405909402		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 0.5180289405909402 | validation: 0.9659095514903439]
	TIME [epoch: 24.4 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9162655931597812		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 0.9162655931597812 | validation: 0.8652874111004936]
	TIME [epoch: 24.3 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.792729079089505		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 0.792729079089505 | validation: 0.6521722109245597]
	TIME [epoch: 24.3 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5854102486326223		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 0.5854102486326223 | validation: 0.849162184095513]
	TIME [epoch: 24.4 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7925713351995836		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 0.7925713351995836 | validation: 0.7134947554529592]
	TIME [epoch: 24.3 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6814608531769599		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 0.6814608531769599 | validation: 0.6672314236897086]
	TIME [epoch: 24.4 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5210597374746047		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 0.5210597374746047 | validation: 0.5687652502697846]
	TIME [epoch: 24.4 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6059171636414566		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 0.6059171636414566 | validation: 0.6206446758013118]
	TIME [epoch: 24.3 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6274218045695807		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.6274218045695807 | validation: 0.6579291020378369]
	TIME [epoch: 24.4 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6493726018909401		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 0.6493726018909401 | validation: 1.2761830188118652]
	TIME [epoch: 24.3 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.131961248121684		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 1.131961248121684 | validation: 1.0957309641741442]
	TIME [epoch: 24.3 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9206486527070818		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 0.9206486527070818 | validation: 0.9986566830117741]
	TIME [epoch: 24.4 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0304568174295963		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 1.0304568174295963 | validation: 1.0436874046475353]
	TIME [epoch: 24.3 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9301190471395437		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 0.9301190471395437 | validation: 0.9624446297356366]
	TIME [epoch: 24.4 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8744529215143864		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 0.8744529215143864 | validation: 0.8664672145528616]
	TIME [epoch: 24.3 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8335299481372451		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 0.8335299481372451 | validation: 0.8117912731448473]
	TIME [epoch: 24.3 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7173929480476322		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 0.7173929480476322 | validation: 0.8700115382544571]
	TIME [epoch: 24.4 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7448405975828896		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 0.7448405975828896 | validation: 0.9282142146570856]
	TIME [epoch: 24.3 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8277812113225581		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 0.8277812113225581 | validation: 0.813115872073761]
	TIME [epoch: 24.4 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7280990666711353		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 0.7280990666711353 | validation: 0.7211834536326382]
	TIME [epoch: 24.4 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5814744740458702		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 0.5814744740458702 | validation: 0.6714289087837828]
	TIME [epoch: 24.3 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7060233596164713		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 0.7060233596164713 | validation: 0.6386556922023998]
	TIME [epoch: 24.4 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5469819270181875		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 0.5469819270181875 | validation: 0.6335817719183923]
	TIME [epoch: 24.4 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5282201776405332		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 0.5282201776405332 | validation: 0.591754326916343]
	TIME [epoch: 24.3 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6768061698759636		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 0.6768061698759636 | validation: 0.6704688738597122]
	TIME [epoch: 24.4 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.577055661087276		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 0.577055661087276 | validation: 0.5830679726441055]
	TIME [epoch: 24.3 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7490674550737547		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 0.7490674550737547 | validation: 0.8291636077993116]
	TIME [epoch: 24.4 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.670369760294573		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.670369760294573 | validation: 0.5549646030848856]
	TIME [epoch: 24.4 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5860451060012175		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 0.5860451060012175 | validation: 0.8234939826247214]
	TIME [epoch: 24.3 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6630340519637591		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 0.6630340519637591 | validation: 0.5306703838980149]
	TIME [epoch: 24.3 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5020754792594202		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 0.5020754792594202 | validation: 0.6232574273961493]
	TIME [epoch: 24.3 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5027886461136454		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 0.5027886461136454 | validation: 0.7283903456118503]
	TIME [epoch: 24.3 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4794635747301714		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 0.4794635747301714 | validation: 0.9958772163049706]
	TIME [epoch: 24.4 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8258657844174453		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 0.8258657844174453 | validation: 0.7315251516295679]
	TIME [epoch: 140 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6235729688074505		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 0.6235729688074505 | validation: 0.5909122434326992]
	TIME [epoch: 48.5 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47993659707244307		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 0.47993659707244307 | validation: 0.5519510437418316]
	TIME [epoch: 48.4 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5309818940245137		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 0.5309818940245137 | validation: 0.508719588622184]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5573770980952942		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 0.5573770980952942 | validation: 0.5378277666667817]
	TIME [epoch: 48.4 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5280390671837014		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 0.5280390671837014 | validation: 0.5452845186020996]
	TIME [epoch: 48.4 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47839711323185563		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 0.47839711323185563 | validation: 0.7223507948998413]
	TIME [epoch: 48.4 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6132351924253402		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 0.6132351924253402 | validation: 0.5043661167471921]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45776829788887363		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 0.45776829788887363 | validation: 0.4908984293486838]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4330185743868652		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 0.4330185743868652 | validation: 0.5368679792921401]
	TIME [epoch: 48.4 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5924235243222129		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 0.5924235243222129 | validation: 0.7453584877392794]
	TIME [epoch: 48.4 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6711362973916027		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 0.6711362973916027 | validation: 0.7818468312464473]
	TIME [epoch: 48.3 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5813965727401299		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 0.5813965727401299 | validation: 0.7968843440559834]
	TIME [epoch: 48.3 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5436675547185672		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.5436675547185672 | validation: 0.9062396330102384]
	TIME [epoch: 48.4 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5476924285607828		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 0.5476924285607828 | validation: 0.77940109634005]
	TIME [epoch: 48.5 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5061030250208891		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 0.5061030250208891 | validation: 0.6728514295602954]
	TIME [epoch: 48.4 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48984030345708907		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 0.48984030345708907 | validation: 0.47711806319220385]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4334132865819056		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 0.4334132865819056 | validation: 0.5002699056926901]
	TIME [epoch: 48.4 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4220034591088307		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 0.4220034591088307 | validation: 0.5117778654505076]
	TIME [epoch: 48.4 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4995536790878644		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 0.4995536790878644 | validation: 0.46657690806466]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4532675036045357		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 0.4532675036045357 | validation: 0.5115475515286415]
	TIME [epoch: 48.4 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4718512927730012		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 0.4718512927730012 | validation: 0.5920972072637939]
	TIME [epoch: 48.4 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42928884275387064		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 0.42928884275387064 | validation: 0.47162418614112245]
	TIME [epoch: 48.4 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4038771236154054		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 0.4038771236154054 | validation: 0.5073177942654752]
	TIME [epoch: 48.4 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5020814345635092		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 0.5020814345635092 | validation: 0.5806326062050808]
	TIME [epoch: 48.4 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4883128568398889		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 0.4883128568398889 | validation: 0.5224807594509953]
	TIME [epoch: 48.3 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39573720561083103		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 0.39573720561083103 | validation: 0.5639883228099072]
	TIME [epoch: 48.4 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193336607666403		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 0.5193336607666403 | validation: 0.5751311054865352]
	TIME [epoch: 48.3 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.474578247047317		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 0.474578247047317 | validation: 0.453498017195218]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38891653241163215		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 0.38891653241163215 | validation: 0.6021740476066675]
	TIME [epoch: 48.3 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48199397223408924		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 0.48199397223408924 | validation: 0.609767092875247]
	TIME [epoch: 48.3 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4848640971510071		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 0.4848640971510071 | validation: 0.5850383987595306]
	TIME [epoch: 48.2 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46811543543189693		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.46811543543189693 | validation: 0.5075017581008711]
	TIME [epoch: 48.2 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38180823914343276		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 0.38180823914343276 | validation: 0.46208119610520226]
	TIME [epoch: 48.4 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5491259984437415		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 0.5491259984437415 | validation: 0.5035063592015941]
	TIME [epoch: 48.4 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42053974589087		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 0.42053974589087 | validation: 0.44674971012407283]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41255989929428877		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.41255989929428877 | validation: 0.5089362913054208]
	TIME [epoch: 48.3 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4492675530426365		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.4492675530426365 | validation: 0.6036750272663147]
	TIME [epoch: 48.3 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44656655970933545		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.44656655970933545 | validation: 0.4532566483445932]
	TIME [epoch: 48.3 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40122400559441607		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 0.40122400559441607 | validation: 0.5038779809651973]
	TIME [epoch: 48.2 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4208554127860867		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.4208554127860867 | validation: 0.5332426866255475]
	TIME [epoch: 48.3 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4257134343416831		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 0.4257134343416831 | validation: 0.44067467366335134]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45502424622647814		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 0.45502424622647814 | validation: 0.43462737609590285]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41028575444848475		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 0.41028575444848475 | validation: 0.41463973914515306]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3702192554835108		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 0.3702192554835108 | validation: 0.4098550353263751]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38487600143374445		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.38487600143374445 | validation: 0.4536516767741008]
	TIME [epoch: 48.3 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4389857541130673		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 0.4389857541130673 | validation: 0.5611713408822618]
	TIME [epoch: 48.4 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4346577698731061		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.4346577698731061 | validation: 0.3958523871635776]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36279271266091284		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.36279271266091284 | validation: 0.4981587774687193]
	TIME [epoch: 48.4 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4104448265581683		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 0.4104448265581683 | validation: 0.3807576048714061]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39261108543910783		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 0.39261108543910783 | validation: 0.646523371870917]
	TIME [epoch: 48.4 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6556385682819884		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.6556385682819884 | validation: 0.6002826643312855]
	TIME [epoch: 48.4 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49178282343324514		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 0.49178282343324514 | validation: 0.45057639465455934]
	TIME [epoch: 48.4 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4772961245913644		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 0.4772961245913644 | validation: 0.40112698313717]
	TIME [epoch: 48.4 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3974002803552341		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 0.3974002803552341 | validation: 0.4578169710204274]
	TIME [epoch: 48.4 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.392065587065728		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.392065587065728 | validation: 0.5076972509117736]
	TIME [epoch: 48.4 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4387947594632326		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.4387947594632326 | validation: 0.5207251405708698]
	TIME [epoch: 48.4 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43525849781973525		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.43525849781973525 | validation: 0.4028396966100143]
	TIME [epoch: 48.4 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36359493431230583		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 0.36359493431230583 | validation: 0.45544309447076625]
	TIME [epoch: 48.3 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5955315264735623		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.5955315264735623 | validation: 0.4337583375896]
	TIME [epoch: 48.3 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4222783836429954		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 0.4222783836429954 | validation: 0.43185599583287554]
	TIME [epoch: 48.3 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4288810525646288		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 0.4288810525646288 | validation: 0.5170505120729042]
	TIME [epoch: 48.3 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41400315584303626		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 0.41400315584303626 | validation: 0.4986835151301555]
	TIME [epoch: 48.3 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4042827830381119		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 0.4042827830381119 | validation: 0.44536305683989497]
	TIME [epoch: 48.4 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6211674537694396		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 0.6211674537694396 | validation: 1.2685995823078224]
	TIME [epoch: 48.4 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9010408673891274		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.9010408673891274 | validation: 0.6179245291547991]
	TIME [epoch: 48.4 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.519941720199312		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 0.519941720199312 | validation: 0.45779719820291065]
	TIME [epoch: 48.3 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3867003800916699		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 0.3867003800916699 | validation: 0.4881913971521201]
	TIME [epoch: 48.3 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4544951874798079		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.4544951874798079 | validation: 0.4887773587414571]
	TIME [epoch: 48.4 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4098487103788659		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 0.4098487103788659 | validation: 0.530987063057459]
	TIME [epoch: 48.4 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.456451704802293		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.456451704802293 | validation: 0.47094428713058256]
	TIME [epoch: 48.3 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42348177172655305		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 0.42348177172655305 | validation: 0.44340078210315187]
	TIME [epoch: 48.4 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.402972170007414		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 0.402972170007414 | validation: 0.4314681826159135]
	TIME [epoch: 48.4 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4811225057096314		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 0.4811225057096314 | validation: 0.439021477457913]
	TIME [epoch: 48.4 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4286249555732762		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 0.4286249555732762 | validation: 0.41776870891890705]
	TIME [epoch: 48.4 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3962425364349553		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.3962425364349553 | validation: 0.7284066879898399]
	TIME [epoch: 48.4 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4359358972369287		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.4359358972369287 | validation: 0.400810024815893]
	TIME [epoch: 48.4 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648676605027487		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 0.3648676605027487 | validation: 0.4041028537584811]
	TIME [epoch: 48.4 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3559385641222115		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.3559385641222115 | validation: 0.44358323808215905]
	TIME [epoch: 48.3 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38424955023323115		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.38424955023323115 | validation: 0.822445033311819]
	TIME [epoch: 48.4 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5006885190221216		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.5006885190221216 | validation: 0.41609189116387413]
	TIME [epoch: 48.4 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3610791280940361		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.3610791280940361 | validation: 0.37397655460201124]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42063499197166565		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.42063499197166565 | validation: 0.4482987120266119]
	TIME [epoch: 48.4 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3749745737237031		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 0.3749745737237031 | validation: 0.40132584312435254]
	TIME [epoch: 48.3 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.355144101212569		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 0.355144101212569 | validation: 0.3666174122421562]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35819257679432676		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 0.35819257679432676 | validation: 0.40019800095533214]
	TIME [epoch: 48.3 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3295649674218738		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 0.3295649674218738 | validation: 0.36310755885024304]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35515245898974457		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 0.35515245898974457 | validation: 0.43858593250512734]
	TIME [epoch: 48.2 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37846665479162483		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 0.37846665479162483 | validation: 0.36523412483493434]
	TIME [epoch: 48.4 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34474499579303963		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.34474499579303963 | validation: 0.3973106618302012]
	TIME [epoch: 48.4 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3339807649746314		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 0.3339807649746314 | validation: 0.389152853882972]
	TIME [epoch: 48.3 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.360174251663941		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 0.360174251663941 | validation: 0.3597489897808024]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40137089519246716		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 0.40137089519246716 | validation: 0.4054775693999466]
	TIME [epoch: 48.3 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3278765311016233		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 0.3278765311016233 | validation: 0.4074565489193121]
	TIME [epoch: 48.2 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3852914923791435		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 0.3852914923791435 | validation: 0.4169929355006983]
	TIME [epoch: 48.2 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3525723123425473		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 0.3525723123425473 | validation: 0.3720346817498155]
	TIME [epoch: 48.2 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3464976623408151		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 0.3464976623408151 | validation: 0.34371546160354477]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3514082907818083		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.3514082907818083 | validation: 0.5227644562221703]
	TIME [epoch: 48.2 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35872316246713476		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.35872316246713476 | validation: 0.47914796556431055]
	TIME [epoch: 48.2 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32251746727848124		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.32251746727848124 | validation: 0.363064180428457]
	TIME [epoch: 48.3 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33997058766429716		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 0.33997058766429716 | validation: 0.33795308914859656]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3195373096495493		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.3195373096495493 | validation: 0.3713270848311164]
	TIME [epoch: 48.2 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30432493751928985		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.30432493751928985 | validation: 0.35723340963806105]
	TIME [epoch: 48.2 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38567803469155637		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.38567803469155637 | validation: 0.9309199559529442]
	TIME [epoch: 48.2 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6630923354477898		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.6630923354477898 | validation: 0.46897554353334414]
	TIME [epoch: 48.2 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4262118372877469		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 0.4262118372877469 | validation: 0.505522152556889]
	TIME [epoch: 48.2 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43971582641998846		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 0.43971582641998846 | validation: 0.3734308626764032]
	TIME [epoch: 48.2 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3344194571748143		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.3344194571748143 | validation: 0.4216871747301516]
	TIME [epoch: 48.3 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37517946634056887		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.37517946634056887 | validation: 0.4343810646797803]
	TIME [epoch: 48.3 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33684698147634917		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.33684698147634917 | validation: 0.37062159565928887]
	TIME [epoch: 48.2 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32657615402369744		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.32657615402369744 | validation: 0.3484805790815952]
	TIME [epoch: 48.2 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31659328277864884		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.31659328277864884 | validation: 0.5569071368477231]
	TIME [epoch: 48.2 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4802386993354155		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 0.4802386993354155 | validation: 0.3899314312657991]
	TIME [epoch: 48.2 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38279807241100783		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.38279807241100783 | validation: 0.4239383922504283]
	TIME [epoch: 48.3 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3176126448027656		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.3176126448027656 | validation: 0.33139968171958933]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008490278692648		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.3008490278692648 | validation: 0.37288629063559525]
	TIME [epoch: 48.3 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152530035162701		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.3152530035162701 | validation: 0.35433193302228416]
	TIME [epoch: 48.2 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3630248083661918		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.3630248083661918 | validation: 0.426194335983031]
	TIME [epoch: 48.2 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33697002495271267		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.33697002495271267 | validation: 0.3307036224285944]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31082057928992546		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 0.31082057928992546 | validation: 0.33879054817595355]
	TIME [epoch: 48.2 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921300075175		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.2921300075175 | validation: 0.43972016217635834]
	TIME [epoch: 48.2 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32554852502549725		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.32554852502549725 | validation: 0.38356779893405446]
	TIME [epoch: 48.3 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3777535205689756		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 0.3777535205689756 | validation: 0.35011632772956336]
	TIME [epoch: 48.3 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3175050545359104		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.3175050545359104 | validation: 0.3272028684322891]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41986389803715346		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 0.41986389803715346 | validation: 0.46751984238034416]
	TIME [epoch: 48.2 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4355057967620922		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.4355057967620922 | validation: 0.40192425939994714]
	TIME [epoch: 48.3 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31613709479089336		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.31613709479089336 | validation: 0.3336456588640257]
	TIME [epoch: 48.3 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3012214720458225		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.3012214720458225 | validation: 0.5984170101954627]
	TIME [epoch: 48.2 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4064888632165556		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.4064888632165556 | validation: 0.4936629903468759]
	TIME [epoch: 48.2 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3925944381656308		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.3925944381656308 | validation: 0.3351258606602144]
	TIME [epoch: 48.3 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31565772707391987		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.31565772707391987 | validation: 0.3243906092429707]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34124140555141375		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 0.34124140555141375 | validation: 0.33818865082225746]
	TIME [epoch: 48.3 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43945812651607563		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 0.43945812651607563 | validation: 0.37987931578888845]
	TIME [epoch: 48.3 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44260637593971325		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 0.44260637593971325 | validation: 0.4297813552611097]
	TIME [epoch: 48.3 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3717397383059564		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 0.3717397383059564 | validation: 0.44112655479408136]
	TIME [epoch: 48.3 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3417746059899973		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 0.3417746059899973 | validation: 0.34920744539254267]
	TIME [epoch: 48.3 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29238685141391213		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 0.29238685141391213 | validation: 0.34405109621347346]
	TIME [epoch: 48.3 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3212647206936954		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 0.3212647206936954 | validation: 0.33985912699965515]
	TIME [epoch: 48.3 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31011706838689035		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 0.31011706838689035 | validation: 0.31946413911549926]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32467040462932945		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 0.32467040462932945 | validation: 0.31116366466387035]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2837139778592522		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 0.2837139778592522 | validation: 0.31312453680511554]
	TIME [epoch: 48.2 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.272347338522972		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 0.272347338522972 | validation: 0.33766647922453186]
	TIME [epoch: 48.3 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3106951292105451		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 0.3106951292105451 | validation: 0.32600509208269496]
	TIME [epoch: 48.3 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2925030023595278		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 0.2925030023595278 | validation: 0.3479503188511722]
	TIME [epoch: 48.3 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33145624142090047		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 0.33145624142090047 | validation: 0.30428203153376]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2940749075913467		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 0.2940749075913467 | validation: 0.3162185107833583]
	TIME [epoch: 48.2 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37231658893635494		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.37231658893635494 | validation: 0.39097978578416714]
	TIME [epoch: 48.2 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151924805054942		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 0.3151924805054942 | validation: 0.3409511497904302]
	TIME [epoch: 48.2 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29058984773778385		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 0.29058984773778385 | validation: 0.31894125180693345]
	TIME [epoch: 48.2 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2866217671358272		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 0.2866217671358272 | validation: 0.36462219258935114]
	TIME [epoch: 48.3 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30440618355300275		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 0.30440618355300275 | validation: 0.34327152113764314]
	TIME [epoch: 48.4 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3043808652127642		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 0.3043808652127642 | validation: 0.3004926760094413]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29932694189403847		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 0.29932694189403847 | validation: 0.3140722909585519]
	TIME [epoch: 48.3 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141304019393404		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 0.3141304019393404 | validation: 0.34275145546711727]
	TIME [epoch: 48.3 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873415323021428		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 0.2873415323021428 | validation: 0.314047923573719]
	TIME [epoch: 48.3 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29869834872507584		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 0.29869834872507584 | validation: 0.2998257459282595]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2822090636827708		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 0.2822090636827708 | validation: 0.3173330743742464]
	TIME [epoch: 48.3 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3015443629500122		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 0.3015443629500122 | validation: 0.3474591368794647]
	TIME [epoch: 48.3 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37161915921499766		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 0.37161915921499766 | validation: 0.38893899850042135]
	TIME [epoch: 48.3 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35596617035551514		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 0.35596617035551514 | validation: 0.3573515698505798]
	TIME [epoch: 48.3 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31395077927439263		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 0.31395077927439263 | validation: 0.3522096649574385]
	TIME [epoch: 48.3 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27721950892519825		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 0.27721950892519825 | validation: 0.3328142756108875]
	TIME [epoch: 48.3 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27675595312603735		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 0.27675595312603735 | validation: 0.2880039431667962]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26840688843554245		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 0.26840688843554245 | validation: 0.32367487566720565]
	TIME [epoch: 48.3 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495457198431393		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 0.3495457198431393 | validation: 0.5299401854051812]
	TIME [epoch: 48.2 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33662669098407116		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.33662669098407116 | validation: 0.3220754883825543]
	TIME [epoch: 48.2 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29174139549234174		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 0.29174139549234174 | validation: 0.29482705705906476]
	TIME [epoch: 48.3 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26427201413297213		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 0.26427201413297213 | validation: 0.31212310778910324]
	TIME [epoch: 48.2 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27709020335777795		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 0.27709020335777795 | validation: 0.3715941473438623]
	TIME [epoch: 48.3 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2791341884944738		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 0.2791341884944738 | validation: 0.37515914653544147]
	TIME [epoch: 48.3 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32253248927301736		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 0.32253248927301736 | validation: 0.28710329873963714]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2762082261490053		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 0.2762082261490053 | validation: 0.2949459977685904]
	TIME [epoch: 48.3 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820165037484833		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 0.2820165037484833 | validation: 0.3677368291222608]
	TIME [epoch: 48.3 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29152457637512097		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 0.29152457637512097 | validation: 0.3136847995482258]
	TIME [epoch: 48.3 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2652528202460693		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 0.2652528202460693 | validation: 0.4696082582540143]
	TIME [epoch: 48.3 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5748539626095849		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 0.5748539626095849 | validation: 0.40775534291917437]
	TIME [epoch: 48.3 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35468599515998867		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 0.35468599515998867 | validation: 0.3491630990658983]
	TIME [epoch: 48.3 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34641510796386343		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 0.34641510796386343 | validation: 0.47421433379006783]
	TIME [epoch: 48.3 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4274046206957116		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 0.4274046206957116 | validation: 0.29653400364592486]
	TIME [epoch: 48.3 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29396109648043883		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 0.29396109648043883 | validation: 0.28406347686569533]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.278793615987712		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 0.278793615987712 | validation: 0.2880066820176129]
	TIME [epoch: 48.3 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26028518377343834		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 0.26028518377343834 | validation: 0.3073462034156256]
	TIME [epoch: 48.3 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650676721785663		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 0.2650676721785663 | validation: 0.4070763012624313]
	TIME [epoch: 48.3 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31134417256584335		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 0.31134417256584335 | validation: 0.3166428560391352]
	TIME [epoch: 48.3 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31348727311139546		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.31348727311139546 | validation: 0.31597161683424024]
	TIME [epoch: 48.3 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26326525979428567		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 0.26326525979428567 | validation: 0.28963293598565076]
	TIME [epoch: 48.3 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25802998233886654		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 0.25802998233886654 | validation: 0.28198257328136683]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2723955688944003		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 0.2723955688944003 | validation: 0.335025867546505]
	TIME [epoch: 48.2 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28724187390961226		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 0.28724187390961226 | validation: 0.3305540814457981]
	TIME [epoch: 48.2 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3265545393348177		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 0.3265545393348177 | validation: 0.3153619822797514]
	TIME [epoch: 48.2 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2920343730812403		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 0.2920343730812403 | validation: 0.33250979942940756]
	TIME [epoch: 48.2 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29584088950693443		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 0.29584088950693443 | validation: 0.29823624659237263]
	TIME [epoch: 48.2 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29669989348929543		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 0.29669989348929543 | validation: 0.2878174399627532]
	TIME [epoch: 48.2 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2600425343497989		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 0.2600425343497989 | validation: 0.2758004643859171]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26750873772454786		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 0.26750873772454786 | validation: 0.3367033549593698]
	TIME [epoch: 48.3 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29765705669148473		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 0.29765705669148473 | validation: 0.2779610618000089]
	TIME [epoch: 48.2 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25500326498576614		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 0.25500326498576614 | validation: 0.3199636300769929]
	TIME [epoch: 48.3 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26191539042944567		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 0.26191539042944567 | validation: 0.2893614062200144]
	TIME [epoch: 48.2 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26444763088749434		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 0.26444763088749434 | validation: 0.3247459581079969]
	TIME [epoch: 48.3 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2899321301461995		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 0.2899321301461995 | validation: 0.31407620361451205]
	TIME [epoch: 48.2 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29902612228591713		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 0.29902612228591713 | validation: 0.3458598599141189]
	TIME [epoch: 48.2 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3319258163225715		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 0.3319258163225715 | validation: 0.2990851730167839]
	TIME [epoch: 48.3 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27581024793719555		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 0.27581024793719555 | validation: 0.2962971482661505]
	TIME [epoch: 48.2 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27238615564407453		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.27238615564407453 | validation: 0.27592658813809945]
	TIME [epoch: 48.3 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942641562949017		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 0.2942641562949017 | validation: 0.28119110959423826]
	TIME [epoch: 48.3 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2843000936754018		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 0.2843000936754018 | validation: 0.29399371125757123]
	TIME [epoch: 48.2 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3194267504856655		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 0.3194267504856655 | validation: 0.30118414542267924]
	TIME [epoch: 48.2 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2763108456164323		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 0.2763108456164323 | validation: 0.333689231968186]
	TIME [epoch: 48.3 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30122600162700175		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 0.30122600162700175 | validation: 0.3505098114973144]
	TIME [epoch: 48.2 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28167947926653414		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 0.28167947926653414 | validation: 0.2925860552986681]
	TIME [epoch: 48.2 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628743542019738		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 0.2628743542019738 | validation: 0.27275452703438674]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599228158540374		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 0.2599228158540374 | validation: 0.29563055116961057]
	TIME [epoch: 48.3 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657134267515452		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 0.2657134267515452 | validation: 0.2766658927205154]
	TIME [epoch: 48.3 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25579608200504966		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 0.25579608200504966 | validation: 0.27496158284858346]
	TIME [epoch: 48.3 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2597274258906286		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 0.2597274258906286 | validation: 0.28022304396902065]
	TIME [epoch: 48.3 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.275689833296586		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 0.275689833296586 | validation: 0.27669000579975384]
	TIME [epoch: 48.3 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3107121823761289		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 0.3107121823761289 | validation: 0.35806275887723515]
	TIME [epoch: 48.3 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27115158080320634		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 0.27115158080320634 | validation: 0.26835560949865184]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25026057746128355		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 0.25026057746128355 | validation: 0.29022610623920386]
	TIME [epoch: 48.3 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26043971079003264		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 0.26043971079003264 | validation: 0.28768754541634356]
	TIME [epoch: 48.3 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26923866831950977		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 0.26923866831950977 | validation: 0.2777145900268744]
	TIME [epoch: 48.3 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105131691791736		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 0.3105131691791736 | validation: 0.2685391359195068]
	TIME [epoch: 48.3 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2715486090376222		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.2715486090376222 | validation: 0.2725967994452302]
	TIME [epoch: 48.4 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598684435007641		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 0.2598684435007641 | validation: 0.26891798142774215]
	TIME [epoch: 48.4 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25079017577966817		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 0.25079017577966817 | validation: 0.26579279589487836]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25202251355595584		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 0.25202251355595584 | validation: 0.3053001466521483]
	TIME [epoch: 48.4 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29121312999955806		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 0.29121312999955806 | validation: 0.29934499408728193]
	TIME [epoch: 48.4 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26093525210675134		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 0.26093525210675134 | validation: 0.2821892923327367]
	TIME [epoch: 48.4 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598956916665155		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 0.2598956916665155 | validation: 0.2828086704028846]
	TIME [epoch: 48.4 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2571926923654625		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 0.2571926923654625 | validation: 0.2662008846816916]
	TIME [epoch: 48.4 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2757833491191729		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 0.2757833491191729 | validation: 0.42424910076964845]
	TIME [epoch: 48.3 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3643869041358251		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 0.3643869041358251 | validation: 0.2816581527392533]
	TIME [epoch: 48.4 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2559858338016622		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 0.2559858338016622 | validation: 0.2924830696295405]
	TIME [epoch: 48.4 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.261645028699821		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 0.261645028699821 | validation: 0.28336113003012253]
	TIME [epoch: 48.4 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24757415729453316		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 0.24757415729453316 | validation: 0.2763042524057692]
	TIME [epoch: 48.4 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28105643696548865		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 0.28105643696548865 | validation: 0.26514048296818976]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25681975785009387		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 0.25681975785009387 | validation: 0.26112853009163917]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25478523340417436		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 0.25478523340417436 | validation: 0.308639669393853]
	TIME [epoch: 48.3 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26781311159021526		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 0.26781311159021526 | validation: 0.2962770514945475]
	TIME [epoch: 48.3 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26329817969815694		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 0.26329817969815694 | validation: 0.26273452416409826]
	TIME [epoch: 48.3 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2755306601809156		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 0.2755306601809156 | validation: 0.26631260452802197]
	TIME [epoch: 48.3 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24615746950659928		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.24615746950659928 | validation: 0.26780726736944194]
	TIME [epoch: 48.2 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2450797302378057		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 0.2450797302378057 | validation: 0.2735960735017507]
	TIME [epoch: 48.3 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2549634446184288		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 0.2549634446184288 | validation: 0.2587880083921774]
	TIME [epoch: 48.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26650192175966997		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 0.26650192175966997 | validation: 0.2643808637312889]
	TIME [epoch: 48.3 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24797587211484173		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 0.24797587211484173 | validation: 0.2698980912216739]
	TIME [epoch: 48.3 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26317527837458077		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 0.26317527837458077 | validation: 0.2631395563524767]
	TIME [epoch: 48.3 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.259724103042987		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 0.259724103042987 | validation: 0.2690417498661505]
	TIME [epoch: 48.3 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24635950772827483		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 0.24635950772827483 | validation: 0.2609566702536479]
	TIME [epoch: 48.2 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25083569864700944		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 0.25083569864700944 | validation: 0.2642650638474118]
	TIME [epoch: 48.2 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29970785828292523		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 0.29970785828292523 | validation: 0.3023299471842419]
	TIME [epoch: 187 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26487818398107377		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 0.26487818398107377 | validation: 0.2653027007160393]
	TIME [epoch: 96.5 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2687447006523079		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 0.2687447006523079 | validation: 0.30694966257544515]
	TIME [epoch: 96.4 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2682001600105288		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 0.2682001600105288 | validation: 0.25864425776678474]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25009021951100247		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 0.25009021951100247 | validation: 0.27522662493390593]
	TIME [epoch: 96.4 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2551516416989031		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 0.2551516416989031 | validation: 0.2618502474692175]
	TIME [epoch: 96.4 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540566364642701		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 0.2540566364642701 | validation: 0.2895678365537687]
	TIME [epoch: 96.4 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.264664127144507		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 0.264664127144507 | validation: 0.25984105706431737]
	TIME [epoch: 96.5 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27815477804266087		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 0.27815477804266087 | validation: 0.271469399260722]
	TIME [epoch: 96.4 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629132876896916		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 0.2629132876896916 | validation: 0.26720378264853917]
	TIME [epoch: 96.4 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24788387885307278		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.24788387885307278 | validation: 0.29598724948692173]
	TIME [epoch: 96.4 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547865094143103		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 0.2547865094143103 | validation: 0.2667284323880251]
	TIME [epoch: 96.4 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25179636880653183		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 0.25179636880653183 | validation: 0.28302077258008695]
	TIME [epoch: 96.4 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.254472501097847		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 0.254472501097847 | validation: 0.26267266038870324]
	TIME [epoch: 96.4 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582990259967721		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 0.2582990259967721 | validation: 0.33816543939609556]
	TIME [epoch: 96.5 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760622485871848		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 0.2760622485871848 | validation: 0.27401650169664216]
	TIME [epoch: 96.4 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24410141980865357		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 0.24410141980865357 | validation: 0.26141349505537603]
	TIME [epoch: 96.5 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24521822186791736		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 0.24521822186791736 | validation: 0.25815921588721563]
	TIME [epoch: 96.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24532076234597341		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 0.24532076234597341 | validation: 0.2638833981483521]
	TIME [epoch: 96.4 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25064579257156216		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 0.25064579257156216 | validation: 0.26071752410267485]
	TIME [epoch: 96.4 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23786415523296545		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 0.23786415523296545 | validation: 0.25499549661426674]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27735777653528787		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 0.27735777653528787 | validation: 0.3169085394245887]
	TIME [epoch: 96.4 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25980650611293477		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 0.25980650611293477 | validation: 0.26899622189902606]
	TIME [epoch: 96.4 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25581437312618577		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 0.25581437312618577 | validation: 0.28645679243362154]
	TIME [epoch: 96.4 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26185292115177394		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 0.26185292115177394 | validation: 0.2573923407847647]
	TIME [epoch: 96.4 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2412136816093061		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 0.2412136816093061 | validation: 0.2580310337467703]
	TIME [epoch: 96.4 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24038358739105928		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 0.24038358739105928 | validation: 0.2531659557688761]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26446498693579334		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 0.26446498693579334 | validation: 0.2541359802797856]
	TIME [epoch: 96.2 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24438746951034718		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 0.24438746951034718 | validation: 0.28798215650792924]
	TIME [epoch: 96.2 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24896563642153555		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.24896563642153555 | validation: 0.263788347104349]
	TIME [epoch: 96.3 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24686142762529928		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 0.24686142762529928 | validation: 0.2766002769090138]
	TIME [epoch: 96.2 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2442300256676637		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 0.2442300256676637 | validation: 0.2574841755009508]
	TIME [epoch: 96.4 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24245785213435622		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 0.24245785213435622 | validation: 0.2730701979589145]
	TIME [epoch: 96.3 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2430258152727392		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 0.2430258152727392 | validation: 0.2687233079370327]
	TIME [epoch: 96.3 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24430502864311832		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 0.24430502864311832 | validation: 0.2626474361938878]
	TIME [epoch: 96.3 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2433502710033663		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 0.2433502710033663 | validation: 0.25805667434674623]
	TIME [epoch: 96.2 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2711729019792944		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 0.2711729019792944 | validation: 0.25785795507108283]
	TIME [epoch: 96.4 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2526875947012461		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 0.2526875947012461 | validation: 0.251344712942599]
	TIME [epoch: 96.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2353068138317676		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 0.2353068138317676 | validation: 0.2738076155123299]
	TIME [epoch: 96.4 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2532257882779866		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 0.2532257882779866 | validation: 0.26624350426692484]
	TIME [epoch: 96.3 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26392777178018423		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 0.26392777178018423 | validation: 0.29622409502908964]
	TIME [epoch: 96.2 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27362245782177264		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 0.27362245782177264 | validation: 0.2693700839656153]
	TIME [epoch: 96.3 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24171474190510508		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 0.24171474190510508 | validation: 0.2616467884304365]
	TIME [epoch: 96.2 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23965941190260504		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 0.23965941190260504 | validation: 0.25284204828228424]
	TIME [epoch: 96.3 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2419602975919219		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 0.2419602975919219 | validation: 0.24942653341221513]
	TIME [epoch: 96.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_545.pth
	Model improved!!!
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24012018737610782		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 0.24012018737610782 | validation: 0.25548433514472146]
	TIME [epoch: 96.4 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27653507992710324		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 0.27653507992710324 | validation: 0.2616017810630335]
	TIME [epoch: 96.3 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25174514453896363		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 0.25174514453896363 | validation: 0.278708952049907]
	TIME [epoch: 96.3 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25947150808870056		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.25947150808870056 | validation: 0.2613486477936203]
	TIME [epoch: 96.3 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24684686122508592		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 0.24684686122508592 | validation: 0.24906504838214583]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24439267268208228		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 0.24439267268208228 | validation: 0.25564859684765806]
	TIME [epoch: 96.3 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23972802098279136		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 0.23972802098279136 | validation: 0.251609336720136]
	TIME [epoch: 96.4 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23616195573336196		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 0.23616195573336196 | validation: 0.2864525327690538]
	TIME [epoch: 96.4 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567238966695242		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 0.2567238966695242 | validation: 0.26127358040967535]
	TIME [epoch: 96.3 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24235422135011228		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 0.24235422135011228 | validation: 0.25668526194794977]
	TIME [epoch: 96.3 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24201400439946202		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 0.24201400439946202 | validation: 0.26468562637295645]
	TIME [epoch: 96.4 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23670489340089607		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 0.23670489340089607 | validation: 0.27114738411821027]
	TIME [epoch: 96.3 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25043019667659266		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 0.25043019667659266 | validation: 0.2610728670318705]
	TIME [epoch: 96.3 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24923966534792533		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 0.24923966534792533 | validation: 0.2559934692970358]
	TIME [epoch: 96.3 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24564646020906578		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 0.24564646020906578 | validation: 0.2495580124637642]
	TIME [epoch: 96.4 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24580874765676045		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 0.24580874765676045 | validation: 0.2709472714796506]
	TIME [epoch: 96.3 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2588498337992844		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 0.2588498337992844 | validation: 0.2932047914458083]
	TIME [epoch: 96.3 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.251683754600212		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 0.251683754600212 | validation: 0.2560963163513986]
	TIME [epoch: 96.4 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23673481916367098		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 0.23673481916367098 | validation: 0.2484613711568252]
	TIME [epoch: 96.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2411187449718295		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 0.2411187449718295 | validation: 0.24952652410616888]
	TIME [epoch: 96.4 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23491137620455899		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 0.23491137620455899 | validation: 0.25490912389071246]
	TIME [epoch: 96.4 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23686570072181823		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 0.23686570072181823 | validation: 0.29493910377162186]
	TIME [epoch: 96.4 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27676282591112467		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.27676282591112467 | validation: 0.27183804755890695]
	TIME [epoch: 96.4 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24214776065014149		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 0.24214776065014149 | validation: 0.2480915209634054]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24459663728332867		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 0.24459663728332867 | validation: 0.26263671273962685]
	TIME [epoch: 96.3 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24770102658959692		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 0.24770102658959692 | validation: 0.2575528326309455]
	TIME [epoch: 96.3 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24118865703473216		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 0.24118865703473216 | validation: 0.2652170928265146]
	TIME [epoch: 96.2 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27211243139734076		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 0.27211243139734076 | validation: 0.2563530945129532]
	TIME [epoch: 96.3 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23443463079827645		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 0.23443463079827645 | validation: 0.2905800647877717]
	TIME [epoch: 96.2 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24956259002827616		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 0.24956259002827616 | validation: 0.2465115748739517]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_575.pth
	Model improved!!!
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2522726381577356		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 0.2522726381577356 | validation: 0.24949868863242475]
	TIME [epoch: 96.3 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23421483244433247		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 0.23421483244433247 | validation: 0.2504400314073158]
	TIME [epoch: 96.3 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.240964263348256		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 0.240964263348256 | validation: 0.2462124122176475]
	TIME [epoch: 96.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2880356668897688		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 0.2880356668897688 | validation: 0.3736730509583771]
	TIME [epoch: 96.2 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.292030066507381		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 0.292030066507381 | validation: 0.26313496456598656]
	TIME [epoch: 96.3 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641611040649495		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 0.2641611040649495 | validation: 0.25340325869771085]
	TIME [epoch: 96.3 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24256220527800298		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 0.24256220527800298 | validation: 0.25368636952786405]
	TIME [epoch: 96.3 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2462181411451311		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 0.2462181411451311 | validation: 0.25108228494462664]
	TIME [epoch: 96.2 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23453085692575878		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 0.23453085692575878 | validation: 0.24598963654440964]
	TIME [epoch: 96.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23774373994046588		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 0.23774373994046588 | validation: 0.24980526411575144]
	TIME [epoch: 96.3 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23388273761153341		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 0.23388273761153341 | validation: 0.25247236012562346]
	TIME [epoch: 96.5 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23352427855694505		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.23352427855694505 | validation: 0.25270007937875916]
	TIME [epoch: 96.4 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23961058193556428		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 0.23961058193556428 | validation: 0.2501126597605886]
	TIME [epoch: 96.5 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24133903810694024		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 0.24133903810694024 | validation: 0.24861678828899803]
	TIME [epoch: 96.5 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23561597419671584		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 0.23561597419671584 | validation: 0.24926025912174607]
	TIME [epoch: 96.6 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2444486210777977		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 0.2444486210777977 | validation: 0.249411758668197]
	TIME [epoch: 96.6 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23294632338396212		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 0.23294632338396212 | validation: 0.2623403319178742]
	TIME [epoch: 96.7 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23650797134451718		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 0.23650797134451718 | validation: 0.2465304734953566]
	TIME [epoch: 96.6 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2357905476811387		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 0.2357905476811387 | validation: 0.2599786226766156]
	TIME [epoch: 96.7 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23705383518017273		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 0.23705383518017273 | validation: 0.25224794582091037]
	TIME [epoch: 96.6 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24326560495071525		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 0.24326560495071525 | validation: 0.26173366553060434]
	TIME [epoch: 96.7 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25455891327594515		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 0.25455891327594515 | validation: 0.24663738696311233]
	TIME [epoch: 96.6 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23330507508916448		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 0.23330507508916448 | validation: 0.25837901808603375]
	TIME [epoch: 96.7 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23552640719270657		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 0.23552640719270657 | validation: 0.25597849761987]
	TIME [epoch: 96.6 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23851835868042556		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 0.23851835868042556 | validation: 0.27076981590594973]
	TIME [epoch: 96.7 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2440044986729032		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 0.2440044986729032 | validation: 0.2581654654408245]
	TIME [epoch: 96.5 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23414692550641283		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 0.23414692550641283 | validation: 0.250682633993358]
	TIME [epoch: 96.4 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.244052293007278		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 0.244052293007278 | validation: 0.24947718787244977]
	TIME [epoch: 96.4 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23219725990009396		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 0.23219725990009396 | validation: 0.2539300621238666]
	TIME [epoch: 96.4 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2367237601622122		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 0.2367237601622122 | validation: 0.24739342530043093]
	TIME [epoch: 96.5 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23751095147494547		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.23751095147494547 | validation: 0.2804587294043461]
	TIME [epoch: 96.4 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24672446488892708		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 0.24672446488892708 | validation: 0.2582343582027853]
	TIME [epoch: 96.4 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23951149955258305		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 0.23951149955258305 | validation: 0.24923848140785265]
	TIME [epoch: 96.4 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23784511292658864		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 0.23784511292658864 | validation: 0.2449944707911884]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23824898625397148		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 0.23824898625397148 | validation: 0.2635075806546021]
	TIME [epoch: 96.4 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24636089166364247		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 0.24636089166364247 | validation: 0.27747487833041995]
	TIME [epoch: 96.4 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24956250094415502		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 0.24956250094415502 | validation: 0.2567859368036415]
	TIME [epoch: 96.4 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23366528962245758		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 0.23366528962245758 | validation: 0.2578406528637318]
	TIME [epoch: 96.4 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23815658270666779		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 0.23815658270666779 | validation: 0.2511796243082382]
	TIME [epoch: 96.4 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23103583111061138		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 0.23103583111061138 | validation: 0.254408865286645]
	TIME [epoch: 96.4 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23254583798221107		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 0.23254583798221107 | validation: 0.2562064927996909]
	TIME [epoch: 96.5 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24737222887574695		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 0.24737222887574695 | validation: 0.2434488017295154]
	TIME [epoch: 96.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241012_121422/states/model_phiq_1a_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2351994765863784		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 0.2351994765863784 | validation: 0.24797290288993326]
	TIME [epoch: 96.4 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23587122135889854		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 0.23587122135889854 | validation: 0.24467940812639896]
	TIME [epoch: 96.3 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23368899169534896		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 0.23368899169534896 | validation: 0.262815977224209]
	TIME [epoch: 96.3 sec]
EPOCH 621/1000:
	Training over batches...
